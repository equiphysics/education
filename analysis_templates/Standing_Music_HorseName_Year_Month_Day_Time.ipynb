{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/equiphysics/education/blob/main/analysis_templates/Standing_Music_HorseName_Year_Month_Day_Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1553b710"
      },
      "source": [
        "## EquiPhysics Data Analysis Notebook - Horses Standing and Listening to Music\n",
        "\n",
        "This notebook provides a comprehensive workflow for synchronizing and analyzing physiological and movement data from Polar devices with video recordings of horses. It integrates heart rate (HR), accelerometer (ACC) data, and audio/video features to offer insights into horse performance, welfare, and training.\n",
        "\n",
        "**Workflow Overview:**\n",
        "1.  **Cell 1: Data Loading & Synchronization:** Mounts Google Drive, allows file upload, loads Polar HR/ACC data and video, automatically aligns video to data based on movement patterns, and extracts audio.\n",
        "2.  **Cell 2: Feature Extraction & Export:** Computes various metrics (Instant HR, RMSSD, ACC movement, audio loudness, spectral flux, spectral centroid) and saves all processed data and metadata to Google Drive.\n",
        "3.  **Cell 3: Side-by-Side Video Renderer:** Generates a video with synchronized graphs of key metrics displayed alongside the original video.\n",
        "4.  **Cell 4: Portrait Reel Renderer:** Creates a vertical video suitable for social media, featuring the video (cropped to fill) and stacked graphs of metrics with a moving cursor.\n",
        "5.  **Cell 5: Correlation & Causality Analysis:**  Performs statistical analysis to explore relationships (correlations, time lags, and predictive causality) between music features, movement, and physiological states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ed3c915"
      },
      "source": [
        "## Session Notes\n",
        "\n",
        "Use this section to record important details about the current analysis session. This information will be saved along with your data exports in a `meta.json` file, providing context for your results.\n",
        "\n",
        "---\n",
        "\n",
        "**Date of Session:** [e.g., 2023-10-27]\n",
        "\n",
        "**Horse's Name:** [e.g., Spirit, Blackjack]\n",
        "\n",
        "**Brief Session Description:** [e.g., Dressage training, trail ride, lunging session]\n",
        "\n",
        "**Environmental Conditions:** [e.g., weather (temperature, humidity, wind), and ground surface (arena sand, grass, trail conditions)]\n",
        "\n",
        "**Horse's Subjective State:** [e.g., horse's energy levels, behavior, or apparent comfort before, during, and after the session]\n",
        "\n",
        "**Music Used (if any):** [e.g., Classical, Pop, None]\n",
        "\n",
        "**Notable Events/Interruptions/Anomalies:** [e.g., Dog barked at 1:30 in video, sensor slipped between 3:00-3:15]\n",
        "\n",
        "**Key Observations During Session:** [e.g., horse looked out the arena as a person walked by, etc.]\n",
        "\n",
        "**Additional Notes:**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d282f249"
      },
      "source": [
        "## Cell 1: Data Loading, Synchronization, and Setup\n",
        "\n",
        "**Purpose:** This cell is the starting point of the analysis. It provides an interactive interface to load your raw data files (Polar HR.txt, ACC.txt, and a video file), synchronize them, and define the overall analysis window.\n",
        "\n",
        "**What it does:**\n",
        "*   **File Input:** Allows you to either mount your Google Drive and select files or upload them directly from your computer.\n",
        "*   **Data Processing:** Reads Polar HR and ACC data, calculates an accelerometer-based movement proxy, and determines video duration.\n",
        "*   **Automatic Synchronization:** Based on user-defined video and data 'sync windows', it automatically searches for the optimal time offset to align the video with your physiological data by comparing movement patterns.\n",
        "*   **Audio Extraction:** Extracts the full audio track from the synchronized video.\n",
        "*   **Analysis Window Setup:** Sets a default analysis time window in 'data time' for subsequent processing cells.\n",
        "\n",
        "**How to use it:**\n",
        "1.  **Choose Source:** Select whether your files are on Google Drive or if you want to upload them.\n",
        "2.  **Mount/Upload:** If using Google Drive, click 'Mount Google Drive' and then 'Scan Drive Folder' after specifying the folder path. If uploading, click 'Upload HR/ACC/Video' and select your files.\n",
        "3.  **Load Files:** Once HR, ACC, and Video files are selected (or auto-detected), click 'Load selected files'. This will display a plot of the ACC movement proxy.\n",
        "4.  **Define Sync Windows:** Using the ACC movement proxy plot, identify a segment of clear horse movement (e.g., a canter transition) and enter the corresponding start and duration times for both the 'Video' and 'Data' streams. The 'Slack (s)' parameter determines the search range for the automatic offset, and 'Tweak (s)' allows for fine manual adjustment.\n",
        "5.  **Compute Offset:** Click 'Compute offset from windows' to run the synchronization algorithm. A diagnostic plot will be displayed to help you verify the alignment.\n",
        "6.  **Save Analysis Window:** Adjust the 'Analysis start (data s)' and 'Analysis stop (data s)' values to define the final time segment for your analysis, then click 'Save analysis window'. This prepares the `EQUIPHY` dictionary for the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ6_H0gi8v9V",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ==========================================================\n",
        "# Cell 1 (COLAB WIDGETS): Drive or Upload + USER-SEGMENT AUTO-SYNC (NO input())\n",
        "#\n",
        "# This cell provides a user interface to:\n",
        "# - Mount Google Drive (optional)\n",
        "# - Choose to load files from Google Drive OR upload them from your computer.\n",
        "# - Reads Polar HR.txt and ACC.txt data, and loads a video file.\n",
        "# - Computes and plots an ACC movement proxy against data time.\n",
        "# - Allows the user to define a video sync window [v0, v1] and a matching data sync window [d0, d1].\n",
        "# - Automatically searches for the optimal offset to align data_time = video_time + offset,\n",
        "#   constraining the search within the implied range of the provided sync windows.\n",
        "# - Displays a diagnostic overlay plot after alignment for verification.\n",
        "# - Extracts full audio from the synchronized video, saving it as `extracted_audio.wav`.\n",
        "# - Sets a default analysis window in DATA time for subsequent cells.\n",
        "# ==========================================================\n",
        "\n",
        "import os, sys, io, json, subprocess, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Colab widgets setup ---\n",
        "# Check if running in Google Colab environment\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if not IN_COLAB:\n",
        "    raise RuntimeError(\"This cell is written for Google Colab (widgets + Drive mount).\")\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Global dictionary to store state and processed data across cells\n",
        "EQUIPHY = {}\n",
        "\n",
        "# Supported video file extensions\n",
        "VIDEO_EXTS = (\".mp4\", \".mov\", \".m4v\", \".avi\", \".webm\", \".mkv\")\n",
        "\n",
        "# ---------------- Helper Functions ----------------\n",
        "\n",
        "def _run(cmd, quiet=True):\n",
        "    \"\"\"Helper to run shell commands and capture output.\"\"\"\n",
        "    if quiet:\n",
        "        return subprocess.check_output(cmd, stderr=subprocess.STDOUT).decode(\"utf-8\", errors=\"replace\")\n",
        "    return subprocess.check_output(cmd).decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "def auto_detect_names(names):\n",
        "    \"\"\"Automatically detects HR, ACC, and video files from a list of filenames.\"\"\"\n",
        "    names = list(names)\n",
        "    # Prioritize files with 'hr', 'acc' in their name for .txt files\n",
        "    hr  = next((f for f in names if f.lower().endswith(\".txt\") and \"hr\"  in f.lower()), None)\n",
        "    acc = next((f for f in names if f.lower().endswith(\".txt\") and \"acc\" in f.lower()), None)\n",
        "    # Detect video files based on common extensions\n",
        "    vid = next((f for f in names if f.lower().endswith(VIDEO_EXTS)), None)\n",
        "    return hr, acc, vid\n",
        "\n",
        "def read_polar_txt_bytes(file_bytes: bytes) -> pd.DataFrame:\n",
        "    \"\"\"Reads Polar HR/ACC data from byte content of a .txt file.\"\"\"\n",
        "    raw = file_bytes.replace(b\"\\x00\", b\"\") # Remove null bytes if present\n",
        "    lines = raw.decode(\"utf-8\", errors=\"replace\").splitlines()\n",
        "\n",
        "    # Find the start of data rows (after header/comments)\n",
        "    data_start = None\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.startswith(\"#\") or line.strip() == \"\":\n",
        "            continue\n",
        "        data_start = i\n",
        "        break\n",
        "    if data_start is None:\n",
        "        raise ValueError(\"Could not find data rows (file looks empty or only comments).\")\n",
        "\n",
        "    # Read data into DataFrame, convert relevant columns to numeric\n",
        "    df = pd.read_csv(io.StringIO(\"\\n\".join(lines[data_start:])), skipinitialspace=True)\n",
        "    df.columns = [c.strip() for c in df.columns] # Clean column names\n",
        "    for col in [\"MS\",\"RR\",\"SC\",\"ACCX\",\"ACCY\",\"ACCZ\",\"HR\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"MS\"]).reset_index(drop=True) # Drop rows with missing MS values\n",
        "    return df\n",
        "\n",
        "def ffprobe_duration(path: str):\n",
        "    \"\"\"Uses ffprobe to get video duration in seconds.\"\"\"\n",
        "    cmd = [\"ffprobe\",\"-v\",\"error\",\"-show_entries\",\"format=duration\",\"-of\",\"json\", path]\n",
        "    js = json.loads(_run(cmd))\n",
        "    try:\n",
        "        return float(js.get(\"format\", {}).get(\"duration\", None))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def ffprobe_wh(path: str):\n",
        "    \"\"\"Uses ffprobe to get video width and height.\"\"\"\n",
        "    cmd = [\"ffprobe\",\"-v\",\"error\",\"-select_streams\",\"v:0\",\"-show_entries\",\"stream=width,height\",\"-of\",\"json\", path]\n",
        "    js = json.loads(_run(cmd))\n",
        "    st = (js.get(\"streams\") or [{}])[0]\n",
        "    w = int(st.get(\"width\", 0) or 0)\n",
        "    h = int(st.get(\"height\", 0) or 0)\n",
        "    return w, h\n",
        "\n",
        "def extract_audio_wav(video_path: str, out_wav: str = \"extracted_audio.wav\", sr=44100):\n",
        "    \"\"\"Extracts audio from a video file using ffmpeg and saves it as a WAV.\"\"\"\n",
        "    cmd = [\"ffmpeg\",\"-y\",\"-i\",video_path,\"-vn\",\"-ac\",\"1\",\"-ar\",str(sr),\"-c:a\",\"pcm_s16le\",out_wav]\n",
        "    try:\n",
        "        subprocess.check_output(cmd, stderr=subprocess.STDOUT).decode(\"utf-8\", errors=\"replace\")\n",
        "        return out_wav\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"\\n⚠️ ffmpeg audio extraction failed.\")\n",
        "        print(e.output.decode(\"utf-8\", errors=\"replace\")[:1200])\n",
        "        return None\n",
        "\n",
        "# ---------- ACC Movement Index Calculation ----------\n",
        "from scipy.signal import butter, filtfilt\n",
        "def acc_movement_index(df_acc: pd.DataFrame, ms0: float,\n",
        "                       hp_hz=0.3, win_sec=0.5, hop_sec=0.1):\n",
        "    \"\"\"Calculates a movement index from accelerometer data.\n",
        "    It high-passes the acceleration, computes RMS magnitude in windows.\n",
        "    \"\"\"\n",
        "    req = {\"ACCX\",\"ACCY\",\"ACCZ\",\"MS\"}\n",
        "    missing = req - set(df_acc.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"ACC missing columns: {sorted(missing)}\")\n",
        "\n",
        "    # Convert MS to seconds relative to a common 'ms0' start time\n",
        "    t = (df_acc[\"MS\"].to_numpy(float) - ms0) / 1000.0\n",
        "    ax = df_acc[\"ACCX\"].to_numpy(float) / 1000.0 # Convert to g\n",
        "    ay = df_acc[\"ACCY\"].to_numpy(float) / 1000.0\n",
        "    az = df_acc[\"ACCZ\"].to_numpy(float) / 1000.0\n",
        "\n",
        "    # Estimate sampling frequency from MS timestamps\n",
        "    dms = np.diff(df_acc[\"MS\"].to_numpy(float))\n",
        "    dms = dms[np.isfinite(dms) & (dms > 0)]\n",
        "    fs = 100.0 if len(dms) == 0 else 1000.0 / np.median(dms)\n",
        "\n",
        "    # Apply a high-pass filter to remove gravity and slow movements\n",
        "    b, a = butter(2, hp_hz/(fs/2), btype=\"highpass\")\n",
        "    axf = filtfilt(b, a, ax)\n",
        "    ayf = filtfilt(b, a, ay)\n",
        "    azf = filtfilt(b, a, az)\n",
        "    mag = np.sqrt(axf*axf + ayf*ayf + azf*azf) # Magnitude of filtered acceleration\n",
        "\n",
        "    # Compute RMS of magnitude in sliding windows\n",
        "    win = max(1, int(fs*win_sec))\n",
        "    hop = max(1, int(fs*hop_sec))\n",
        "    n = len(mag)\n",
        "    starts = np.arange(0, n-win+1, hop, dtype=int)\n",
        "    if len(starts) < 3:\n",
        "        return np.array([]), np.array([]), fs\n",
        "\n",
        "    idx = np.array([np.sqrt(np.mean(mag[s:s+win]**2)) for s in starts], dtype=float) # RMS\n",
        "    tt  = t[starts + win//2] # Time for the center of each window\n",
        "    return tt, idx, fs\n",
        "\n",
        "# ---------- Video Motion Energy via FFmpeg ----------\n",
        "def video_motion_energy_ffmpeg(video_path: str, t_start=0.0, t_end=None, fps=5, resize_w=160):\n",
        "    \"\"\"Extracts video motion energy using ffmpeg.\n",
        "    It extracts frames, converts to grayscale, and computes mean absolute frame difference.\n",
        "    \"\"\"\n",
        "    t_start = float(max(0.0, t_start))\n",
        "    dur_total = ffprobe_duration(video_path)\n",
        "    if dur_total is not None:\n",
        "        if t_end is None:\n",
        "            t_end = dur_total\n",
        "        t_end = float(min(t_end, dur_total))\n",
        "    else:\n",
        "        if t_end is None:\n",
        "            raise RuntimeError(\"Could not determine video duration; please provide t_end.\")\n",
        "        t_end = float(t_end)\n",
        "\n",
        "    if t_end <= t_start + (2.0/fps):\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    # Compute scaled height preserving aspect ratio for consistent processing\n",
        "    W0, H0 = ffprobe_wh(video_path)\n",
        "    if W0 <= 0 or H0 <= 0:\n",
        "        # Fallback if video dimensions cannot be determined\n",
        "        Hs = 90\n",
        "    else:\n",
        "        Hs = int(round(H0 * (resize_w / W0)))\n",
        "        Hs = max(2, 2*(Hs//2))  # Ensure even height for ffmpeg compatibility\n",
        "\n",
        "    duration = t_end - t_start\n",
        "\n",
        "    # FFmpeg command to extract grayscale frames at a specific FPS\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-v\",\"error\",\n",
        "        \"-ss\", f\"{t_start:.6f}\",\n",
        "        \"-t\",  f\"{duration:.6f}\",\n",
        "        \"-i\",  video_path,\n",
        "        \"-vf\", f\"fps={fps},scale={resize_w}:{Hs},format=gray\",\n",
        "        \"-f\",\"rawvideo\",\"-pix_fmt\",\"gray\",\"pipe:1\" # Output raw grayscale to stdout\n",
        "    ]\n",
        "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    raw = p.stdout.read()\n",
        "    p.stdout.close()\n",
        "    err = p.stderr.read().decode(\"utf-8\", errors=\"replace\")\n",
        "    p.stderr.close()\n",
        "    rc = p.wait()\n",
        "\n",
        "    if rc != 0 or len(raw) == 0:\n",
        "        raise RuntimeError(\"ffmpeg frame extraction failed.\\n\" + err[:1200])\n",
        "\n",
        "    frame_size = resize_w * Hs  # Size of one grayscale frame (bytes)\n",
        "    n_frames = len(raw) // frame_size\n",
        "    if n_frames < 3:\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    buf = np.frombuffer(raw[:n_frames*frame_size], dtype=np.uint8)\n",
        "    frames = buf.reshape((n_frames, Hs, resize_w)).astype(np.float32)\n",
        "\n",
        "    # Compute mean absolute frame difference as motion energy\n",
        "    diffs = np.mean(np.abs(frames[1:] - frames[:-1]), axis=(1,2))\n",
        "    times = t_start + (np.arange(1, n_frames) / float(fps))\n",
        "    return times.astype(float), diffs.astype(float)\n",
        "\n",
        "def zscore(x):\n",
        "    \"\"\"Standardizes data to have a mean of 0 and std dev of 1.\"\"\"\n",
        "    x = np.asarray(x, float)\n",
        "    m = np.isfinite(x)\n",
        "    if m.sum() < 5:\n",
        "        return x\n",
        "    mu = np.nanmean(x[m])\n",
        "    sd = np.nanstd(x[m]) + 1e-9 # Add small epsilon to prevent division by zero\n",
        "    return (x - mu) / sd\n",
        "\n",
        "def corr_at_offset(t_acc, x_acc, t_vid, y_vid, offset):\n",
        "    \"\"\"Calculates correlation between ACC movement and video motion at a given offset.\n",
        "    data_time = video_time + offset\n",
        "    \"\"\"\n",
        "    # Map video times to data times using the offset\n",
        "    td = t_vid + offset\n",
        "    # Select data points where video time overlaps with ACC data range\n",
        "    m = (td >= np.nanmin(t_acc)) & (td <= np.nanmax(t_acc)) & np.isfinite(td)\n",
        "    if m.sum() < 10:\n",
        "        return np.nan\n",
        "    # Interpolate ACC movement onto the shifted video times\n",
        "    xa = np.interp(td[m], t_acc, x_acc)\n",
        "    ya = y_vid[m]\n",
        "    # Z-score normalize for correlation calculation\n",
        "    xa = zscore(xa); ya = zscore(ya)\n",
        "    mm = np.isfinite(xa) & np.isfinite(ya)\n",
        "    if mm.sum() < 10:\n",
        "        return np.nan\n",
        "    return float(np.corrcoef(xa[mm], ya[mm])[0,1])\n",
        "\n",
        "def search_best_offset(t_acc, x_acc, t_vid, y_vid,\n",
        "                       offset_min, offset_max,\n",
        "                       coarse_step=0.5, fine_step=0.05, fine_window=3.0):\n",
        "    \"\"\"Searches for the best offset (highest correlation) between two time series.\n",
        "    Performs a coarse search, then a fine search around the best coarse result.\n",
        "    \"\"\"\n",
        "    # Coarse search\n",
        "    offsets = np.arange(offset_min, offset_max + 1e-9, coarse_step, dtype=float)\n",
        "    best = (None, -np.inf)\n",
        "    for off in offsets:\n",
        "        r = corr_at_offset(t_acc, x_acc, t_vid, y_vid, off)\n",
        "        if np.isfinite(r) and r > best[1]:\n",
        "            best = (off, r)\n",
        "\n",
        "    if best[0] is None:\n",
        "        return None, np.nan\n",
        "\n",
        "    # Fine search around the best coarse offset\n",
        "    c0 = best[0]\n",
        "    offsets2 = np.arange(c0 - fine_window, c0 + fine_window + 1e-9, fine_step, dtype=float)\n",
        "    best2 = (c0, best[1])\n",
        "    for off in offsets2:\n",
        "        if off < offset_min or off > offset_max:\n",
        "            continue\n",
        "        r = corr_at_offset(t_acc, x_acc, t_vid, y_vid, off)\n",
        "        if np.isfinite(r) and r > best2[1]:\n",
        "            best2 = (off, r)\n",
        "\n",
        "    return float(best2[0]), float(best2[1])\n",
        "\n",
        "# ---------------- UI Widget Definitions ----------------\n",
        "ui_out = widgets.Output() # Widget for displaying output messages and plots\n",
        "\n",
        "# Radio buttons to choose source (Google Drive or Upload)\n",
        "mode = widgets.RadioButtons(\n",
        "    options=[(\"Google Drive\", \"drive\"), (\"Upload from computer\", \"upload\")],\n",
        "    value=\"drive\",\n",
        "    description=\"Source:\"\n",
        ")\n",
        "\n",
        "# Google Drive related buttons and input\n",
        "mount_btn = widgets.Button(description=\"Mount Google Drive\", button_style=\"info\")\n",
        "scan_btn  = widgets.Button(description=\"Scan Drive Folder\", button_style=\"primary\")\n",
        "\n",
        "drive_folder = widgets.Text(\n",
        "    value=\"/content/drive/MyDrive/\",\n",
        "    description=\"Drive folder:\",\n",
        "    layout=widgets.Layout(width=\"700px\")\n",
        ")\n",
        "# Option to copy video locally for faster processing\n",
        "copy_local = widgets.Checkbox(value=True, description=\"Copy video to /content for speed\")\n",
        "\n",
        "# Dropdowns for selecting HR, ACC, and Video files (shared for both modes)\n",
        "hr_dd  = widgets.Dropdown(options=[], description=\"HR file:\", layout=widgets.Layout(width=\"700px\"))\n",
        "acc_dd = widgets.Dropdown(options=[], description=\"ACC file:\", layout=widgets.Layout(width=\"700px\"))\n",
        "vid_dd = widgets.Dropdown(options=[], description=\"Video file:\", layout=widgets.Layout(width=\"700px\"))\n",
        "\n",
        "# Upload button for computer upload mode\n",
        "upload_btn = widgets.Button(description=\"Upload HR/ACC/Video\", button_style=\"primary\")\n",
        "\n",
        "# Button to load selected files\n",
        "load_btn  = widgets.Button(description=\"Load selected files\", button_style=\"success\")\n",
        "\n",
        "# Sync parameter widgets\n",
        "sync_box = widgets.VBox([]) # Container for sync-related input widgets\n",
        "sync_btn = widgets.Button(description=\"Compute offset from windows\", button_style=\"warning\")\n",
        "win_box = widgets.VBox([]) # Container for analysis window input widgets\n",
        "savewin_btn = widgets.Button(description=\"Save analysis window\", button_style=\"success\")\n",
        "\n",
        "# Numeric input fields for sync windows\n",
        "v0_w = widgets.FloatText(value=0.0, description=\"Video start (s):\")\n",
        "vd_w = widgets.FloatText(value=20.0, description=\"Video dur (s):\")\n",
        "d0_w = widgets.FloatText(value=0.0, description=\"Data start (s):\")\n",
        "dd_w = widgets.FloatText(value=20.0, description=\"Data dur (s):\")\n",
        "slack_w = widgets.FloatText(value=10.0, description=\"Slack (s):\") # Search range for offset\n",
        "tweak_w = widgets.FloatText(value=0.0, description=\"Tweak (s):\") # Manual adjustment to offset\n",
        "\n",
        "# Analysis window input fields\n",
        "t0_w = widgets.FloatText(value=0.0, description=\"Analysis start (data s):\")\n",
        "t1_w = widgets.FloatText(value=300.0, description=\"Analysis stop (data s):\")\n",
        "\n",
        "# ---------------- UI Action Callbacks ----------------\n",
        "\n",
        "def mount_drive(_):\n",
        "    \"\"\"Callback to mount Google Drive.\"\"\"\n",
        "    with ui_out:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    with ui_out:\n",
        "        print(\"Drive mounted at /content/drive\")\n",
        "\n",
        "def scan_drive(_):\n",
        "    \"\"\"Callback to scan the selected Google Drive folder for HR, ACC, and video files.\"\"\"\n",
        "    folder = drive_folder.value.strip()\n",
        "    p = Path(folder)\n",
        "    with ui_out:\n",
        "        clear_output()\n",
        "        if not p.exists():\n",
        "            print(\"Folder not found:\", folder)\n",
        "            return\n",
        "\n",
        "        # Scan for likely candidate files based on naming conventions and extensions\n",
        "        names = [str(x) for x in sorted(p.glob(\"*\")) if x.is_file()]\n",
        "        hr_cands  = [n for n in names if n.lower().endswith(\".txt\") and \"hr\" in Path(n).name.lower()]\n",
        "        acc_cands = [n for n in names if n.lower().endswith(\".txt\") and \"acc\" in Path(n).name.lower()]\n",
        "        vid_cands = [n for n in names if Path(n).suffix.lower() in VIDEO_EXTS]\n",
        "\n",
        "        # If strict matching fails, offer broader txt list for HR/ACC\n",
        "        if not hr_cands:\n",
        "            hr_cands = [n for n in names if n.lower().endswith(\".txt\")]\n",
        "        if not acc_cands:\n",
        "            acc_cands = [n for n in names if n.lower().endswith(\".txt\")]\n",
        "\n",
        "        # Update dropdown options\n",
        "        hr_dd.options  = hr_cands\n",
        "        acc_dd.options = acc_cands\n",
        "        vid_dd.options = vid_cands\n",
        "\n",
        "        print(f\"Scanned: {folder}\")\n",
        "        print(f\"  HR candidates:  {len(hr_cands)}\")\n",
        "        print(f\"  ACC candidates: {len(acc_cands)}\")\n",
        "        print(f\"  Video candidates:{len(vid_cands)}\")\n",
        "        if len(hr_cands)==0 or len(acc_cands)==0 or len(vid_cands)==0:\n",
        "            print(\"If lists are empty, point 'Drive folder' to the directory containing the files.\")\n",
        "\n",
        "def do_upload(_):\n",
        "    \"\"\"Callback for uploading files from the local computer.\"\"\"\n",
        "    with ui_out:\n",
        "        clear_output()\n",
        "        print(\"Upload dialog should appear below. Select HR*.txt, ACC*.txt, and the video file.\")\n",
        "    from google.colab import files\n",
        "    up = files.upload()\n",
        "    if not up:\n",
        "        with ui_out:\n",
        "            print(\"No files uploaded.\")\n",
        "        return\n",
        "\n",
        "    names = list(up.keys())\n",
        "    hr_name, acc_name, vid_name = auto_detect_names(names) # Try to auto-detect files\n",
        "\n",
        "    # Fallback if auto-detection is not perfect, populate dropdowns for manual selection\n",
        "    if hr_name is None or acc_name is None or vid_name is None:\n",
        "        txts = [n for n in names if n.lower().endswith(\".txt\")]\n",
        "        vids = [n for n in names if n.lower().endswith(VIDEO_EXTS)]\n",
        "        hr_dd.options  = txts\n",
        "        acc_dd.options = txts\n",
        "        vid_dd.options = vids\n",
        "        with ui_out:\n",
        "            print(\"Auto-detect failed. Pick files in the dropdowns, then click 'Load selected files'.\")\n",
        "        EQUIPHY[\"_uploaded_dict\"] = up # Store uploaded bytes temporarily\n",
        "        return\n",
        "\n",
        "    # If auto-detection succeeded, pre-select in dropdowns\n",
        "    EQUIPHY[\"_uploaded_dict\"] = up\n",
        "    hr_dd.options  = [hr_name]\n",
        "    acc_dd.options = [acc_name]\n",
        "    vid_dd.options = [vid_name]\n",
        "    hr_dd.value = hr_name\n",
        "    acc_dd.value = acc_name\n",
        "    vid_dd.value = vid_name\n",
        "\n",
        "    with ui_out:\n",
        "        print(\"Uploaded and auto-detected:\")\n",
        "        print(\"  HR :\", hr_name)\n",
        "        print(\"  ACC:\", acc_name)\n",
        "        print(\"  VID:\", vid_name)\n",
        "        print(\"Now click 'Load selected files'.\")\n",
        "\n",
        "def load_files(_):\n",
        "    \"\"\"Callback to load the selected HR, ACC, and video files, process them, and plot ACC movement.\"\"\"\n",
        "    with ui_out:\n",
        "        clear_output()\n",
        "        print(\"Loading selected files...\")\n",
        "\n",
        "    src = mode.value\n",
        "\n",
        "    if src == \"drive\":\n",
        "        hr_path = Path(hr_dd.value)\n",
        "        acc_path = Path(acc_dd.value)\n",
        "        vid_path = Path(vid_dd.value)\n",
        "\n",
        "        if not hr_path.exists() or not acc_path.exists() or not vid_path.exists():\n",
        "            with ui_out:\n",
        "                print(\"One or more selected paths do not exist.\")\n",
        "            return\n",
        "\n",
        "        EQUIPHY[\"hr_name\"] = hr_path.name\n",
        "        EQUIPHY[\"acc_name\"] = acc_path.name\n",
        "        EQUIPHY[\"video_name\"] = vid_path.name\n",
        "\n",
        "        EQUIPHY[\"hr_bytes\"]  = hr_path.read_bytes()\n",
        "        EQUIPHY[\"acc_bytes\"] = acc_path.read_bytes()\n",
        "\n",
        "        # Copy video to /content for potentially faster access in Colab\n",
        "        if copy_local.value:\n",
        "            local_vid = Path(\"/content\") / vid_path.name\n",
        "            if not local_vid.exists():\n",
        "                shutil.copy2(str(vid_path), str(local_vid))\n",
        "            EQUIPHY[\"video_path\"] = str(local_vid)\n",
        "            EQUIPHY[\"video_path_original\"] = str(vid_path)\n",
        "        else:\n",
        "            EQUIPHY[\"video_path\"] = str(vid_path)\n",
        "            EQUIPHY[\"video_path_original\"] = str(vid_path)\n",
        "\n",
        "    else:  # Upload mode\n",
        "        up = EQUIPHY.get(\"_uploaded_dict\", None)\n",
        "        if up is None:\n",
        "            with ui_out:\n",
        "                print(\"No uploaded files found. Click 'Upload HR/ACC/Video' first.\")\n",
        "            return\n",
        "\n",
        "        EQUIPHY[\"hr_name\"] = Path(hr_dd.value).name\n",
        "        EQUIPHY[\"acc_name\"] = Path(acc_dd.value).name\n",
        "        EQUIPHY[\"video_name\"] = Path(vid_dd.value).name\n",
        "\n",
        "        EQUIPHY[\"hr_bytes\"]  = up[hr_dd.value]\n",
        "        EQUIPHY[\"acc_bytes\"] = up[acc_dd.value]\n",
        "        vid_bytes = up[vid_dd.value]\n",
        "\n",
        "        # Write uploaded video bytes to a local file for ffmpeg/ffprobe\n",
        "        local_vid = Path(\"/content\") / Path(vid_dd.value).name\n",
        "        with open(local_vid, \"wb\") as f:\n",
        "            f.write(vid_bytes)\n",
        "        EQUIPHY[\"video_path\"] = str(local_vid)\n",
        "        EQUIPHY[\"video_path_original\"] = None # Original path not relevant for uploaded files\n",
        "\n",
        "    # Parse HR and ACC dataframes\n",
        "    df_hr  = read_polar_txt_bytes(EQUIPHY[\"hr_bytes\"])\n",
        "    df_acc = read_polar_txt_bytes(EQUIPHY[\"acc_bytes\"])\n",
        "\n",
        "    # Determine common starting MS timestamp (ms0) and total data duration\n",
        "    ms0 = float(np.nanmin([df_hr[\"MS\"].min(), df_acc[\"MS\"].min()]))\n",
        "    data_end = float(np.nanmax([df_hr[\"MS\"].max(), df_acc[\"MS\"].max()]) - ms0) / 1000.0\n",
        "\n",
        "    EQUIPHY[\"ms0\"] = ms0\n",
        "    EQUIPHY[\"data_duration_s\"] = data_end\n",
        "\n",
        "    # Get video duration using ffprobe\n",
        "    vid_dur = ffprobe_duration(EQUIPHY[\"video_path\"])\n",
        "    EQUIPHY[\"video_duration_s\"] = vid_dur\n",
        "\n",
        "    # Compute ACC movement proxy\n",
        "    t_acc, mov_acc, fs_acc = acc_movement_index(df_acc, ms0, hp_hz=0.3, win_sec=0.5, hop_sec=0.1)\n",
        "    if len(t_acc) < 20:\n",
        "        raise RuntimeError(\"ACC movement series too short to sync. Check ACC columns/data.\")\n",
        "\n",
        "    # Store important metrics in EQUIPHY\n",
        "    EQUIPHY[\"_df_hr_rows\"] = len(df_hr)\n",
        "    EQUIPHY[\"_df_acc_rows\"] = len(df_acc)\n",
        "    EQUIPHY[\"_t_acc\"] = t_acc\n",
        "    EQUIPHY[\"_mov_acc\"] = mov_acc\n",
        "    EQUIPHY[\"_fs_acc\"] = fs_acc\n",
        "\n",
        "    # Plot ACC movement proxy for user to estimate sync window\n",
        "    with ui_out:\n",
        "        clear_output()\n",
        "        print(\"Loaded ✅\")\n",
        "        print(\"  HR   :\", EQUIPHY[\"hr_name\"], f\"(rows={len(df_hr)})\")\n",
        "        print(\"  ACC  :\", EQUIPHY[\"acc_name\"], f\"(rows={len(df_acc)}, fs≈{fs_acc:.1f} Hz)\")\n",
        "        print(\"  VIDEO:\", EQUIPHY[\"video_name\"])\n",
        "        print(\"  Data duration (s):\", f\"{data_end:.2f}\")\n",
        "        print(\"  Video duration (s):\", \"unknown\" if vid_dur is None else f\"{vid_dur:.2f}\")\n",
        "        print(\"\\nMovement proxy plot (use this to estimate a matching DATA window):\")\n",
        "\n",
        "        plt.figure(figsize=(12,4))\n",
        "        plt.plot(t_acc, mov_acc, linewidth=1.0)\n",
        "        plt.xlabel(\"DATA time (s since Polar start)\")\n",
        "        plt.ylabel(\"Movement proxy (g RMS, high-pass)\")\n",
        "        plt.title(\"ACC movement proxy vs time\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    # Set default values for sync window inputs\n",
        "    v0_w.value = 0.0\n",
        "    vd_w.value = 20.0\n",
        "    d0_w.value = 0.0\n",
        "    dd_w.value = 20.0\n",
        "\n",
        "    # Display sync window input widgets\n",
        "    sync_box.children = [\n",
        "        widgets.HTML(\"<h3>Step 2: Enter sync windows</h3>\"\n",
        "                     \"<p><b>VIDEO</b>: pick a segment with clear horse motion + minimal camera motion.<br>\"\n",
        "                     \"<b>DATA</b>: use the movement proxy plot to guess the matching time window.</p>\"\n",
        "                     \"<p><b>Slack (s)</b>: This defines how much wider than your initial guess the automatic search for the best offset will be. A larger slack value allows for a broader search if your initial sync window guess is slightly off.</p>\"\n",
        "                     \"<p><b>Tweak (s)</b>: A manual adjustment to the automatically computed offset. Use this if you visually inspect the result and feel a small correction is needed.</p>\"),\n",
        "        widgets.HBox([v0_w, vd_w]),\n",
        "        widgets.HBox([d0_w, dd_w]),\n",
        "        widgets.HBox([slack_w, tweak_w]),\n",
        "        sync_btn\n",
        "    ]\n",
        "\n",
        "def compute_offset(_):\n",
        "    \"\"\"Callback to compute the optimal time offset between video and data based on user-defined windows.\"\"\"\n",
        "    with ui_out:\n",
        "        print(\"\\nComputing offset...\")\n",
        "\n",
        "    video_path = EQUIPHY[\"video_path\"]\n",
        "    t_acc = EQUIPHY[\"_t_acc\"]\n",
        "    mov_acc = EQUIPHY[\"_mov_acc\"]\n",
        "    data_end = float(EQUIPHY[\"data_duration_s\"])\n",
        "    vid_dur = EQUIPHY.get(\"video_duration_s\", None)\n",
        "\n",
        "    # Get user-defined sync window values\n",
        "    v_sync_start = float(v0_w.value)\n",
        "    v_sync_end   = float(v0_w.value + vd_w.value)\n",
        "\n",
        "    # Clamp video sync window to video duration if known\n",
        "    if vid_dur is not None:\n",
        "        v_sync_start = max(0.0, min(v_sync_start, vid_dur-0.1))\n",
        "        v_sync_end   = max(v_sync_start+0.1, min(v_sync_end, vid_dur))\n",
        "\n",
        "    d_sync_start = float(d0_w.value)\n",
        "    d_sync_end   = float(d0_w.value + dd_w.value)\n",
        "    # Clamp data sync window to data duration\n",
        "    d_sync_start = max(0.0, min(d_sync_start, data_end-0.1))\n",
        "    d_sync_end   = max(d_sync_start+0.1, min(d_sync_end, data_end))\n",
        "\n",
        "    # Extract video motion energy for the specified video sync window\n",
        "    t_vid, mot = video_motion_energy_ffmpeg(\n",
        "        video_path, t_start=v_sync_start, t_end=v_sync_end, fps=5, resize_w=160\n",
        "    )\n",
        "    if len(t_vid) < 20:\n",
        "        raise RuntimeError(\"Video motion series too short. Increase duration or pick a different segment.\")\n",
        "    mot_z = zscore(mot) # Z-score normalize video motion\n",
        "\n",
        "    # Define the search range for the offset based on the sync windows and slack\n",
        "    center_off = 0.5*((d_sync_start - v_sync_start) + (d_sync_end - v_sync_end))\n",
        "    span_off   = max(abs((d_sync_start - v_sync_start) - center_off),\n",
        "                     abs((d_sync_end   - v_sync_end) - center_off))\n",
        "    slack = float(slack_w.value)\n",
        "    guess = center_off\n",
        "    rng   = max(5.0, span_off + slack)\n",
        "\n",
        "    # Feasibility bounds for the offset (ensuring data overlap)\n",
        "    offset_min_feas = 0.0 - float(np.max(t_vid))\n",
        "    offset_max_feas = float(data_end) - float(np.min(t_vid))\n",
        "    o_min = max(offset_min_feas, guess - rng)\n",
        "    o_max = min(offset_max_feas, guess + rng)\n",
        "\n",
        "    # Search for the best offset\n",
        "    best_off, best_r = search_best_offset(\n",
        "        t_acc, mov_acc, t_vid, mot_z,\n",
        "        offset_min=o_min, offset_max=o_max,\n",
        "        coarse_step=0.5, fine_step=0.05, fine_window=3.0\n",
        "    )\n",
        "    if best_off is None or not np.isfinite(best_r):\n",
        "        raise RuntimeError(\"Could not find a stable offset. Try a different sync segment or widen slack.\")\n",
        "\n",
        "    # Apply manual tweak if provided\n",
        "    best_off = float(best_off + float(tweak_w.value))\n",
        "\n",
        "    # Store computed offset and sync details in EQUIPHY\n",
        "    EQUIPHY[\"video_to_data_offset_s\"] = best_off\n",
        "    EQUIPHY[\"audio_offset_s\"] = best_off # Audio offset is same as video\n",
        "    EQUIPHY[\"sync_video_segment\"] = (float(v_sync_start), float(v_sync_end))\n",
        "    EQUIPHY[\"sync_data_segment_guess\"] = (float(d_sync_start), float(d_sync_end))\n",
        "    EQUIPHY[\"sync_corr\"] = float(best_r)\n",
        "\n",
        "    # Diagnostic overlay plot to visualize alignment\n",
        "    td = t_vid + best_off\n",
        "    m = (td >= np.nanmin(t_acc)) & (td <= np.nanmax(t_acc))\n",
        "    acc_on_vid = np.interp(td[m], t_acc, mov_acc)\n",
        "\n",
        "    with ui_out:\n",
        "        print(f\"\\n✅ Best offset: {best_off:+.3f} s  (corr={best_r:.3f})\")\n",
        "        print(\"Offset convention: data_time = video_time + offset\")\n",
        "        print(\"\\nDiagnostic overlay (z-scored):\")\n",
        "\n",
        "        plt.figure(figsize=(12,4))\n",
        "        plt.plot(td[m], zscore(acc_on_vid), label=\"ACC movement (z)\", linewidth=1.2)\n",
        "        plt.plot(td[m], zscore(mot_z[m]),   label=\"Video motion (z)\", linewidth=1.2)\n",
        "        plt.axvspan(d_sync_start, d_sync_end, alpha=0.15, label=\"your DATA-window guess\")\n",
        "        plt.xlabel(\"DATA time (s)\")\n",
        "        plt.title(f\"Overlay after alignment (corr={best_r:.3f})\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    # Extract FULL audio from the original video using the full duration\n",
        "    with ui_out:\n",
        "        print(\"\\nExtracting full audio -> extracted_audio.wav ...\")\n",
        "    wav_path = extract_audio_wav(video_path, out_wav=\"extracted_audio.wav\", sr=44100)\n",
        "    if wav_path is None or (not os.path.exists(wav_path)):\n",
        "        with ui_out:\n",
        "            print(\"⚠️ No extracted audio WAV produced.\")\n",
        "        EQUIPHY[\"aud_name\"] = None\n",
        "        EQUIPHY[\"aud_path\"] = None\n",
        "        EQUIPHY[\"aud_bytes\"] = None\n",
        "        EQUIPHY[\"aud_sr\"] = None\n",
        "    else:\n",
        "        EQUIPHY[\"aud_name\"] = Path(wav_path).name\n",
        "        EQUIPHY[\"aud_path\"] = str(Path(wav_path).resolve())\n",
        "        EQUIPHY[\"aud_bytes\"] = Path(wav_path).read_bytes()\n",
        "        EQUIPHY[\"aud_sr\"] = 44100\n",
        "        with ui_out:\n",
        "            print(\"✅ Extracted audio:\", EQUIPHY[\"aud_name\"], f\"(sr={EQUIPHY['aud_sr']})\")\n",
        "\n",
        "    # Set default analysis window based on computed offset and durations\n",
        "    if vid_dur is None:\n",
        "        t_start_default = max(0.0, best_off)\n",
        "        t_stop_default  = min(data_end, t_start_default + 300.0)\n",
        "    else:\n",
        "        t_start_default = float(np.clip(best_off, 0.0, data_end))\n",
        "        t_stop_default  = min(data_end, t_start_default + 300.0, best_off + vid_dur)\n",
        "\n",
        "    t0_w.value = float(t_start_default)\n",
        "    t1_w.value = float(t_stop_default)\n",
        "\n",
        "    # Display analysis window input widgets\n",
        "    win_box.children = [\n",
        "        widgets.HTML(\"<h3>Step 3: Choose analysis window (DATA time)</h3>\"),\n",
        "        widgets.HBox([t0_w, t1_w]),\n",
        "        savewin_btn\n",
        "    ]\n",
        "\n",
        "def save_window(_):\n",
        "    \"\"\"Callback to save the final analysis window and mark EQUIPHY as ready.\"\"\"\n",
        "    t0 = float(t0_w.value)\n",
        "    t1 = float(t1_w.value)\n",
        "    if t1 <= t0:\n",
        "        raise ValueError(\"Stop time must be > start time.\")\n",
        "    EQUIPHY[\"t_start_s\"] = t0\n",
        "    EQUIPHY[\"t_stop_s\"] = t1\n",
        "    EQUIPHY[\"ready\"] = True # Indicate that EQUIPHY is prepared for downstream cells\n",
        "    with ui_out:\n",
        "        print(\"\\nSaved ✅ EQUIPHY is ready for Cell 2\")\n",
        "        print(f\"  video_to_data_offset_s: {EQUIPHY.get('video_to_data_offset_s', np.nan):+.3f}\")\n",
        "        print(f\"  Analysis window (data s): [{t0:.2f}, {t1:.2f}]\")\n",
        "        print(\"\\nNext: Run Cell 2 (timestamp-free / ms0-based) and compute series.\")\n",
        "        print(\"Reminder: in Cell 2, use t_s = (MS - EQUIPHY['ms0'])/1000.0\")\n",
        "\n",
        "# --- Wire Buttons to Callbacks ---\n",
        "mount_btn.on_click(mount_drive)\n",
        "scan_btn.on_click(scan_drive)\n",
        "upload_btn.on_click(do_upload)\n",
        "load_btn.on_click(load_files)\n",
        "sync_btn.on_click(compute_offset)\n",
        "savewin_btn.on_click(save_window)\n",
        "\n",
        "# --- UI elements specific to Google Drive mode ---\n",
        "drive_controls_ui = widgets.VBox([\n",
        "    widgets.HBox([mount_btn]),\n",
        "    drive_folder,\n",
        "    widgets.HBox([scan_btn, copy_local]),\n",
        "])\n",
        "\n",
        "# --- UI elements specific to Upload from computer mode ---\n",
        "upload_controls_ui = widgets.VBox([\n",
        "    upload_btn,\n",
        "])\n",
        "\n",
        "source_box = widgets.VBox([mode])\n",
        "\n",
        "def _toggle_ui(*args):\n",
        "    \"\"\"Toggles the visibility of Drive UI vs. Upload UI based on selected mode.\"\"\"\n",
        "    if mode.value == \"drive\":\n",
        "        drive_controls_ui.layout.display = \"flex\" # Show Drive UI\n",
        "        upload_controls_ui.layout.display = \"none\" # Hide Upload UI\n",
        "    else: # mode.value == \"upload\"\n",
        "        drive_controls_ui.layout.display = \"none\" # Hide Drive UI\n",
        "        upload_controls_ui.layout.display = \"flex\" # Show Upload UI\n",
        "\n",
        "# Set initial visibility and observe changes to the mode radio buttons\n",
        "_toggle_ui() # Apply initial state\n",
        "mode.observe(_toggle_ui, names=\"value\") # Update on radio button change\n",
        "\n",
        "# Define the header widget\n",
        "header = widgets.HTML(\"<h2>EquiPhysics Data Sync</h2>\")\n",
        "\n",
        "# Main UI composition\n",
        "ui = widgets.VBox([\n",
        "    header,\n",
        "    source_box,\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    widgets.HTML(\"<h3>Choose files</h3>\"),\n",
        "    # Mode-specific controls (visibility toggled)\n",
        "    drive_controls_ui,\n",
        "    upload_controls_ui,\n",
        "    # Shared file selection dropdowns\n",
        "    hr_dd, acc_dd, vid_dd,\n",
        "    widgets.HBox([load_btn]),\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    sync_box,\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    win_box,\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    ui_out\n",
        "])\n",
        "\n",
        "display(ui)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09a90ed0"
      },
      "source": [
        "## Alignment Notes\n",
        "\n",
        "Use this section to record your observations and assessments of the video and data alignment. This context will be crucial for interpreting subsequent analysis results.\n",
        "\n",
        "---\n",
        "\n",
        "**Overall Assessment of Alignment:** [e.g., Good, acceptable, fair, poor]\n",
        "\n",
        "**Visual Inspection Feedback:** [e.g., Did the movement in the video visually match the peaks in the ACC movement proxy? Was the synchronization diagnostic plot clear?]\n",
        "\n",
        "**Any Concerns or Anomalies:** [e.g., Were there any periods where the alignment seemed off? Did the 'slack' parameter need to be adjusted significantly?]\n",
        "\n",
        "**Suggestions for Re-alignment (if any):** [e.g., Should a different sync window be chosen for the video/data?]\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7438f719"
      },
      "source": [
        "## Cell 2: Feature Extraction & Export\n",
        "\n",
        "**Purpose:** This cell processes the loaded HR, ACC, and audio data to compute various physiological and movement metrics. It then prepares these metrics for export, along with metadata and a diagnostic plot.\n",
        "\n",
        "**What it does:**\n",
        "*   **Reads Data:** Accesses the raw HR and ACC data loaded in Cell 1, as well as any extracted audio.\n",
        "*   **Computes Metrics:** Calculates:\n",
        "    *   Instantaneous Heart Rate (HR)\n",
        "    *   Rolling Root Mean Square of Successive Differences (RMSSD) as a Heart Rate Variability (HRV) metric.\n",
        "    *   Accelerometer (ACC) Movement Index.\n",
        "    *   Audio Loudness, Spectral Flux (a beat proxy), and Spectral Centroid (a frequency proxy) if audio is available.\n",
        "*   **Applies Analysis Window:** Filters all computed metrics to the `Analysis start` and `Analysis stop` times defined in Cell 1.\n",
        "*   **Generates Diagnostic Plot:** Creates a multi-panel plot visualizing all computed metrics for quick review.\n",
        "*   **Exports Data:** Saves all processed time series data as a compressed NumPy `.npz` file, individual `.csv` files for easy inspection, a `meta.json` file containing run details and settings, and a `.png` image of the diagnostic plot to a structured folder in your Google Drive.\n",
        "\n",
        "**How to use it:**\n",
        "1.  **Ensure Cell 1 is complete:** Make sure you have successfully loaded your files, computed the offset, and saved the analysis window in Cell 1. The `EQUIPHY['ready']` flag should be `True`.\n",
        "2.  **Review `USER SETTINGS`:** Adjust parameters like `SAVE_TO_DRIVE`, `DRIVE_BASE_DIR`, `RMSSD_WINDOW_BEATS`, etc., at the beginning of the cell to match your preferences.\n",
        "3.  **Run the cell:** Execute this cell. It will process the data, display the diagnostic plot, and print messages indicating where your exported files are saved in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeNKWJH0-Wzn",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ==========================================================\n",
        "# Cell 2 (EXPORT + SAVE TO GOOGLE DRIVE)\n",
        "# Instant HR + Rolling RMSSD + Movement + Audio/music proxies\n",
        "#\n",
        "# This cell processes the loaded HR, ACC, and (optionally) audio data.\n",
        "# It computes various physiological and movement metrics over the defined\n",
        "# analysis window and prepares them for export. It also generates a\n",
        "# diagnostic plot and saves all processed data and metadata to Google Drive.\n",
        "#\n",
        "# Requires:\n",
        "#   - Cell 1 already ran and set EQUIPHY[\"ready\"]=True and:\n",
        "#   - EQUIPHY[\"hr_bytes\"], [\"acc_bytes\"], [\"ms0\"] (raw data and common start time)\n",
        "#   - EQUIPHY[\"t_start_s\"], [\"t_stop_s\"] (the analysis window in data time)\n",
        "#   - EQUIPHY[\"audio_offset_s\"] (the video/audio to data alignment offset)\n",
        "#   - (optional) EQUIPHY[\"aud_bytes\"] or extracted_audio.wav present for audio analysis\n",
        "#\n",
        "# Outputs:\n",
        "#   - EQUIPHY[\"series\"] populated (a dictionary containing all computed time series)\n",
        "#   - Saves a folder to Google Drive containing:\n",
        "#       meta.json  (small metadata + settings of the run)\n",
        "#       series_arrays.npz (all computed time series as NumPy arrays)\n",
        "#       beats.csv / movement.csv / audio_*.csv (optional, for quick checks)\n",
        "#       preview.png (the 6-panel diagnostic plot)\n",
        "# ==========================================================\n",
        "\n",
        "import os, io, json, time, platform\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt, stft, resample_poly\n",
        "\n",
        "# ---------------- USER SETTINGS ----------------\n",
        "# Configure saving options and output directories\n",
        "SAVE_TO_DRIVE = True # Set to False to prevent saving anything to Drive\n",
        "DRIVE_BASE_DIR = \"/content/drive/MyDrive/EquiPhysics/exports\"  # Base directory for exports on Drive\n",
        "EXPORT_CSVS = True       # Set to True to export data into individual CSV files\n",
        "EXPORT_PLOT_PNG = True   # Set to True to save the diagnostic plot as a PNG image\n",
        "RUN_TAG = None           # Custom tag for the export folder. If None, an auto-generated name is used.\n",
        "\n",
        "# --------- (optional) force inline plotting ----------\n",
        "# Ensures matplotlib plots are displayed within the notebook output.\n",
        "try:\n",
        "    from IPython import get_ipython\n",
        "    get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# --------- processing knobs ----------\n",
        "# These parameters control how the various metrics are calculated.\n",
        "RMSSD_WINDOW_BEATS = 30        # Window size in beats for calculating rolling RMSSD (e.g., 20–50 beats)\n",
        "DETREND_RR = True              # If True, removes slow trends from RR intervals before RMSSD calculation\n",
        "RR_TREND_ALPHA = 0.02          # Alpha parameter for exponential moving average detrending of RR intervals\n",
        "\n",
        "AUDIO_DOWNSAMPLE_TO = 11025    # Target sample rate for audio processing (e.g., for beat/frequency proxies)\n",
        "LOUD_WIN_SEC, LOUD_HOP_SEC = 0.5, 0.1 # Window and hop size for audio loudness calculation (in seconds)\n",
        "\n",
        "ACC_HP_HZ = 0.3                # High-pass filter frequency for accelerometer data (removes gravity/slow motion)\n",
        "ACC_WIN_SEC, ACC_HOP_SEC = 1.0, 0.2 # Window and hop size for ACC movement index calculation (in seconds)\n",
        "\n",
        "# --------- helper functions ----------\n",
        "# These functions are internal to Cell 2 and assist with data processing.\n",
        "\n",
        "def _read_polar_txt_bytes(file_bytes: bytes) -> pd.DataFrame:\n",
        "    \"\"\"Reads Polar HR/ACC data from byte content, handling common Polar file formats.\"\"\"\n",
        "    raw = file_bytes.replace(b\"\\x00\", b\"\")\n",
        "    lines = raw.decode(\"utf-8\", errors=\"replace\").splitlines()\n",
        "\n",
        "    data_start = None\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.startswith(\"#\") or line.strip() == \"\":\n",
        "            continue\n",
        "        data_start = i\n",
        "        break\n",
        "    if data_start is None:\n",
        "        raise ValueError(\"Could not find data rows (file looks empty or only comments).\")\n",
        "\n",
        "    df = pd.read_csv(io.StringIO(\"\\n\".join(lines[data_start:])), skipinitialspace=True)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    for col in [\"MS\",\"HR\",\"RR\",\"SC\",\"ACCX\",\"ACCY\",\"ACCZ\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    df = df.dropna(subset=[\"MS\"]).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def _read_audio_bytes(file_bytes: bytes, filename: str):\n",
        "    \"\"\"Reads audio data from byte content, attempting with soundfile, falling back to scipy.\"\"\"\n",
        "    # soundfile is nice; fallback to scipy wavfile\n",
        "    try:\n",
        "        import soundfile as sf\n",
        "        y, sr = sf.read(io.BytesIO(file_bytes))\n",
        "        if y.ndim == 2:\n",
        "            y = y.mean(axis=1)\n",
        "        return y.astype(np.float32), int(sr)\n",
        "    except Exception:\n",
        "        from scipy.io import wavfile\n",
        "        sr, y = wavfile.read(io.BytesIO(file_bytes))\n",
        "        y = y.astype(np.float32)\n",
        "        if y.ndim == 2:\n",
        "            y = y.mean(axis=1)\n",
        "        m = np.max(np.abs(y)) if len(y) else 1.0\n",
        "        if m > 1.5:\n",
        "            y = y / m\n",
        "        return y, int(sr)\n",
        "\n",
        "def _moving_average(x, k=7):\n",
        "    \"\"\"Calculates a simple moving average of an array.\"\"\"\n",
        "    k = int(max(1, k))\n",
        "    if k == 1:\n",
        "        return x\n",
        "    w = np.ones(k) / k\n",
        "    return np.convolve(x, w, mode=\"same\")\n",
        "\n",
        "def ew_mean(x, alpha=0.02):\n",
        "    \"\"\"Calculates an exponential weighted moving average (EWM) for detrending.\"\"\"\n",
        "    x = np.asarray(x, float)\n",
        "    out = np.full_like(x, np.nan, dtype=float)\n",
        "    m = np.isfinite(x)\n",
        "    if not m.any():\n",
        "        return out\n",
        "    y = x[m]\n",
        "    ew = np.empty_like(y, dtype=float)\n",
        "    ew[0] = y[0]\n",
        "    for i in range(1, len(y)):\n",
        "        ew[i] = alpha*y[i] + (1-alpha)*ew[i-1]\n",
        "    out[m] = ew\n",
        "    return out\n",
        "\n",
        "def rolling_rmssd(rr_ms, window_beats=30):\n",
        "    \"\"\"Computes the Root Mean Square of Successive Differences (RMSSD) of RR intervals.\"\"\"\n",
        "    rr = pd.Series(rr_ms, dtype=\"float64\")\n",
        "    drr2 = rr.diff().pow(2) # Squared differences between successive RR intervals\n",
        "    w = max(3, int(window_beats))\n",
        "    rmssd = np.sqrt(drr2.rolling(window=w-1, min_periods=w-1).mean())\n",
        "    return rmssd.to_numpy()\n",
        "\n",
        "def audio_loudness_db(y, sr, win_sec=0.5, hop_sec=0.1, eps=1e-8):\n",
        "    \"\"\"Calculates audio loudness in dB over sliding windows.\"\"\"\n",
        "    win = max(1, int(sr * win_sec))\n",
        "    hop = max(1, int(sr * hop_sec))\n",
        "    n = len(y)\n",
        "    n_frames = 1 + max(0, (n - win) // hop)\n",
        "\n",
        "    t = np.zeros(n_frames, dtype=float)\n",
        "    db = np.zeros(n_frames, dtype=float)\n",
        "    for i in range(n_frames):\n",
        "        s = i * hop\n",
        "        frame = y[s:s+win]\n",
        "        rms = np.sqrt(np.mean(frame * frame) + eps)\n",
        "        db[i] = 20 * np.log10(rms + eps)\n",
        "        t[i] = (s + win/2) / sr\n",
        "    return t, db\n",
        "\n",
        "def audio_beat_and_freq_proxies(y, sr, downsample_to=11025, nperseg=2048, hop=512, eps=1e-12):\n",
        "    \"\"\"Calculates audio beat proxy (spectral flux) and frequency proxy (spectral centroid).\"\"\"\n",
        "    # Beat proxy: spectral flux. Frequency proxy: spectral centroid (Hz).\n",
        "    y = np.asarray(y, np.float32)\n",
        "    if y.ndim == 2:\n",
        "        y = y.mean(axis=1)\n",
        "\n",
        "    # Downsample audio to reduce computational load for STFT\n",
        "    if sr > downsample_to:\n",
        "        if sr % downsample_to == 0:\n",
        "            down = sr // downsample_to\n",
        "            y_ds = resample_poly(y, up=1, down=down)\n",
        "            sr_ds = sr // down\n",
        "        else:\n",
        "            g = np.gcd(sr, downsample_to)\n",
        "            up = downsample_to // g\n",
        "            down = sr // g\n",
        "            y_ds = resample_poly(y, up=up, down=down)\n",
        "            sr_ds = downsample_to\n",
        "    else:\n",
        "        y_ds, sr_ds = y, sr\n",
        "\n",
        "    noverlap = max(0, nperseg - hop)\n",
        "    f, t, Z = stft(y_ds, fs=sr_ds, nperseg=nperseg, noverlap=noverlap, boundary=None)\n",
        "    mag = np.abs(Z).astype(np.float32)\n",
        "\n",
        "    # Spectral Centroid: weighted average of frequencies present in a frame\n",
        "    denom = mag.sum(axis=0) + eps\n",
        "    centroid_hz = (f[:, None] * mag).sum(axis=0) / denom\n",
        "\n",
        "    # Spectral Flux: a measure of how quickly the power spectrum of a signal is changing\n",
        "    dmag = np.diff(mag, axis=1)\n",
        "    flux = np.maximum(dmag, 0).sum(axis=0) # Only positive changes contribute to flux\n",
        "    return t[1:], flux, t, centroid_hz\n",
        "\n",
        "def movement_index_from_arrays(ax, ay, az, t_s, fs, hp_hz=0.3, win_sec=1.0, hop_sec=0.2):\n",
        "    \"\"\"Calculates a movement index from high-pass filtered accelerometer data arrays.\"\"\"\n",
        "    # Apply a high-pass filter to remove gravity and slow movements\n",
        "    b, a = butter(2, hp_hz / (fs / 2), btype=\"highpass\")\n",
        "    axf = filtfilt(b, a, ax)\n",
        "    ayf = filtfilt(b, a, ay)\n",
        "    azf = filtfilt(b, a, az)\n",
        "    mag = np.sqrt(axf*axf + ayf*ayf + azf*azf) # Magnitude of filtered acceleration\n",
        "\n",
        "    # Compute RMS of magnitude in sliding windows to get movement index\n",
        "    win = max(1, int(fs * win_sec))\n",
        "    hop = max(1, int(fs * hop_sec))\n",
        "    n = len(mag)\n",
        "    n_frames = 1 + max(0, (n - win) // hop)\n",
        "\n",
        "    tt = np.zeros(n_frames, dtype=float)\n",
        "    idx = np.zeros(n_frames, dtype=float)\n",
        "    for i in range(n_frames):\n",
        "        s = i * hop\n",
        "        frame = mag[s:s+win]\n",
        "        idx[i] = np.sqrt(np.mean(frame * frame)) # RMS value\n",
        "        tt[i] = t_s[min(s + win//2, n-1)] # Time at the center of the window\n",
        "    return tt, idx\n",
        "\n",
        "def _safe_np(a):\n",
        "    \"\"\"Converts input to a float NumPy array, handling None by returning empty array.\"\"\"\n",
        "    if a is None:\n",
        "        return np.array([], dtype=float)\n",
        "    return np.asarray(a, dtype=float)\n",
        "\n",
        "def _now_tag():\n",
        "    \"\"\"Generates a timestamp string for naming output folders.\"\"\"\n",
        "    return time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# --------- sanity checks ----------\n",
        "# Ensures Cell 1 has been run and EQUIPHY is ready for processing.\n",
        "if not globals().get(\"EQUIPHY\", {}).get(\"ready\", False):\n",
        "    raise RuntimeError(\"Run Cell 1 first (EQUIPHY must be ready).\")\n",
        "\n",
        "# --------- mount drive if saving ----------\n",
        "# Automatically mounts Google Drive if SAVE_TO_DRIVE is True.\n",
        "if SAVE_TO_DRIVE:\n",
        "    from google.colab import drive\n",
        "    if not Path(\"/content/drive\").exists():\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# --------- pull necessary data from EQUIPHY ----------\n",
        "# Retrieves raw data bytes and synchronization parameters from EQUIPHY.\n",
        "df_hr  = _read_polar_txt_bytes(EQUIPHY[\"hr_bytes\"])\n",
        "df_acc = _read_polar_txt_bytes(EQUIPHY[\"acc_bytes\"])\n",
        "\n",
        "ms0 = float(EQUIPHY.get(\"ms0\", np.nanmin([df_hr[\"MS\"].min(), df_acc[\"MS\"].min()]))) # Common millisecond start offset\n",
        "t0 = float(EQUIPHY.get(\"t_start_s\", 0.0)) # Start of analysis window (data time)\n",
        "t1 = float(EQUIPHY.get(\"t_stop_s\", np.inf)) # End of analysis window (data time)\n",
        "audio_offset = float(EQUIPHY.get(\"audio_offset_s\", 0.0)) # Offset from video/audio time to data time\n",
        "\n",
        "hr_name  = EQUIPHY.get(\"hr_name\", \"HR\")\n",
        "acc_name = EQUIPHY.get(\"acc_name\", \"ACC\")\n",
        "vid_name = EQUIPHY.get(\"video_name\", \"VIDEO\")\n",
        "aud_name = EQUIPHY.get(\"aud_name\", None)\n",
        "\n",
        "# Convert MS timestamps to seconds relative to 'ms0' for consistent time axes.\n",
        "df_hr[\"t_s\"]  = (df_hr[\"MS\"].to_numpy(float)  - ms0) / 1000.0\n",
        "df_acc[\"t_s\"] = (df_acc[\"MS\"].to_numpy(float) - ms0) / 1000.0\n",
        "\n",
        "# =========================\n",
        "# 1) Beats: Instant HR + flags\n",
        "# Processes HR data to extract instant heart rate and flag invalid beats.\n",
        "# =========================\n",
        "if \"RR\" not in df_hr.columns:\n",
        "    raise ValueError(\"HR file must contain RR (ms) to compute instantaneous HR.\")\n",
        "\n",
        "t_hr_all  = df_hr[\"t_s\"].to_numpy(float)\n",
        "rr_all    = df_hr[\"RR\"].to_numpy(float)\n",
        "# SC (Signal Quality) column, defaulting to 1 (good) if not present\n",
        "sc_all    = df_hr[\"SC\"].to_numpy(int) if \"SC\" in df_hr.columns else np.ones(len(df_hr), dtype=int)\n",
        "\n",
        "# Filter for data within the analysis window and valid RR intervals\n",
        "in_win = np.isfinite(t_hr_all) & (t_hr_all >= t0) & (t_hr_all <= t1)\n",
        "valid_rr = np.isfinite(rr_all) & (rr_all > 0)\n",
        "\n",
        "good = in_win & valid_rr & (sc_all == 1) # Good quality beats\n",
        "flag = in_win & valid_rr & (sc_all != 1) # Flagged beats (e.g., due to poor signal quality)\n",
        "\n",
        "t_beats = t_hr_all[good]\n",
        "rr_ms   = rr_all[good]\n",
        "hr_inst = 60000.0 / rr_ms # Calculate instant HR from RR intervals (60,000 ms/min / RR_ms)\n",
        "\n",
        "t_flag = t_hr_all[flag]\n",
        "hr_flag = (60000.0 / rr_all[flag]) if flag.any() else np.array([], dtype=float)\n",
        "\n",
        "# =========================\n",
        "# 2) Rolling RMSSD (beats)\n",
        "# Computes RMSSD, a common HRV metric, over a rolling window.\n",
        "# =========================\n",
        "rr_use = rr_ms.copy()\n",
        "if DETREND_RR and len(rr_use) > 0:\n",
        "    # Detrend RR intervals to remove slow variations if configured\n",
        "    rr_use = rr_use - ew_mean(rr_use, alpha=RR_TREND_ALPHA)\n",
        "rmssd_roll = rolling_rmssd(rr_use, window_beats=RMSSD_WINDOW_BEATS)\n",
        "\n",
        "# =========================\n",
        "# 3) Movement proxy (ACC)\n",
        "# Derives a movement index from accelerometer data.\n",
        "# =========================\n",
        "ms_acc = df_acc[\"MS\"].to_numpy(float)\n",
        "# Estimate accelerometer sampling frequency\n",
        "dms = np.diff(ms_acc)\n",
        "dms = dms[np.isfinite(dms) & (dms > 0)]\n",
        "fs_acc = 100.0 if len(dms) == 0 else 1000.0 / np.median(dms)\n",
        "\n",
        "if not all(c in df_acc.columns for c in [\"ACCX\",\"ACCY\",\"ACCZ\"]):\n",
        "    raise ValueError(\"ACC file missing ACCX/ACCY/ACCZ columns.\")\n",
        "\n",
        "# Convert raw accelerometer values to 'g' (gravity units)\n",
        "ax = df_acc[\"ACCX\"].to_numpy(float) / 1000.0\n",
        "ay = df_acc[\"ACCY\"].to_numpy(float) / 1000.0\n",
        "az = df_acc[\"ACCZ\"].to_numpy(float) / 1000.0\n",
        "t_acc = df_acc[\"t_s\"].to_numpy(float)\n",
        "\n",
        "# Calculate the movement index over the full data range first\n",
        "t_mov_full, mov_idx_full = movement_index_from_arrays(\n",
        "    ax, ay, az, t_acc, fs_acc,\n",
        "    hp_hz=ACC_HP_HZ, win_sec=ACC_WIN_SEC, hop_sec=ACC_HOP_SEC\n",
        ")\n",
        "# Then filter for the desired analysis window\n",
        "m_mov = np.isfinite(t_mov_full) & (t_mov_full >= t0) & (t_mov_full <= t1)\n",
        "t_mov = t_mov_full[m_mov]\n",
        "mov_idx = mov_idx_full[m_mov]\n",
        "\n",
        "# =========================\n",
        "# 4) Audio/music metrics (optional)\n",
        "# Extracts loudness, beat, and frequency proxies from audio data.\n",
        "# audio_time 'a' -> data_time = 'a' + audio_offset\n",
        "# =========================\n",
        "have_audio = False\n",
        "y = sr = None\n",
        "\n",
        "# Attempt to load audio bytes from EQUIPHY (if extracted by Cell 1)\n",
        "if (EQUIPHY.get(\"aud_bytes\", None) is not None) and (EQUIPHY.get(\"aud_name\", None) is not None):\n",
        "    y, sr = _read_audio_bytes(EQUIPHY[\"aud_bytes\"], EQUIPHY[\"aud_name\"])\n",
        "    have_audio = True\n",
        "else:\n",
        "    # Fallback: check if 'extracted_audio.wav' exists in the current working directory\n",
        "    if Path(\"extracted_audio.wav\").exists():\n",
        "        try:\n",
        "            import soundfile as sf\n",
        "            y, sr = sf.read(\"extracted_audio.wav\")\n",
        "            if y.ndim == 2:\n",
        "                y = y.mean(axis=1)\n",
        "            y = y.astype(np.float32)\n",
        "            have_audio = True\n",
        "            aud_name = \"extracted_audio.wav\"\n",
        "        except Exception:\n",
        "            have_audio = False\n",
        "\n",
        "if have_audio:\n",
        "    # Compute audio features if audio data is available\n",
        "    t_loud_a, loud_db = audio_loudness_db(y, sr, win_sec=LOUD_WIN_SEC, hop_sec=LOUD_HOP_SEC)\n",
        "    t_flux_a, flux, t_cent_a, centroid_hz = audio_beat_and_freq_proxies(y, sr, downsample_to=AUDIO_DOWNSAMPLE_TO)\n",
        "\n",
        "    # Shift audio times to data time using the determined offset\n",
        "    t_loud = t_loud_a + audio_offset\n",
        "    t_flux = t_flux_a + audio_offset\n",
        "    t_cent = t_cent_a + audio_offset\n",
        "\n",
        "    # Filter audio features for the analysis window\n",
        "    m_loud = np.isfinite(t_loud) & (t_loud >= t0) & (t_loud <= t1)\n",
        "    m_flux = np.isfinite(t_flux) & (t_flux >= t0) & (t_flux <= t1)\n",
        "    m_cent = np.isfinite(t_cent) & (t_cent >= t0) & (t_cent <= t1)\n",
        "\n",
        "    t_loud, loud_db = t_loud[m_loud], loud_db[m_loud]\n",
        "    t_flux, flux = t_flux[m_flux], flux[m_flux]\n",
        "    t_cent, centroid_hz = t_cent[m_cent], centroid_hz[m_cent]\n",
        "\n",
        "    # Post-process flux for better visualization (z-score and smoothing)\n",
        "    if len(flux) > 0:\n",
        "        flux_z = (flux - np.mean(flux)) / (np.std(flux) + 1e-9)\n",
        "        flux_z = _moving_average(flux_z, k=9)\n",
        "    else:\n",
        "        flux_z = flux\n",
        "\n",
        "    centroid_sm = _moving_average(centroid_hz, k=7) if len(centroid_hz) else centroid_hz\n",
        "else:\n",
        "    # Set audio related variables to None if no audio is present\n",
        "    t_loud = loud_db = t_flux = flux_z = t_cent = centroid_sm = None\n",
        "\n",
        "# =========================\n",
        "# Export series (arrays)\n",
        "# Stores all computed time series into the EQUIPHY[\"series\"] dictionary.\n",
        "# =========================\n",
        "EQUIPHY[\"series\"] = {\n",
        "    \"t0\": float(t0), \"t1\": float(t1),\n",
        "    \"ms0\": float(ms0),\n",
        "    \"audio_offset_s\": float(audio_offset),\n",
        "    \"fs_acc\": float(fs_acc),\n",
        "\n",
        "    \"t_beats\": _safe_np(t_beats),\n",
        "    \"rr_ms\": _safe_np(rr_ms),\n",
        "    \"hr_inst\": _safe_np(hr_inst),\n",
        "    \"rmssd_roll\": _safe_np(rmssd_roll),\n",
        "\n",
        "    \"t_flag\": _safe_np(t_flag) if len(t_flag) else np.array([], dtype=float), # Flagged HR points\n",
        "    \"hr_flag\": _safe_np(hr_flag) if len(t_flag) else np.array([], dtype=float), # HR values for flagged points\n",
        "\n",
        "    \"t_mov\": _safe_np(t_mov),\n",
        "    \"mov_idx\": _safe_np(mov_idx),\n",
        "\n",
        "    \"have_audio\": bool(have_audio),\n",
        "    \"t_loud\": _safe_np(t_loud) if have_audio and t_loud is not None else None,\n",
        "    \"loud_db\": _safe_np(loud_db) if have_audio and t_loud is not None else None,\n",
        "    \"t_flux\": _safe_np(t_flux) if have_audio and t_flux is not None else None,\n",
        "    \"flux_z\": _safe_np(flux_z) if have_audio and t_flux is not None else None,\n",
        "    \"t_cent\": _safe_np(t_cent) if have_audio and t_cent is not None else None,\n",
        "    \"centroid_sm\": _safe_np(centroid_sm) if have_audio and t_cent is not None else None,\n",
        "}\n",
        "print(\"✅ Exported EQUIPHY['series']\")\n",
        "\n",
        "# =========================\n",
        "# Plot (preview)\n",
        "# Generates a multi-panel plot to visualize the computed metrics.\n",
        "# This plot helps in quick verification of the data and alignment.\n",
        "# =========================\n",
        "print(\"\\nLoaded:\")\n",
        "print(\"  HR   :\", hr_name, f\"(rows={len(df_hr)})\")\n",
        "print(\"  ACC  :\", acc_name, f\"(rows={len(df_acc)}, fs≈{fs_acc:.1f} Hz)\")\n",
        "print(\"  VIDEO:\", vid_name)\n",
        "print(\"  ms0  :\", f\"{ms0:.0f} ms  (shared origin)\")\n",
        "print(f\"Analysis window (DATA time): [{t0:.2f}, {t1:.2f}] s\")\n",
        "print(f\"RMSSD window: {RMSSD_WINDOW_BEATS} beats | detrend RR: {DETREND_RR} (alpha={RR_TREND_ALPHA})\")\n",
        "print(f\"Audio: {aud_name} | audio_offset_s (audio→data): {audio_offset:+.3f} | have_audio={have_audio}\")\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12)) # Create a figure for the 6-panel plot\n",
        "\n",
        "# Plot 1: Instantaneous Heart Rate\n",
        "ax1 = plt.subplot(6, 1, 1)\n",
        "ax1.plot(t_beats, hr_inst, linewidth=1.5)\n",
        "if len(t_flag):\n",
        "    ax1.scatter(t_flag, hr_flag, s=18, marker=\"x\", label=\"flagged (SC!=1)\")\n",
        "ax1.set_ylabel(\"Instant HR\\n(bpm)\")\n",
        "ax1.grid(True, alpha=0.3)\n",
        "if len(t_flag):\n",
        "    ax1.legend(loc=\"upper right\", fontsize=8)\n",
        "\n",
        "# Plot 2: Rolling RMSSD\n",
        "ax2 = plt.subplot(6, 1, 2, sharex=ax1) # Share X-axis with the HR plot\n",
        "ax2.plot(t_beats, rmssd_roll, linewidth=1.5)\n",
        "ax2.set_ylabel(\"RMSSD (rolling)\\n(ms)\")\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: ACC Movement Index\n",
        "ax3 = plt.subplot(6, 1, 3, sharex=ax1)\n",
        "ax3.plot(t_mov, mov_idx, linewidth=1.5)\n",
        "ax3.set_ylabel(\"Movement\\n(g RMS)\")\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plots 4, 5, 6: Audio features (if available)\n",
        "ax4 = plt.subplot(6, 1, 4, sharex=ax1)\n",
        "ax5 = plt.subplot(6, 1, 5, sharex=ax1)\n",
        "ax6 = plt.subplot(6, 1, 6, sharex=ax1)\n",
        "\n",
        "if have_audio and t_loud is not None:\n",
        "    ax4.plot(t_loud, loud_db, linewidth=1.5)\n",
        "    ax4.set_ylabel(\"Loudness\\n(RMS dB)\")\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    ax5.plot(t_flux, flux_z, linewidth=1.5)\n",
        "    ax5.set_ylabel(\"Beat proxy\\n(flux z)\")\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    ax6.plot(t_cent, centroid_sm, linewidth=1.5)\n",
        "    ax6.set_ylabel(\"Freq proxy\\n(centroid Hz)\")\n",
        "    ax6.set_xlabel(\"Time (data s)\")\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "else:\n",
        "    # Display a message if audio data is not available\n",
        "    ax4.text(0.02, 0.5, \"No extracted audio available\\n(skipping music metrics)\", transform=ax4.transAxes)\n",
        "    ax4.set_axis_off() # Hide axes for empty plots\n",
        "    ax5.set_axis_off()\n",
        "    ax6.set_axis_off()\n",
        "\n",
        "# Set X-axis limits for all shared plots\n",
        "xmax = t1 if np.isfinite(t1) else (t_beats[-1] if len(t_beats) else t0 + 10)\n",
        "ax1.set_xlim(t0, xmax)\n",
        "\n",
        "plt.tight_layout() # Adjust layout to prevent overlapping elements\n",
        "plt.show()\n",
        "\n",
        "# =========================\n",
        "# SAVE TO DRIVE (metadata + arrays)\n",
        "# Saves processed data and metadata to a structured folder on Google Drive.\n",
        "# =========================\n",
        "if SAVE_TO_DRIVE:\n",
        "    run_tag = RUN_TAG\n",
        "    if run_tag is None:\n",
        "        # Generate a unique run tag based on video name and current timestamp\n",
        "        stem = Path(vid_name).stem[:40].replace(\" \", \"_\")\n",
        "        run_tag = f\"{stem}__{_now_tag()}\"\n",
        "    out_dir = Path(DRIVE_BASE_DIR) / run_tag # Create a dedicated output folder\n",
        "    out_dir.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
        "\n",
        "    # --- metadata (small json) ---\n",
        "    # Stores key information about the run, settings, and file details.\n",
        "    meta = {\n",
        "        \"run_tag\": run_tag,\n",
        "        \"created_localtime\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"platform\": {\n",
        "            \"python\": platform.python_version(),\n",
        "            \"system\": platform.platform(),\n",
        "        },\n",
        "        \"files\": {\n",
        "            \"hr_name\": hr_name,\n",
        "            \"acc_name\": acc_name,\n",
        "            \"video_name\": vid_name,\n",
        "            \"audio_name\": aud_name,\n",
        "        },\n",
        "        \"alignment\": {\n",
        "            \"video_to_data_offset_s\": float(EQUIPHY.get(\"video_to_data_offset_s\", np.nan)),\n",
        "            \"audio_offset_s\": float(audio_offset),\n",
        "            \"sync_corr\": float(EQUIPHY.get(\"sync_corr\", np.nan)),\n",
        "            \"sync_video_segment\": EQUIPHY.get(\"sync_video_segment\", None),\n",
        "            \"sync_data_segment_guess\": EQUIPHY.get(\"sync_data_segment_guess\", None),\n",
        "        },\n",
        "        \"window\": {\n",
        "            \"t0\": float(t0),\n",
        "            \"t1\": float(t1),\n",
        "            \"ms0\": float(ms0),\n",
        "        },\n",
        "        \"settings\": {\n",
        "            \"RMSSD_WINDOW_BEATS\": int(RMSSD_WINDOW_BEATS),\n",
        "            \"DETREND_RR\": bool(DETREND_RR),\n",
        "            \"RR_TREND_ALPHA\": float(RR_TREND_ALPHA),\n",
        "            \"ACC_HP_HZ\": float(ACC_HP_HZ),\n",
        "            \"ACC_WIN_SEC\": float(ACC_WIN_SEC),\n",
        "            \"ACC_HOP_SEC\": float(ACC_HOP_SEC),\n",
        "            \"AUDIO_DOWNSAMPLE_TO\": int(AUDIO_DOWNSAMPLE_TO),\n",
        "            \"LOUD_WIN_SEC\": float(LOUD_WIN_SEC),\n",
        "            \"LOUD_HOP_SEC\": float(LOUD_HOP_SEC),\n",
        "        }\n",
        "    }\n",
        "    meta_path = out_dir / \"meta.json\"\n",
        "    meta_path.write_text(json.dumps(meta, indent=2)) # Save metadata as a pretty-printed JSON file\n",
        "\n",
        "    # --- arrays (npz) ---\n",
        "    # Saves all computed time series arrays into a single compressed NumPy .npz file.\n",
        "    arrays = {}\n",
        "    for k, v in EQUIPHY[\"series\"].items():\n",
        "        if isinstance(v, np.ndarray):\n",
        "            arrays[k] = v\n",
        "        elif v is None:\n",
        "            continue\n",
        "        elif isinstance(v, (float, int, bool)):\n",
        "            arrays[k] = np.array([v]) # Store single values as arrays for consistency\n",
        "        # Strings/dicts from EQUIPHY[\"series\"] live in meta.json, not here.\n",
        "    npz_path = out_dir / \"series_arrays.npz\"\n",
        "    np.savez_compressed(npz_path, **arrays) # Save compressed NumPy arrays\n",
        "\n",
        "    # --- optional CSV exports ---\n",
        "    # Exports individual data streams to CSV files for easy viewing/sharing.\n",
        "    if EXPORT_CSVS:\n",
        "        beats_df = pd.DataFrame({\n",
        "            \"t_beats_s\": t_beats,\n",
        "            \"rr_ms\": rr_ms,\n",
        "            \"hr_bpm\": hr_inst,\n",
        "            \"rmssd_roll_ms\": rmssd_roll\n",
        "        })\n",
        "        beats_df.to_csv(out_dir / \"beats.csv\", index=False)\n",
        "\n",
        "        mov_df = pd.DataFrame({\"t_mov_s\": t_mov, \"mov_idx\": mov_idx})\n",
        "        mov_df.to_csv(out_dir / \"movement.csv\", index=False)\n",
        "\n",
        "        if have_audio and (t_loud is not None):\n",
        "            pd.DataFrame({\"t_loud_s\": t_loud, \"loud_db\": loud_db}).to_csv(out_dir / \"audio_loudness.csv\", index=False)\n",
        "        if have_audio and (t_flux is not None):\n",
        "            pd.DataFrame({\"t_flux_s\": t_flux, \"flux_z\": flux_z}).to_csv(out_dir / \"audio_flux.csv\", index=False)\n",
        "        if have_audio and (t_cent is not None):\n",
        "            pd.DataFrame({\"t_cent_s\": t_cent, \"centroid_hz_sm\": centroid_sm}).to_csv(out_dir / \"audio_centroid.csv\", index=False)\n",
        "\n",
        "    # --- optional plot png ---\n",
        "    # Saves the diagnostic plot as a PNG image.\n",
        "    if EXPORT_PLOT_PNG:\n",
        "        fig_path = out_dir / \"preview.png\"\n",
        "        fig.savefig(fig_path, dpi=150)\n",
        "\n",
        "    # Store paths to exported files in EQUIPHY for potential downstream use\n",
        "    EQUIPHY[\"export_dir\"] = str(out_dir)\n",
        "    EQUIPHY[\"export_meta_json\"] = str(meta_path)\n",
        "    EQUIPHY[\"export_series_npz\"] = str(npz_path)\n",
        "\n",
        "    print(\"\\n✅ Saved exports to Drive:\")\n",
        "    print(\"  folder:\", out_dir)\n",
        "    print(\"  meta  :\", meta_path.name)\n",
        "    print(\"  npz   :\", npz_path.name)\n",
        "    if EXPORT_CSVS:\n",
        "        print(\"  csv   : beats.csv, movement.csv (+ audio_*.csv if audio present)\")\n",
        "    if EXPORT_PLOT_PNG:\n",
        "        print(\"  plot  : preview.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31681db8"
      },
      "source": [
        "## Graph Observations (Cell 2)\n",
        "\n",
        "Use this section to record your observations and initial interpretations of the physiological and movement graphs generated in Cell 2. This helps to connect the numerical data with potential real-world events or horse states.\n",
        "\n",
        "---\n",
        "\n",
        "**Overall Impression of Graphs:** [e.g., Clear and easy to read, some noise in HR, audio data looks consistent]\n",
        "\n",
        "**Heart Rate (HR) Trends:** [e.g., Steady HR around 30-35 bpm, a few spikes, slight increase towards the end]\n",
        "\n",
        "**Heart Rate Variability (RMSSD) Trends:** [e.g., RMSSD generally stable, decreased during HR spikes, increased during calm periods]\n",
        "\n",
        "**Movement (ACC) Patterns:** [e.g., Low overall movement, a few brief periods of increased movement (shifting weight, head movement)]\n",
        "\n",
        "**Audio Feature Observations:** [e.g., Music started at ~50s mark, loudness consistent, spectral centroid showed changes with different music parts]\n",
        "\n",
        "**Noteworthy Correlations/Relationships:** [e.g., HR slightly increased when movement occurred, no clear link between music and HR/movement]\n",
        "\n",
        "**Any Anomalies or Unexpected Patterns:** [e.g., A sudden drop in HR that doesn't seem to align with movement, unexpected high-frequency audio noise]\n",
        "\n",
        "**Additional Notes on Interpretation:**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Side-by-Side Video Renderer\n",
        "\n",
        "**Purpose:** This cell generates a video with synchronized graphs of key metrics displayed alongside the original video. It allows for a direct visual correlation between the horse's behavior and the physiological/movement data.\n",
        "\n",
        "**What it does:**\n",
        "*   **Video Processing:** Cuts the original video to the defined analysis window.\n",
        "*   **Graph Synchronization:** Overlays synchronized plots of Heart Rate, RMSSD, Movement, and Audio Frequency metrics.\n",
        "*   **Rendering:** Combines the video and the dynamic dashboard into a single MP4 file.\n",
        "\n",
        "**How to use it:**\n",
        "1.  **Run the Cell:** Execute the code to start the rendering process.\n",
        "2.  **Monitor Progress:** A progress bar will be displayed to show the rendering status.\n",
        "3.  **View Output:** The final video will be saved to the `renders` folder in your defined Google Drive directory.\n",
        "\n",
        "**Warning:** Video rendering is computationally intensive. Depending on the video length and resolution, this process can take several minutes. Please keep the browser tab open and active while the progress bar updates.\n",
        "\n"
      ],
      "metadata": {
        "id": "SDYl6HvT-Htc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTrLBRaMEm6T",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ==========================================================\n",
        "# Cell 3: Movie renderer (SAVE TO GOOGLE DRIVE):\n",
        "# Video (left) + 4 stacked graphs (right) + moving cursor\n",
        "# Graphs: HR, rolling RMSSD, Movement, Frequency proxy (spectral centroid)\n",
        "# Audio: extracted from SAME video subclip via ffmpeg\n",
        "#\n",
        "# Requires:\n",
        "#   - Cell 1: EQUIPHY[\"video_path\"], [\"video_to_data_offset_s\"], [\"t_start_s\"], [\"t_stop_s\"], EQUIPHY[\"ready\"]=True\n",
        "#   - Cell 2: EQUIPHY[\"series\"] populated (and ideally saved export_dir)\n",
        "#\n",
        "# Output:\n",
        "#   - Writes MP4 directly to Google Drive\n",
        "# ==========================================================\n",
        "\n",
        "import os, subprocess, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "# Ensure tqdm is available for progress bars\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    !pip install -q tqdm\n",
        "    from tqdm import tqdm\n",
        "\n",
        "from moviepy.editor import VideoFileClip, VideoClip, clips_array, AudioFileClip\n",
        "\n",
        "# ---------------- knobs ----------------\n",
        "OUT_FPS = 15                 # 15 looks nicer but is slower; 10–12 is a good compromise\n",
        "H = 720                      # output video height\n",
        "PANEL_WIDTH_PX = 720\n",
        "PANEL_HEIGHT_IN = 7.5\n",
        "SCROLL_WINDOW_S = 60         # None = full window, else last N seconds\n",
        "FINE_TUNE_OFFSET_S = 0.0     # optional micro-adjust if you see lag\n",
        "AUDIO_SR = 22050\n",
        "\n",
        "# save location\n",
        "SAVE_TO_DRIVE = True\n",
        "DRIVE_FALLBACK_DIR = \"/content/drive/MyDrive/EquiPhysics/renders\"\n",
        "ALSO_COPY_TO_CONTENT = False  # if True, also copy mp4 to /content\n",
        "\n",
        "# -------------- checks ----------------\n",
        "if not globals().get(\"EQUIPHY\", {}).get(\"ready\", False):\n",
        "    raise RuntimeError(\"Run Cell 1 first (EQUIPHY must be ready).\")\n",
        "if \"video_path\" not in EQUIPHY:\n",
        "    raise RuntimeError(\"EQUIPHY['video_path'] missing. Run Cell 1 to load the video.\")\n",
        "if \"series\" not in EQUIPHY:\n",
        "    raise RuntimeError(\"EQUIPHY['series'] missing. Run Cell 2 first.\")\n",
        "\n",
        "S = EQUIPHY[\"series\"]\n",
        "video_path = EQUIPHY[\"video_path\"]\n",
        "\n",
        "t0_req = float(EQUIPHY.get(\"t_start_s\", S.get(\"t0\", 0.0)))\n",
        "t1_req = float(EQUIPHY.get(\"t_stop_s\",  S.get(\"t1\", 0.0)))\n",
        "if t1_req <= t0_req:\n",
        "    raise ValueError(\"Bad t0/t1: t_stop_s must be > t_start_s.\")\n",
        "\n",
        "vid_to_data = float(EQUIPHY.get(\"video_to_data_offset_s\", 0.0)) + float(FINE_TUNE_OFFSET_S)\n",
        "\n",
        "# Pull aligned series (already in DATA time)\n",
        "t_beats = np.asarray(S.get(\"t_beats\", []), float)\n",
        "hr_inst = np.asarray(S.get(\"hr_inst\", []), float)\n",
        "rmssd   = np.asarray(S.get(\"rmssd_roll\", []), float)\n",
        "t_mov   = np.asarray(S.get(\"t_mov\", []), float)\n",
        "mov_idx = np.asarray(S.get(\"mov_idx\", []), float)\n",
        "\n",
        "t_cent   = S.get(\"t_cent\", None)\n",
        "centroid = S.get(\"centroid_sm\", None)\n",
        "have_centroid = (t_cent is not None) and (centroid is not None)\n",
        "if have_centroid:\n",
        "    t_cent = np.asarray(t_cent, float)\n",
        "    centroid = np.asarray(centroid, float)\n",
        "\n",
        "def _clip_xy(t, y, a, b):\n",
        "    t = np.asarray(t, float); y = np.asarray(y, float)\n",
        "    m = np.isfinite(t) & (t >= a) & (t <= b)\n",
        "    return t[m], y[m]\n",
        "\n",
        "# ---------------- mount Drive + choose output dir ----------------\n",
        "if SAVE_TO_DRIVE:\n",
        "    from google.colab import drive\n",
        "    if not Path(\"/content/drive\").exists():\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "export_dir = EQUIPHY.get(\"export_dir\", None)\n",
        "if SAVE_TO_DRIVE:\n",
        "    if export_dir is not None and str(export_dir).startswith(\"/content/drive\"):\n",
        "        out_dir = Path(export_dir) / \"renders\"\n",
        "    else:\n",
        "        out_dir = Path(DRIVE_FALLBACK_DIR)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "else:\n",
        "    out_dir = Path(\".\")\n",
        "\n",
        "# ---------------- choose video subclip from DATA window ----------------\n",
        "# video_time = data_time - offset\n",
        "# Keep base open until AFTER write_videofile finishes\n",
        "base = VideoFileClip(video_path, audio=False)\n",
        "\n",
        "v_start = t0_req - vid_to_data\n",
        "v_end   = t1_req - vid_to_data\n",
        "\n",
        "v_start = max(0.0, float(v_start))\n",
        "v_end   = min(float(base.duration), float(v_end))\n",
        "\n",
        "if v_end <= v_start:\n",
        "    base.close()\n",
        "    raise RuntimeError(\"Invalid video subclip after clamping. Check offset or t0/t1.\")\n",
        "\n",
        "clip = base.subclip(v_start, v_end)  # no audio\n",
        "duration = float(clip.duration)\n",
        "\n",
        "# This is exact mapping for this rendered clip\n",
        "out_data_start = v_start + vid_to_data\n",
        "out_data_end   = out_data_start + duration\n",
        "\n",
        "print(\"Render mapping:\")\n",
        "print(f\"  requested DATA window: [{t0_req:.2f}, {t1_req:.2f}] s\")\n",
        "print(f\"  video subclip:         [{v_start:.2f}, {v_start+duration:.2f}] s  (dur={duration:.2f}s)\")\n",
        "print(f\"  offset (data=video+off): {vid_to_data:+.3f} s\")\n",
        "print(f\"  output DATA window:    [{out_data_start:.2f}, {out_data_end:.2f}] s\")\n",
        "\n",
        "# Clip plot series to actual output data range\n",
        "t_beats_c, hr_c    = _clip_xy(t_beats, hr_inst, out_data_start, out_data_end)\n",
        "_,         rmssd_c = _clip_xy(t_beats, rmssd,   out_data_start, out_data_end)\n",
        "t_mov_c,   mov_c   = _clip_xy(t_mov,   mov_idx, out_data_start, out_data_end)\n",
        "\n",
        "if have_centroid:\n",
        "    t_cent_c, cent_c = _clip_xy(t_cent, centroid, out_data_start, out_data_end)\n",
        "else:\n",
        "    t_cent_c = np.array([out_data_start, out_data_end], float)\n",
        "    cent_c   = np.array([np.nan, np.nan], float)\n",
        "\n",
        "if len(t_beats_c) < 2 or len(t_mov_c) < 2:\n",
        "    clip.close(); base.close()\n",
        "    raise RuntimeError(\"Not enough beat/movement data in the output window. Widen t0/t1 or check Cell 2 export.\")\n",
        "\n",
        "# ---------------- extract audio from SAME subclip via ffmpeg ----------------\n",
        "wav_path = \"_subclip_audio.wav\"\n",
        "audio_clip = None\n",
        "\n",
        "ff_cmd = [\n",
        "    \"ffmpeg\", \"-y\",\n",
        "    \"-i\",  video_path,\n",
        "    \"-ss\", f\"{v_start:.6f}\",\n",
        "    \"-t\",  f\"{duration:.6f}\",\n",
        "    \"-vn\",\n",
        "    \"-ac\", \"1\",\n",
        "    \"-ar\", str(AUDIO_SR),\n",
        "    \"-c:a\", \"pcm_s16le\",\n",
        "    wav_path\n",
        "]\n",
        "try:\n",
        "    subprocess.run(ff_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    if os.path.exists(wav_path) and os.path.getsize(wav_path) > 0:\n",
        "        audio_clip = AudioFileClip(wav_path).set_duration(duration)\n",
        "except Exception as e:\n",
        "    print(\"WARNING: could not extract/attach audio:\", e)\n",
        "    audio_clip = None\n",
        "\n",
        "# ---------------- panel renderer ----------------\n",
        "class PanelRenderer:\n",
        "    def __init__(self):\n",
        "        self.fig = plt.figure(figsize=(PANEL_WIDTH_PX/100, PANEL_HEIGHT_IN), dpi=100)\n",
        "        self.canvas = FigureCanvas(self.fig)\n",
        "        self.axes = [self.fig.add_subplot(4,1,i+1) for i in range(4)]\n",
        "\n",
        "        self.axes[0].plot(t_beats_c, hr_c, linewidth=1.6)\n",
        "        self.axes[0].set_ylabel(\"HR (bpm)\")\n",
        "        self.axes[0].grid(True, alpha=0.25)\n",
        "\n",
        "        self.axes[1].plot(t_beats_c, rmssd_c, linewidth=1.6)\n",
        "        self.axes[1].set_ylabel(\"RMSSD (ms)\")\n",
        "        self.axes[1].grid(True, alpha=0.25)\n",
        "\n",
        "        self.axes[2].plot(t_mov_c, mov_c, linewidth=1.6)\n",
        "        self.axes[2].set_ylabel(\"Move (RMS)\")\n",
        "        self.axes[2].grid(True, alpha=0.25)\n",
        "\n",
        "        self.axes[3].plot(t_cent_c, cent_c, linewidth=1.6)\n",
        "        self.axes[3].set_ylabel(\"Freq (Hz)\")\n",
        "        self.axes[3].set_xlabel(\"DATA time (s)\")\n",
        "        self.axes[3].grid(True, alpha=0.25)\n",
        "\n",
        "        self.vlines = [ax.axvline(out_data_start, linewidth=2.5) for ax in self.axes]\n",
        "        for ax in self.axes:\n",
        "            ax.set_xlim(out_data_start, out_data_end)\n",
        "\n",
        "        self.fig.tight_layout()\n",
        "\n",
        "    def __call__(self, t_sub):\n",
        "        data_t = out_data_start + float(t_sub)\n",
        "\n",
        "        if SCROLL_WINDOW_S is None:\n",
        "            xmin, xmax = out_data_start, out_data_end\n",
        "        else:\n",
        "            xmin = max(out_data_start, data_t - SCROLL_WINDOW_S)\n",
        "            xmax = min(out_data_end, xmin + SCROLL_WINDOW_S)\n",
        "            if xmax <= xmin:\n",
        "                xmax = xmin + 1.0\n",
        "\n",
        "        for ax in self.axes:\n",
        "            ax.set_xlim(xmin, xmax)\n",
        "        for vl in self.vlines:\n",
        "            vl.set_xdata([data_t, data_t])\n",
        "\n",
        "        self.canvas.draw()\n",
        "        rgba = np.asarray(self.canvas.buffer_rgba())\n",
        "        return rgba[..., :3].copy()\n",
        "\n",
        "panel_clip = VideoClip(PanelRenderer(), duration=duration).set_fps(OUT_FPS)\n",
        "\n",
        "# ---------------- compose side-by-side ----------------\n",
        "clip_l = clip.resize(height=H)\n",
        "panel_r = panel_clip.resize(height=H)\n",
        "\n",
        "final = clips_array([[clip_l, panel_r]]).set_duration(duration)\n",
        "if audio_clip is not None:\n",
        "    final = final.set_audio(audio_clip)\n",
        "\n",
        "# ---------------- write to drive ----------------\n",
        "stem = Path(video_path).stem[:40].replace(\" \", \"_\")\n",
        "out_name = f\"{stem}__dash__data_{t0_req:.1f}_{t1_req:.1f}.mp4\"\n",
        "out_path = out_dir / out_name\n",
        "\n",
        "print(\"\\nWriting MP4 to:\", out_path)\n",
        "\n",
        "final.write_videofile(\n",
        "    str(out_path),\n",
        "    fps=OUT_FPS,\n",
        "    codec=\"libx264\",\n",
        "    audio_codec=\"aac\",\n",
        "    threads=4,\n",
        "    preset=\"medium\",\n",
        "    verbose=False,\n",
        "    logger=\"bar\"\n",
        ")\n",
        "\n",
        "# optional copy to /content for easy download button\n",
        "if ALSO_COPY_TO_CONTENT:\n",
        "    shutil.copy2(out_path, Path(\"/content\") / out_name)\n",
        "\n",
        "# cleanup + close resources\n",
        "try:\n",
        "    if os.path.exists(wav_path):\n",
        "        os.remove(wav_path)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    if audio_clip is not None:\n",
        "        audio_clip.close()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    final.close()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    panel_clip.close()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    clip.close()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    base.close()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(\"\\n✅ Saved movie to Drive:\")\n",
        "print(\"  \", out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Portrait Reel Renderer\n",
        "\n",
        "**Purpose:** This cell is designed to create a vertical video (9:16 aspect ratio) suitable for social media platforms like Instagram Reels, TikTok, or YouTube Shorts. It visualizes your data in a mobile-friendly format.\n",
        "\n",
        "**What it does:**\n",
        "*   **Vertical Layout:** Crops the original video to fill the top portion of the frame.\n",
        "*   **Stacked Dashboard:** Places synchronized graphs of Heart Rate, RMSSD, Movement, Loudness, and Frequency in the bottom portion.\n",
        "*   **Social Ready:** Exports a high-quality vertical MP4 file that is ready for sharing.\n",
        "\n",
        "**How to use it:**\n",
        "1.  **Run the Cell:** Execute the code.\n",
        "2.  **Define Render Window:** You may be asked to input a specific start and stop time (in data seconds) for the clip you want to render, or it will default to a specific segment.\n",
        "3.  **Wait for Processing:** Similar to Cell 3, this renders frame-by-frame and may take some time.\n",
        "4.  **Locate File:** The finished video will be saved in your Drive's `renders` folder with `__reel__` in the filename.\n"
      ],
      "metadata": {
        "id": "tAomF39p_ch6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u14Wr9oHdul",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ==========================================================\n",
        "# Cell 4: Reel renderer (portrait 9:16, 1080x1920) — SAVE TO DRIVE\n",
        "# Top: video (cropped to fill)\n",
        "# Bottom: 5 graphs (HR, RMSSD, Move, Loudness, Freq centroid) + progress bar + cursor\n",
        "#\n",
        "# Requires:\n",
        "#   - Cell 1: EQUIPHY[\"video_path\"], EQUIPHY[\"video_to_data_offset_s\"], EQUIPHY[\"ready\"]=True\n",
        "#   - Cell 2: EQUIPHY[\"series\"] populated (recommended)\n",
        "#\n",
        "# Output:\n",
        "#   - Saved to Google Drive (folder configurable below)\n",
        "# ==========================================================\n",
        "\n",
        "import os, io, subprocess\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from scipy.signal import stft\n",
        "\n",
        "# ---------- moviepy import (install if needed) ----------\n",
        "try:\n",
        "    from moviepy.editor import (\n",
        "        VideoFileClip, VideoClip, CompositeVideoClip, AudioFileClip\n",
        "    )\n",
        "    import moviepy.video.fx.all as vfx\n",
        "except Exception as e:\n",
        "    print(\"Installing moviepy...\")\n",
        "    !pip -q install moviepy\n",
        "    from moviepy.editor import (\n",
        "        VideoFileClip, VideoClip, CompositeVideoClip, AudioFileClip\n",
        "    )\n",
        "    import moviepy.video.fx.all as vfx\n",
        "\n",
        "# ------------------ knobs ------------------\n",
        "OUT_W, OUT_H = 1080, 1920\n",
        "VIDEO_H = 1080\n",
        "PANEL_H = OUT_H - VIDEO_H\n",
        "\n",
        "FPS = 15  # 20 is smoother but slower; 12–15 is a good compromise\n",
        "\n",
        "SCROLL_WINDOW_S = None   # None = full window; e.g. 30 = scrolling last 30s\n",
        "\n",
        "# audio features\n",
        "AUDIO_SR = 22050\n",
        "LOUD_WIN_SEC, LOUD_HOP_SEC = 0.5, 0.1\n",
        "CENTROID_NPERSEG = 2048\n",
        "CENTROID_HOP = 512\n",
        "\n",
        "# If you need a tiny sync nudge:\n",
        "FINE_TUNE_OFFSET_S = 0.0\n",
        "\n",
        "# Render window defaults (DATA time)\n",
        "DEFAULT_DUR_S = 20.0\n",
        "\n",
        "# Drive save\n",
        "SAVE_TO_DRIVE = True\n",
        "DRIVE_OUT_DIR = \"/content/drive/MyDrive/EquiPhysics/renders\"\n",
        "\n",
        "# ------------------ checks ------------------\n",
        "if not globals().get(\"EQUIPHY\", {}).get(\"ready\", False):\n",
        "    raise RuntimeError(\"Run Cell 1 first (EQUIPHY must be ready).\")\n",
        "if \"video_path\" not in EQUIPHY:\n",
        "    raise RuntimeError(\"EQUIPHY['video_path'] missing.\")\n",
        "\n",
        "video_path = EQUIPHY[\"video_path\"]\n",
        "vid_to_data = float(EQUIPHY.get(\"video_to_data_offset_s\", 0.0)) + float(FINE_TUNE_OFFSET_S)\n",
        "\n",
        "if \"series\" not in EQUIPHY:\n",
        "    raise RuntimeError(\"EQUIPHY['series'] missing. Run Cell 2 first (recommended).\")\n",
        "\n",
        "S = EQUIPHY[\"series\"]\n",
        "\n",
        "# ------------------ mount drive + output dir ------------------\n",
        "if SAVE_TO_DRIVE:\n",
        "    from google.colab import drive\n",
        "    if not Path(\"/content/drive\").exists():\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    out_dir = Path(EQUIPHY.get(\"export_dir\", DRIVE_OUT_DIR))\n",
        "    if not str(out_dir).startswith(\"/content/drive\"):\n",
        "        out_dir = Path(DRIVE_OUT_DIR)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "else:\n",
        "    out_dir = Path(\".\")\n",
        "\n",
        "# ------------------ helpers ------------------\n",
        "def clip_by_t(t, y, t0, t1):\n",
        "    t = np.asarray(t, float); y = np.asarray(y, float)\n",
        "    m = np.isfinite(t) & (t >= t0) & (t <= t1)\n",
        "    return t[m], y[m]\n",
        "\n",
        "def fit_to_box(clip_in, box_w, box_h):\n",
        "    \"\"\"Resize to cover the box then center-crop (fill).\"\"\"\n",
        "    c = clip_in.resize(height=box_h)\n",
        "    if c.w < box_w:\n",
        "        c = clip_in.resize(width=box_w)\n",
        "    c = c.fx(vfx.crop, width=box_w, height=box_h, x_center=c.w/2, y_center=c.h/2)\n",
        "    return c\n",
        "\n",
        "def loudness_db(y, sr, win_sec=0.5, hop_sec=0.1, eps=1e-8):\n",
        "    y = np.asarray(y, float)\n",
        "    if y.ndim == 2:\n",
        "        y = y.mean(axis=1)\n",
        "    win = max(1, int(sr*win_sec))\n",
        "    hop = max(1, int(sr*hop_sec))\n",
        "    n = len(y)\n",
        "    starts = np.arange(0, max(0, n - win + 1), hop, dtype=int)\n",
        "    t = (starts + win/2) / sr\n",
        "    db = np.empty(len(starts), dtype=float)\n",
        "    for i, s in enumerate(starts):\n",
        "        frame = y[s:s+win]\n",
        "        rms = np.sqrt(np.mean(frame*frame) + eps)\n",
        "        db[i] = 20*np.log10(rms + eps)\n",
        "    return t, db\n",
        "\n",
        "def spectral_centroid(y, sr, nperseg=2048, hop=512, eps=1e-12):\n",
        "    y = np.asarray(y, float)\n",
        "    if y.ndim == 2:\n",
        "        y = y.mean(axis=1)\n",
        "    noverlap = max(0, nperseg - hop)\n",
        "    f, t, Z = stft(y, fs=sr, nperseg=nperseg, noverlap=noverlap, boundary=None)\n",
        "    mag = np.abs(Z).astype(np.float32)\n",
        "    denom = mag.sum(axis=0) + eps\n",
        "    cent = (f[:, None] * mag).sum(axis=0) / denom\n",
        "    return t, cent\n",
        "\n",
        "# ------------------ choose render window (DATA time) ------------------\n",
        "t0_default = float(EQUIPHY.get(\"t_start_s\", S.get(\"t0\", 0.0)))\n",
        "t1_default = float(EQUIPHY.get(\"t_stop_s\",  min(t0_default + DEFAULT_DUR_S, S.get(\"t1\", t0_default + DEFAULT_DUR_S))))\n",
        "\n",
        "print(f\"Suggested render window (DATA time): start={t0_default:.2f}, stop={t1_default:.2f}\")\n",
        "try:\n",
        "    render_t0 = float(input(f\"Render start (data s) [default {t0_default:.2f}]: \") or f\"{t0_default:.2f}\")\n",
        "    render_t1 = float(input(f\"Render stop  (data s) [default {t1_default:.2f}]: \") or f\"{t1_default:.2f}\")\n",
        "except Exception:\n",
        "    # If inputs don't show (Colab forms sometimes), fall back silently\n",
        "    render_t0, render_t1 = t0_default, t1_default\n",
        "\n",
        "if render_t1 <= render_t0:\n",
        "    raise ValueError(\"Render stop must be > render start.\")\n",
        "\n",
        "# ------------------ map DATA window to VIDEO subclip ------------------\n",
        "# data = video + offset  => video = data - offset\n",
        "base = VideoFileClip(video_path, audio=False)\n",
        "\n",
        "v0 = max(0.0, render_t0 - vid_to_data)\n",
        "v1 = min(float(base.duration), render_t1 - vid_to_data)\n",
        "\n",
        "if v1 <= v0:\n",
        "    base.close()\n",
        "    raise RuntimeError(\"Invalid video window after mapping. Check video_to_data_offset_s and render window.\")\n",
        "\n",
        "clip = base.subclip(v0, v1)     # keep base open until end\n",
        "duration = float(clip.duration)\n",
        "\n",
        "out_data_start = v0 + vid_to_data\n",
        "out_data_end = out_data_start + duration\n",
        "\n",
        "print(\"\\nRender mapping:\")\n",
        "print(f\"  DATA window requested: [{render_t0:.2f}, {render_t1:.2f}]\")\n",
        "print(f\"  VIDEO subclip:         [{v0:.2f}, {v0+duration:.2f}] (dur={duration:.2f}s)\")\n",
        "print(f\"  offset (data=video+off): {vid_to_data:+.3f}\")\n",
        "print(f\"  OUTPUT DATA window:    [{out_data_start:.2f}, {out_data_end:.2f}]\")\n",
        "\n",
        "# ------------------ pull series from EQUIPHY['series'] and clip to output ------------------\n",
        "t_beats = np.asarray(S.get(\"t_beats\", []), float)\n",
        "hr_inst = np.asarray(S.get(\"hr_inst\", []), float)\n",
        "rmssd   = np.asarray(S.get(\"rmssd_roll\", []), float)\n",
        "\n",
        "t_mov   = np.asarray(S.get(\"t_mov\", []), float)\n",
        "mov_idx = np.asarray(S.get(\"mov_idx\", []), float)\n",
        "\n",
        "t_beats_c, hr_c    = clip_by_t(t_beats, hr_inst, out_data_start, out_data_end)\n",
        "_,         rmssd_c = clip_by_t(t_beats, rmssd,   out_data_start, out_data_end)\n",
        "t_mov_c,   mov_c   = clip_by_t(t_mov,   mov_idx, out_data_start, out_data_end)\n",
        "\n",
        "if len(t_beats_c) < 2 or len(t_mov_c) < 2:\n",
        "    clip.close(); base.close()\n",
        "    raise RuntimeError(\"Not enough HR/movement points in this window. Pick a wider window or check Cell 2 export.\")\n",
        "\n",
        "# ------------------ extract subclip audio via ffmpeg (robust) ------------------\n",
        "wav_path = \"_reel_audio.wav\"\n",
        "audio_clip = None\n",
        "t_loud_data = np.array([out_data_start, out_data_end], float)\n",
        "loud_db = np.array([np.nan, np.nan], float)\n",
        "t_cent_data = np.array([out_data_start, out_data_end], float)\n",
        "cent_hz = np.array([np.nan, np.nan], float)\n",
        "\n",
        "ff_cmd = [\n",
        "    \"ffmpeg\", \"-y\",\n",
        "    \"-i\", video_path,\n",
        "    \"-ss\", f\"{v0:.6f}\",\n",
        "    \"-t\",  f\"{duration:.6f}\",\n",
        "    \"-vn\",\n",
        "    \"-ac\", \"1\",\n",
        "    \"-ar\", str(AUDIO_SR),\n",
        "    \"-c:a\", \"pcm_s16le\",\n",
        "    wav_path\n",
        "]\n",
        "try:\n",
        "    subprocess.run(ff_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    if os.path.exists(wav_path) and os.path.getsize(wav_path) > 0:\n",
        "        audio_clip = AudioFileClip(wav_path).set_duration(duration)\n",
        "\n",
        "        import soundfile as sf\n",
        "        y_aud, sr_aud = sf.read(wav_path, always_2d=False)\n",
        "        if y_aud.ndim == 2:\n",
        "            y_aud = y_aud.mean(axis=1)\n",
        "\n",
        "        # features in subclip time (0..duration), then map -> DATA time\n",
        "        tL, dB = loudness_db(y_aud.astype(np.float32), sr_aud, win_sec=LOUD_WIN_SEC, hop_sec=LOUD_HOP_SEC)\n",
        "        t_loud_data = (v0 + tL) + vid_to_data\n",
        "        t_loud_data, loud_db = clip_by_t(t_loud_data, dB, out_data_start, out_data_end)\n",
        "\n",
        "        tC, cHz = spectral_centroid(y_aud.astype(np.float32), sr_aud, nperseg=CENTROID_NPERSEG, hop=CENTROID_HOP)\n",
        "        t_cent_data = (v0 + tC) + vid_to_data\n",
        "        t_cent_data, cent_hz = clip_by_t(t_cent_data, cHz, out_data_start, out_data_end)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"WARNING: could not extract/attach audio or compute audio features:\", e)\n",
        "    audio_clip = None\n",
        "\n",
        "# ------------------ prep top video (crop to fill) ------------------\n",
        "clip_top = fit_to_box(clip, OUT_W, VIDEO_H)\n",
        "\n",
        "# ------------------ dashboard renderer ------------------\n",
        "class DashboardRenderer:\n",
        "    def __init__(self):\n",
        "        self.fig = plt.figure(figsize=(OUT_W/100, PANEL_H/100), dpi=100)\n",
        "        self.canvas = FigureCanvas(self.fig)\n",
        "\n",
        "        # 6 rows: HR, RMSSD, Move, Loud, Centroid, Progress\n",
        "        gs = self.fig.add_gridspec(6, 1, height_ratios=[1,1,1,1,1,0.32], hspace=0.28)\n",
        "\n",
        "        self.ax_hr   = self.fig.add_subplot(gs[0,0])\n",
        "        self.ax_rms  = self.fig.add_subplot(gs[1,0], sharex=self.ax_hr)\n",
        "        self.ax_mov  = self.fig.add_subplot(gs[2,0], sharex=self.ax_hr)\n",
        "        self.ax_loud = self.fig.add_subplot(gs[3,0], sharex=self.ax_hr)\n",
        "        self.ax_cent = self.fig.add_subplot(gs[4,0], sharex=self.ax_hr)\n",
        "        self.ax_bar  = self.fig.add_subplot(gs[5,0], sharex=self.ax_hr)\n",
        "\n",
        "        # static plots\n",
        "        self.ax_hr.plot(t_beats_c, hr_c, linewidth=2)\n",
        "        self.ax_hr.set_ylabel(\"HR\")\n",
        "        self.ax_hr.grid(True, alpha=0.25)\n",
        "\n",
        "        self.ax_rms.plot(t_beats_c, rmssd_c, linewidth=2)\n",
        "        self.ax_rms.set_ylabel(\"RMSSD\")\n",
        "        self.ax_rms.grid(True, alpha=0.25)\n",
        "\n",
        "        self.ax_mov.plot(t_mov_c, mov_c, linewidth=2)\n",
        "        self.ax_mov.set_ylabel(\"Move\")\n",
        "        self.ax_mov.grid(True, alpha=0.25)\n",
        "\n",
        "        self.ax_loud.plot(t_loud_data, loud_db, linewidth=2)\n",
        "        self.ax_loud.set_ylabel(\"Loud dB\")\n",
        "        self.ax_loud.grid(True, alpha=0.25)\n",
        "\n",
        "        self.ax_cent.plot(t_cent_data, cent_hz, linewidth=2)\n",
        "        self.ax_cent.set_ylabel(\"Freq Hz\")\n",
        "        self.ax_cent.grid(True, alpha=0.25)\n",
        "        self.ax_cent.set_xlabel(\"DATA time (s)\")\n",
        "\n",
        "        # cursor lines\n",
        "        self.vlines = [\n",
        "            self.ax_hr.axvline(out_data_start, linewidth=3),\n",
        "            self.ax_rms.axvline(out_data_start, linewidth=3),\n",
        "            self.ax_mov.axvline(out_data_start, linewidth=3),\n",
        "            self.ax_loud.axvline(out_data_start, linewidth=3),\n",
        "            self.ax_cent.axvline(out_data_start, linewidth=3),\n",
        "        ]\n",
        "\n",
        "        # progress bar\n",
        "        self.ax_bar.set_ylim(0, 1)\n",
        "        self.ax_bar.set_yticks([])\n",
        "        self.ax_bar.grid(False)\n",
        "        self.bar_line = self.ax_bar.axvline(out_data_start, linewidth=3)\n",
        "        self.bar_fill = self.ax_bar.axvspan(out_data_start, out_data_start, ymin=0, ymax=1, alpha=0.35)\n",
        "\n",
        "        # initial xlim\n",
        "        for ax in [self.ax_hr, self.ax_rms, self.ax_mov, self.ax_loud, self.ax_cent, self.ax_bar]:\n",
        "            ax.set_xlim(out_data_start, out_data_end)\n",
        "\n",
        "        self.fig.tight_layout()\n",
        "\n",
        "    def __call__(self, t_sub):\n",
        "        data_t = out_data_start + float(t_sub)\n",
        "\n",
        "        if SCROLL_WINDOW_S is None:\n",
        "            xmin, xmax = out_data_start, out_data_end\n",
        "        else:\n",
        "            xmin = max(out_data_start, data_t - SCROLL_WINDOW_S)\n",
        "            xmax = min(out_data_end, xmin + SCROLL_WINDOW_S)\n",
        "            if xmax <= xmin:\n",
        "                xmax = xmin + 1.0\n",
        "\n",
        "        for ax in [self.ax_hr, self.ax_rms, self.ax_mov, self.ax_loud, self.ax_cent, self.ax_bar]:\n",
        "            ax.set_xlim(xmin, xmax)\n",
        "\n",
        "        for vl in self.vlines:\n",
        "            vl.set_xdata([data_t, data_t])\n",
        "\n",
        "        self.bar_line.set_xdata([data_t, data_t])\n",
        "        self.bar_fill.remove()\n",
        "        self.bar_fill = self.ax_bar.axvspan(out_data_start, min(data_t, out_data_end), ymin=0, ymax=1, alpha=0.35)\n",
        "\n",
        "        self.canvas.draw()\n",
        "        rgba = np.asarray(self.canvas.buffer_rgba())\n",
        "        return rgba[..., :3].copy()\n",
        "\n",
        "panel_clip = VideoClip(DashboardRenderer(), duration=duration).set_fps(FPS)\n",
        "panel_clip = panel_clip.resize((OUT_W, PANEL_H))\n",
        "\n",
        "# ------------------ compose portrait reel ------------------\n",
        "final = CompositeVideoClip(\n",
        "    [\n",
        "        clip_top.set_position((0, 0)),\n",
        "        panel_clip.set_position((0, VIDEO_H)),\n",
        "    ],\n",
        "    size=(OUT_W, OUT_H)\n",
        ").set_duration(duration)\n",
        "\n",
        "if audio_clip is not None:\n",
        "    final = final.set_audio(audio_clip)\n",
        "\n",
        "# ------------------ render + save ------------------\n",
        "stem = Path(video_path).stem[:30].replace(\" \", \"_\")\n",
        "out_name = f\"{stem}__reel__data_{out_data_start:.1f}_{out_data_end:.1f}.mp4\"\n",
        "out_path = out_dir / out_name\n",
        "\n",
        "print(\"\\nWriting:\", out_path)\n",
        "final.write_videofile(\n",
        "    str(out_path),\n",
        "    fps=FPS,\n",
        "    codec=\"libx264\",\n",
        "    audio_codec=\"aac\",\n",
        "    threads=4,\n",
        "    preset=\"medium\",\n",
        "    verbose=False,\n",
        "    logger=None\n",
        ")\n",
        "\n",
        "# ------------------ cleanup ------------------\n",
        "try:\n",
        "    if audio_clip is not None:\n",
        "        audio_clip.close()\n",
        "except: pass\n",
        "try:\n",
        "    final.close()\n",
        "except: pass\n",
        "try:\n",
        "    panel_clip.close()\n",
        "except: pass\n",
        "try:\n",
        "    clip.close()\n",
        "except: pass\n",
        "try:\n",
        "    base.close()\n",
        "except: pass\n",
        "try:\n",
        "    if os.path.exists(wav_path):\n",
        "        os.remove(wav_path)\n",
        "except: pass\n",
        "\n",
        "print(\"\\n✅ Saved reel to:\", out_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340d3149"
      },
      "source": [
        "## Cell 5: Correlation & Causality Analysis\n",
        "\n",
        "**Purpose:** This cell performs advanced statistical analysis to explore relationships between the horse's physiological state (HR, HRV), movement, and the music features.\n",
        "\n",
        "**What it does:**\n",
        "*   **Aligns Data:** Resamples all time series onto a common time grid (defined by `DT`).\n",
        "*   **Correlation Analysis:** Calculates Pearson and Spearman correlations to find linear and monotonic relationships. It also checks for time-lagged correlations (e.g., does HR rise 5 seconds *after* the music gets loud?).\n",
        "*   **Granger Causality:** Tests for predictive relationships. If \"Music Granger-causes HR\", it means past values of music features help predict future HR values better than HR history alone.\n",
        "\n",
        "**How to use it:**\n",
        "1.  **Run the cell:** Just execute the code. It uses the data processed in Cell 2.\n",
        "2.  **Review the Output:** Look at the printed tables and plots.\n",
        "    *   **Correlation Table:** Look for high absolute `pearson_r` values (close to 1 or -1) with low `pearson_p` values (< 0.05). `best_lag_s` tells you the time delay.\n",
        "    *   **Granger Table:** Look for low `best_p` values (< 0.05). This suggests a potential predictive link.\n",
        "3.  **Adjust Parameters (Optional):**\n",
        "    *   `DT`: Resampling interval in seconds (default `1.0`). Smaller values (e.g., `0.5`) give higher resolution but might be noisier.\n",
        "    *   `MAX_LAG_S`: Max time lag to check (default `30` seconds).\n",
        "\n",
        "**Note:** \"Granger causality\" is a statistical concept of predictability, not necessarily proof of real-world cause-and-effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "d6ff67dd"
      },
      "source": [
        "# @title\n",
        "# ==========================================================\n",
        "# Cell 5: Correlations + lag analysis + Granger Causality\n",
        "#   music metrics -> (HR, HRV, movement)\n",
        "#   movement -> (HR, HRV)\n",
        "#\n",
        "# Assumes Cell 2 ran and created EQUIPHY[\"series\"].\n",
        "# Notes:\n",
        "# - Correlation is not causation.\n",
        "# - Granger causality tests whether X helps predict future Y (linear, predictive).\n",
        "# ==========================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# statsmodels is used for Granger tests\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "S = EQUIPHY.get(\"series\", None)\n",
        "if S is None:\n",
        "    raise RuntimeError(\"EQUIPHY['series'] not found. Run Cell 2 first.\")\n",
        "\n",
        "# ----------------- user-tunable parameters -----------------\n",
        "DT = 1.0               # seconds, common resample grid (try 0.5 or 0.2 for higher-res)\n",
        "MAX_LAG_S = 30         # seconds, lag search window for correlation and info-transfer tests\n",
        "MAX_GRANGER_LAG_S = 15 # seconds, max Granger lag (kept smaller than MAX_LAG_S for stability)\n",
        "USE_DIFF_FOR_GRANGER = True  # difference series before Granger to reduce trend-driven false positives\n",
        "MIN_OBS_FOR_CORR = 20  # Minimum number of overlapping observations for a valid correlation\n",
        "MIN_OBS_FOR_GRANGER = 50 # Minimum number of observations for Granger test (higher for stability)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def _interp_to_grid(t, y, t_grid):\n",
        "    \"\"\"Linear interpolation onto t_grid. Returns NaNs where interpolation is impossible.\"\"\"\n",
        "    t = np.asarray(t, dtype=float)\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    m = np.isfinite(t) & np.isfinite(y)\n",
        "    if m.sum() < 2:\n",
        "        return np.full_like(t_grid, np.nan, dtype=float)\n",
        "    tt = t[m]\n",
        "    yy = y[m]\n",
        "\n",
        "    # Sort based on time\n",
        "    idx = np.argsort(tt)\n",
        "    tt = tt[idx]\n",
        "    yy = yy[idx]\n",
        "\n",
        "    # np.interp requires increasing tt\n",
        "    return np.interp(t_grid, tt, yy, left=np.nan, right=np.nan)\n",
        "\n",
        "def _zscore(x):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    m = np.isfinite(x)\n",
        "    if m.sum() < 3:\n",
        "        return x\n",
        "    mu = np.nanmean(x[m])\n",
        "    sd = np.nanstd(x[m])\n",
        "    if sd <= 0:\n",
        "        return x - mu\n",
        "    return (x - mu) / sd\n",
        "\n",
        "def _pairwise_corr(x, y):\n",
        "    \"\"\"Pearson + Spearman, with p-values, ignoring NaNs.\"\"\"\n",
        "    x = np.asarray(x, float)\n",
        "    y = np.asarray(y, float)\n",
        "    m = np.isfinite(x) & np.isfinite(y)\n",
        "    if m.sum() < MIN_OBS_FOR_CORR:\n",
        "        return dict(n=int(m.sum()), pearson_r=np.nan, pearson_p=np.nan, spearman_rho=np.nan, spearman_p=np.nan)\n",
        "    pr, pp = stats.pearsonr(x[m], y[m])\n",
        "    sr, sp = stats.spearmanr(x[m], y[m])\n",
        "    return dict(n=int(m.sum()), pearson_r=float(pr), pearson_p=float(pp), spearman_rho=float(sr), spearman_p=float(sp))\n",
        "\n",
        "def _partial_corr(x, y, covars):\n",
        "    \"\"\"Partial Pearson correlation between x and y controlling for covars (linear residualization).\"\"\"\n",
        "    x = np.asarray(x, float)\n",
        "    y = np.asarray(y, float)\n",
        "    C = np.asarray(covars, float)\n",
        "    if C.ndim == 1:\n",
        "        C = C[:, None]\n",
        "    m = np.isfinite(x) & np.isfinite(y) & np.all(np.isfinite(C), axis=1)\n",
        "    if m.sum() < (MIN_OBS_FOR_CORR + 10): # Need more observations for partial corr\n",
        "        return dict(n=int(m.sum()), r=np.nan, p=np.nan)\n",
        "    X = np.column_stack([np.ones(m.sum()), C[m]])\n",
        "    # residualize x and y\n",
        "    bx, *_ = np.linalg.lstsq(X, x[m], rcond=None)\n",
        "    by, *_ = np.linalg.lstsq(X, y[m], rcond=None)\n",
        "    rx = x[m] - X @ bx\n",
        "    ry = y[m] - X @ by\n",
        "    r, p = stats.pearsonr(rx, ry)\n",
        "    return dict(n=float(m.sum()), r=float(r), p=float(p))\n",
        "\n",
        "def _best_lag_corr(x, y, dt, max_lag_s):\n",
        "    \"\"\"\n",
        "    Find lag (in seconds) maximizing |corr| when x leads y.\n",
        "    We compute corr(x(t), y(t+lag)) for lag in [-max_lag, +max_lag].\n",
        "    Positive lag means x leads y by lag seconds.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, float)\n",
        "    y = np.asarray(y, float)\n",
        "    max_k = int(round(max_lag_s / dt))\n",
        "    lags = np.arange(-max_k, max_k + 1, dtype=int)\n",
        "    best = dict(lag_s=np.nan, r=np.nan, n=0)\n",
        "\n",
        "    for k in lags:\n",
        "        if k == 0:\n",
        "            xx, yy = x, y\n",
        "        elif k > 0:\n",
        "            xx, yy = x[:-k], y[k:]\n",
        "        else:\n",
        "            kk = -k\n",
        "            xx, yy = x[kk:], y[:-kk]\n",
        "        m = np.isfinite(xx) & np.isfinite(yy)\n",
        "        if m.sum() < MIN_OBS_FOR_CORR:\n",
        "            continue\n",
        "        r = np.corrcoef(xx[m], yy[m])[0, 1]\n",
        "        if not np.isfinite(r):\n",
        "            continue\n",
        "        if (not np.isfinite(best[\"r\"])) or (abs(r) > abs(best[\"r\"])): # Maximize absolute correlation\n",
        "            best = dict(lag_s=float(k * dt), r=float(r), n=int(m.sum()))\n",
        "    return best\n",
        "\n",
        "def _granger_best_p(x, y, maxlag_steps):\n",
        "    \"\"\"\n",
        "    Granger test x -> y using statsmodels.\n",
        "    Returns best p-value (ssr_ftest) across lags and the lag where it occurs.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, float)\n",
        "    y = np.asarray(y, float)\n",
        "    m = np.isfinite(x) & np.isfinite(y)\n",
        "    if m.sum() < MIN_OBS_FOR_GRANGER:\n",
        "        return dict(n=int(m.sum()), best_p=np.nan, best_lag=np.nan)\n",
        "    xx = x[m]\n",
        "    yy = y[m]\n",
        "    if USE_DIFF_FOR_GRANGER:\n",
        "        xx = np.diff(xx)\n",
        "        yy = np.diff(yy)\n",
        "        if len(xx) < (maxlag_steps + 20):\n",
        "            return dict(n=len(xx), best_p=np.nan, best_lag=np.nan)\n",
        "    data = np.column_stack([yy, xx])  # [y, x]\n",
        "    try:\n",
        "        res = grangercausalitytests(data, maxlag=maxlag_steps, verbose=False)\n",
        "        pvals = []\n",
        "        for lag, out in res.items():\n",
        "            p = out[0][\"ssr_ftest\"][1]\n",
        "            pvals.append((lag, p))\n",
        "        if not pvals:\n",
        "             return dict(n=int(len(xx)), best_p=np.nan, best_lag=np.nan)\n",
        "        lag_best, p_best = min(pvals, key=lambda t: t[1])\n",
        "        return dict(n=int(len(xx)), best_p=float(p_best), best_lag=float(lag_best))\n",
        "    except Exception:\n",
        "        return dict(n=int(len(xx)), best_p=np.nan, best_lag=np.nan)\n",
        "\n",
        "# ----------------- Build a common, aligned dataframe -----------------\n",
        "t0 = float(S[\"t0\"]); t1 = float(S[\"t1\"])\n",
        "t_grid = np.arange(t0, t1 + 1e-12, DT)\n",
        "\n",
        "# Core physiology/movement\n",
        "HR = _interp_to_grid(S[\"t_beats\"], S[\"hr_inst\"], t_grid)\n",
        "RMSSD = _interp_to_grid(S[\"t_beats\"], S[\"rmssd_roll\"], t_grid)\n",
        "MOV = _interp_to_grid(S[\"t_mov\"], S[\"mov_idx\"], t_grid)\n",
        "\n",
        "# Optional: SDNN rolling (computed on beats, then interpolated)\n",
        "SDNN = np.full_like(t_grid, np.nan, dtype=float)\n",
        "try:\n",
        "    rr = np.asarray(S[\"rr_ms\"], float)\n",
        "    tb = np.asarray(S[\"t_beats\"], float)\n",
        "    m_rr = np.isfinite(rr) & np.isfinite(tb)\n",
        "    rr = rr[m_rr]; tb = tb[m_rr]\n",
        "    if len(rr) > (RMSSD_WINDOW_BEATS + 10): # Ensure enough beats for SDNN calculation\n",
        "        win = RMSSD_WINDOW_BEATS  # beats, match RMSSD_WINDOW_BEATS from Cell 2\n",
        "        sdnn_roll = pd.Series(rr).rolling(win, min_periods=max(10, win//2)).std(ddof=1).to_numpy()\n",
        "        SDNN = _interp_to_grid(tb, sdnn_roll, t_grid)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"t_s\": t_grid,\n",
        "    \"hr_bpm\": HR,\n",
        "    \"rmssd_ms\": RMSSD,\n",
        "    \"sdnn_ms\": SDNN,\n",
        "    \"mov_idx\": MOV,\n",
        "})\n",
        "\n",
        "# Audio/music metrics, if present\n",
        "music_cols = []\n",
        "if bool(S.get(\"have_audio\", False)) and (S.get(\"t_loud\", None) is not None):\n",
        "    df[\"loud_db\"] = _interp_to_grid(S[\"t_loud\"], S[\"loud_db\"], t_grid); music_cols.append(\"loud_db\")\n",
        "if bool(S.get(\"have_audio\", False)) and (S.get(\"t_flux\", None) is not None):\n",
        "    df[\"flux_z\"] = _interp_to_grid(S[\"t_flux\"], S[\"flux_z\"], t_grid); music_cols.append(\"flux_z\")\n",
        "if bool(S.get(\"have_audio\", False)) and (S.get(\"t_cent\", None) is not None):\n",
        "    df[\"centroid_hz\"] = _interp_to_grid(S[\"t_cent\"], S[\"centroid_sm\"], t_grid); music_cols.append(\"centroid_hz\")\n",
        "\n",
        "# Report\n",
        "print(f\"Aligned dataframe: {len(df)} samples @ DT={DT:.3f}s over [{t0:.1f}, {t1:.1f}] s\")\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# ----------------- Correlations -----------------\n",
        "print(\"\\n--- Zero-lag & Lagged Correlations ---\")\n",
        "targets_phys = [\"hr_bpm\", \"rmssd_ms\", \"sdnn_ms\", \"mov_idx\"]\n",
        "targets_hrv = [\"rmssd_ms\", \"sdnn_ms\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "# music -> (HR, HRV, movement)\n",
        "for x in music_cols:\n",
        "    for y in targets_phys:\n",
        "        out = _pairwise_corr(df[x], df[y])\n",
        "        lagbest = _best_lag_corr(df[x], df[y], DT, MAX_LAG_S)\n",
        "        pc = None\n",
        "        # partial corr controlling for movement (only makes sense for y=HR/HRV)\n",
        "        if y in ([\"hr_bpm\"] + targets_hrv):\n",
        "            pc = _partial_corr(df[x], df[y], covars=df[\"mov_idx\"])\n",
        "        rows.append({\n",
        "            \"x\": x, \"y\": y,\n",
        "            **out,\n",
        "            \"best_lag_s (x leads y)\": lagbest[\"lag_s\"],\n",
        "            \"best_lag_r\": lagbest[\"r\"],\n",
        "            \"best_lag_n\": lagbest[\"n\"],\n",
        "            \"partial_r | mov\": (pc[\"r\"] if pc else np.nan),\n",
        "            \"partial_p | mov\": (pc[\"p\"] if pc else np.nan),\n",
        "            \"partial_n | mov\": (pc[\"n\"] if pc else np.nan),\n",
        "        })\n",
        "\n",
        "# movement -> (HR, HRV)\n",
        "for y in [\"hr_bpm\"] + targets_hrv:\n",
        "    out = _pairwise_corr(df[\"mov_idx\"], df[y])\n",
        "    lagbest = _best_lag_corr(df[\"mov_idx\"], df[y], DT, MAX_LAG_S)\n",
        "    rows.append({\n",
        "        \"x\": \"mov_idx\", \"y\": y,\n",
        "        **out,\n",
        "        \"best_lag_s (x leads y)\": lagbest[\"lag_s\"],\n",
        "        \"best_lag_r\": lagbest[\"r\"],\n",
        "        \"best_lag_n\": lagbest[\"n\"],\n",
        "        \"partial_r | mov\": np.nan, # Not applicable for mov -> other, controlling for mov\n",
        "        \"partial_p | mov\": np.nan,\n",
        "        \"partial_n | mov\": np.nan,\n",
        "    })\n",
        "\n",
        "corr_tbl = pd.DataFrame(rows)\n",
        "# Filter out rows with insufficient observations or NaN correlations\n",
        "corr_tbl = corr_tbl[ (corr_tbl['n'] >= MIN_OBS_FOR_CORR) &\n",
        "                     (corr_tbl['pearson_r'].notna()) &\n",
        "                     (corr_tbl['best_lag_n'] >= MIN_OBS_FOR_CORR) &\n",
        "                     (corr_tbl['best_lag_r'].notna())].copy()\n",
        "\n",
        "# Sort by strongest absolute zero-lag Pearson correlation\n",
        "corr_tbl[\"abs_pearson\"] = np.abs(corr_tbl[\"pearson_r\"])\n",
        "corr_tbl = corr_tbl.sort_values([\"abs_pearson\"], ascending=False).drop(columns=[\"abs_pearson\"])\n",
        "\n",
        "# Format for display\n",
        "corr_tbl_display = corr_tbl.round({\n",
        "    \"n\": 0, \"pearson_r\": 3, \"pearson_p\": 4,\n",
        "    \"spearman_rho\": 3, \"spearman_p\": 4,\n",
        "    \"best_lag_s (x leads y)\": 1, \"best_lag_r\": 3, \"best_lag_n\": 0,\n",
        "    \"partial_r | mov\": 3, \"partial_p | mov\": 4, \"partial_n | mov\": 0\n",
        "})\n",
        "# Use Int64 (nullable int) to handle NaNs in integer columns without error\n",
        "corr_tbl_display[\"n\"] = corr_tbl_display[\"n\"].astype(\"Int64\")\n",
        "corr_tbl_display[\"best_lag_n\"] = corr_tbl_display[\"best_lag_n\"].astype(\"Int64\")\n",
        "corr_tbl_display[\"partial_n | mov\"] = corr_tbl_display[\"partial_n | mov\"].astype(\"Int64\")\n",
        "\n",
        "print(\"Interpretation of Correlation Table:\")\n",
        "print(\"  - 'x' vs 'y': The two time series being compared.\")\n",
        "print(\"  - 'n': Number of overlapping, non-NaN data points for zero-lag correlation.\")\n",
        "print(\"  - 'pearson_r': Linear correlation coefficient (range -1 to 1).\")\n",
        "print(\"  - 'pearson_p': P-value for Pearson correlation (lower is more significant).\")\n",
        "print(\"  - 'spearman_rho': Rank correlation coefficient, useful for non-linear relationships.\")\n",
        "print(\"  - 'spearman_p': P-value for Spearman correlation.\")\n",
        "print(\"  - 'best_lag_s (x leads y)': The time lag (in seconds) where the absolute correlation between x and y is maximized. A positive lag means 'x' occurs before 'y'.\")\n",
        "print(\"  - 'best_lag_r': The correlation coefficient at the best lag.\")\n",
        "print(\"  - 'partial_r | mov': Partial Pearson correlation controlling for 'mov_idx'. This helps to see if a relationship between 'x' and 'y' persists even when accounting for movement.\")\n",
        "display(corr_tbl_display)\n",
        "\n",
        "# ----------------- Granger causality (predictive directionality) -----------------\n",
        "print(\"\\n--- Granger Causality (Predictive Directionality) ---\")\n",
        "# Granger causality checks if past values of X help predict Y better than just past values of Y.\n",
        "# We prep by z-scoring (standardizing) the data first.\n",
        "def _prep_for_granger(a, b):\n",
        "    aa = _zscore(a)\n",
        "    bb = _zscore(b)\n",
        "    return aa, bb\n",
        "\n",
        "maxlag_steps = max(1, int(round(MAX_GRANGER_LAG_S / DT))) # Convert seconds to steps\n",
        "info_rows = []\n",
        "\n",
        "def _do_granger_pair(xname, yname):\n",
        "    x, y = _prep_for_granger(df[xname].to_numpy(), df[yname].to_numpy())\n",
        "    g = _granger_best_p(x, y, maxlag_steps=maxlag_steps)\n",
        "    return g\n",
        "\n",
        "# music -> (HR, HRV, movement)\n",
        "for x in music_cols:\n",
        "    for y in targets_phys:\n",
        "        g = _do_granger_pair(x, y)\n",
        "        info_rows.append({\"method\":\"Granger\", \"x\":x, \"y\":y, \"best_p\":g[\"best_p\"], \"best_lag_steps\":g[\"best_lag\"], \"n\":g[\"n\"]})\n",
        "\n",
        "# movement -> (HR, HRV)\n",
        "for y in [\"hr_bpm\"] + targets_hrv:\n",
        "    g = _do_granger_pair(\"mov_idx\", y)\n",
        "    info_rows.append({\"method\":\"Granger\", \"x\":\"mov_idx\", \"y\":y, \"best_p\":g[\"best_p\"], \"best_lag_steps\":g[\"best_lag\"], \"n\":g[\"n\"]})\n",
        "\n",
        "g_tbl = pd.DataFrame(info_rows)\n",
        "# Filter out rows with insufficient observations or NaN p-values\n",
        "g_tbl = g_tbl[ (g_tbl['n'] >= MIN_OBS_FOR_GRANGER) & (g_tbl['best_p'].notna()) ].copy()\n",
        "\n",
        "g_tbl_display = g_tbl.sort_values([\"best_p\"], ascending=True).round({\"best_p\": 4, \"best_lag_steps\": 0, \"n\": 0})\n",
        "g_tbl_display[\"n\"] = g_tbl_display[\"n\"].astype(\"Int64\")\n",
        "\n",
        "print(\"Interpretation of Granger Causality Table:\")\n",
        "print(\"  - 'x' -> 'y': Tests if 'x' can predict future values of 'y'.\")\n",
        "print(\"  - 'n': Number of observations used for the test (after differencing if enabled).\")\n",
        "print(\"  - 'best_p': Smallest p-value across all tested lags. A p-value < 0.05 (or other significance level) suggests 'x' Granger-causes 'y'.\")\n",
        "print(\"  - 'best_lag_steps': The lag (in DT steps) corresponding to the 'best_p'.\")\n",
        "print(\"Note: Granger causality indicates predictive relationship, not necessarily direct causation.\")\n",
        "display(g_tbl_display)\n",
        "\n",
        "# ----------------- Quick plots -----------------\n",
        "# 1) Zero-lag Pearson r heatmap (music + mov) vs outputs\n",
        "heat_x = music_cols + [\"mov_idx\"]\n",
        "heat_y = [\"hr_bpm\", \"rmssd_ms\", \"sdnn_ms\", \"mov_idx\"]\n",
        "H = np.full((len(heat_x), len(heat_y)), np.nan, dtype=float)\n",
        "\n",
        "for i, x_var in enumerate(heat_x):\n",
        "    for j, y_var in enumerate(heat_y):\n",
        "        m = np.isfinite(df[x_var]) & np.isfinite(df[y_var])\n",
        "        if m.sum() >= MIN_OBS_FOR_CORR:\n",
        "            H[i, j] = np.corrcoef(df.loc[m, x_var], df.loc[m, y_var])[0, 1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(1.2*len(heat_y)+2, 0.6*len(heat_x)+2))\n",
        "im = ax.imshow(H, aspect=\"auto\", vmin=-1, vmax=1, cmap=\"RdBu\")\n",
        "ax.set_xticks(range(len(heat_y))); ax.set_xticklabels(heat_y, rotation=45, ha=\"right\")\n",
        "ax.set_yticks(range(len(heat_x))); ax.set_yticklabels(heat_x)\n",
        "ax.set_title(\"Zero-lag Pearson correlation (r)\")\n",
        "fig.colorbar(im, ax=ax, shrink=0.8)\n",
        "plt.show()\n",
        "\n",
        "# 2) Lag-correlation curve for a couple of key pairs (edit list if desired)\n",
        "def _plot_lag_curve(xname, yname):\n",
        "    x = df[xname].to_numpy(float)\n",
        "    y = df[yname].to_numpy(float)\n",
        "    max_k = int(round(MAX_LAG_S / DT))\n",
        "    lags = np.arange(-max_k, max_k+1, dtype=int)\n",
        "    rs = np.full_like(lags, np.nan, dtype=float)\n",
        "    ns = np.zeros_like(lags, dtype=int)\n",
        "    for ii, k in enumerate(lags):\n",
        "        if k == 0:\n",
        "            xx, yy = x, y\n",
        "        elif k > 0:\n",
        "            xx, yy = x[:-k], y[k:]\n",
        "        else:\n",
        "            kk = -k\n",
        "            xx, yy = x[kk:], y[:-kk]\n",
        "        m = np.isfinite(xx) & np.isfinite(yy)\n",
        "        ns[ii] = int(m.sum())\n",
        "        if m.sum() >= MIN_OBS_FOR_CORR:\n",
        "            rs[ii] = np.corrcoef(xx[m], yy[m])[0, 1]\n",
        "    fig, ax = plt.subplots(figsize=(7,3))\n",
        "    ax.plot(lags*DT, rs)\n",
        "    ax.axvline(0, linestyle=\"--\", color='gray')\n",
        "    ax.axhline(0, linestyle=\"--\", color='gray')\n",
        "    ax.set_xlabel(\"Lag (s)  [positive => x leads y]\")\n",
        "    ax.set_ylabel(\"Correlation (r)\")\n",
        "    ax.set_title(f\"Lag correlation: {xname} → {yname}\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Pick a couple representative pairs (customize if you want)\n",
        "if len(music_cols) > 0:\n",
        "    print(f\"\\nLagged correlation plot for {music_cols[0]} -> hr_bpm:\")\n",
        "    _plot_lag_curve(music_cols[0], \"hr_bpm\")\n",
        "    print(f\"\\nLagged correlation plot for mov_idx -> hr_bpm:\")\n",
        "else:\n",
        "    print(\"\\nNo music columns available for lagged correlation plots.\")\n",
        "\n",
        "_plot_lag_curve(\"mov_idx\", \"hr_bpm\")\n",
        "\n",
        "# ----------------- Save outputs next to export_dir if available -----------------\n",
        "out_dir = Path(EQUIPHY.get(\"export_dir\", \".\"))\n",
        "try:\n",
        "    analysis_dir = out_dir / \"analysis\"\n",
        "    analysis_dir.mkdir(parents=True, exist_ok=True)\n",
        "    corr_tbl_display.to_csv(analysis_dir / \"corr_summary.csv\", index=False)\n",
        "    g_tbl_display.to_csv(analysis_dir / \"granger_summary.csv\", index=False)\n",
        "    print(f\"\\n✅ Saved filtered and formatted analysis CSVs to: {analysis_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n⚠️ Could not save analysis outputs: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99f18811"
      },
      "source": [
        "## Analysis Interpretation & Notes\n",
        "\n",
        "Use this section to synthesize the statistical results from Cell 5. Do the numbers support your visual observations?\n",
        "\n",
        "---\n",
        "\n",
        "**Correlation Findings:** [e.g., Strong negative correlation between Music Flux and HRV, suggesting intense music lowers HRV.]\n",
        "\n",
        "**Time Lag Observations:** [e.g., HR seems to react to Loudness with a lag of ~5 seconds.]\n",
        "\n",
        "**Causality (Granger) Results:** [e.g., Movement significantly predicts HRV changes (p<0.05). Did music features show any predictive power?]\n",
        "\n",
        "**Overall Conclusion:** [e.g., While there are some correlations, movement appears to be the primary driver of physiological changes, with music playing a secondary role.]\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}