{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1Am3ItEozC/X6wxdnKAGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/equiphysics/education/blob/main/labs/Duque-Exercise/Duque_Lunge_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style='color:#e91e63'>Duque Weekly Lunge Test: HR + HRV Data Analysis</span>\n",
        "\n",
        "This notebook is for **week-to-week tracking** during Duque’s 90-day fitness routine. The goal is simple: **keep the workout standardized**, collect clean heart data, and use the numbers to ask better questions—not to “diagnose” anything.\n",
        "\n",
        "---\n",
        "\n",
        "## How to use this notebook\n",
        "- Read the prompt for each section.\n",
        "- Run the code cells (Shift+Enter).\n",
        "- When you see **#TODO**, you can change a few settings (you do not need to write new code).\n",
        "- Use the text “Answer” cells to write short interpretations and record field notes.\n",
        "\n",
        "---\n",
        "\n",
        "## What we’re measuring (and why)\n",
        "- **Heart Rate (HR):** how hard the heart is working during the session.\n",
        "- **Heart Rate Variability (HRV):** beat-to-beat timing variation (from RR / NN intervals). Most useful in **quiet standing** and **standing recovery**.\n",
        "- **Recovery:** how quickly HR drops after the trot sets.\n",
        "\n",
        "---\n",
        "\n",
        "## The standardized weekly test (summary)\n",
        "We will analyze the same basic structure each week:\n",
        "1. **Pre-test:** quiet standing baseline  \n",
        "2. **Work:** lunge left, then lunge right (with walk + trot sets)  \n",
        "3. **Post-test:** standing recovery\n",
        "\n",
        "---\n",
        "\n",
        "## Learning goals\n",
        "By the end of this activity, you will be able to:\n",
        "- Upload and parse Duque’s HR/HRV export files.\n",
        "- Plot HR and RR time series and identify baseline / work / recovery segments.\n",
        "- Compute **RMSSD** and **SDNN** on clean segments (especially standing).\n",
        "- Estimate simple recovery metrics (e.g., recovery slope after trot sets).\n",
        "- Compare **left vs right**.\n",
        "- Flag artifacts and explain what you chose to exclude (and why).\n",
        "\n",
        "---\n",
        "\n",
        "## Data you’ll upload\n",
        "You’ll be prompted to upload (file names may vary by date/time):\n",
        "- **Initial resting HR file** (separate short recording taken at rest)\n",
        "- **Workout HR file** (same format; recorded during the weekly lunge test)\n",
        "- **ECG file** (for additional signal checks)\n",
        "- **ACC file** (for movement context)\n",
        "\n",
        "These HR files typically include: **HR**, **RR (ms)**, **timestamps (ms)**, and a **skin-contact flag**.\n",
        "  \n",
        "---\n",
        "\n",
        "## Important note\n",
        "This notebook is **educational** and supports training decisions and discussion with your team. It is **not veterinary diagnosis**.\n",
        "\n"
      ],
      "metadata": {
        "id": "hhaHEP7b5h5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0 — Setup + Upload Data\n",
        "\n",
        "Run the code cell below to:\n",
        "1. Load the Python packages we’ll use throughout the notebook.\n",
        "2. Prompt you to upload your data files.\n",
        "3. Automatically load each file into a table (DataFrame) and summarize what was uploaded.\n",
        "\n",
        "**Upload these files:**\n",
        "- **Initial resting HR file** (separate short recording taken at rest)\n",
        "- **Workout HR file** (recorded during the weekly lunge test)\n",
        "- **Workout ECG** file\n",
        "- **Workout ACC** file\n",
        "\n",
        "After upload, you’ll select which HR file is **Resting** and which is **Workout** (if more than one HR file is present).\n"
      ],
      "metadata": {
        "id": "tHSpQPLg7o-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# Step 0 — Setup + Upload (REQUIRED)\n",
        "# Resting HR is separate; ECG + ACC are workout-only.\n",
        "# =========================\n",
        "\n",
        "import sys\n",
        "import io\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Widgets in Colab\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "    from google.colab import output, files\n",
        "    output.enable_custom_widget_manager()\n",
        "except Exception as e:\n",
        "    print(\"Widget setup note:\", e)\n",
        "    print(\"If widgets don't display, we can still run everything without them.\")\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"pandas:\", pd.__version__)\n",
        "print(\"numpy:\", np.__version__)\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "\n",
        "def _read_lines_strip_nulls(file_path: str) -> list[str]:\n",
        "    \"\"\"Read a text export while stripping null bytes (common in some device exports).\"\"\"\n",
        "    data = Path(file_path).read_bytes().replace(b\"\\x00\", b\"\")\n",
        "    return data.decode(\"utf-8\", errors=\"replace\").splitlines()\n",
        "\n",
        "def _parse_collection_timestamp(header_lines: list[str]):\n",
        "    \"\"\"\n",
        "    Tries to parse: '# Collection Timestamp: 1.19.26 12.11.36'\n",
        "    Interprets it as: month.day.year hour.minute.second.\n",
        "    Returns datetime or None.\n",
        "    \"\"\"\n",
        "    for line in header_lines:\n",
        "        if \"Collection Timestamp:\" in line:\n",
        "            raw = line.split(\"Collection Timestamp:\", 1)[1].strip()\n",
        "            for fmt in (\"%m.%d.%y %H.%M.%S\", \"%m.%d.%y %I.%M.%S\"):\n",
        "                try:\n",
        "                    return datetime.strptime(raw, fmt)\n",
        "                except ValueError:\n",
        "                    pass\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def detect_file_type(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Detect file type from header.\n",
        "    \"\"\"\n",
        "    lines = _read_lines_strip_nulls(file_path)\n",
        "    header = \"\\n\".join(lines[:60]).lower()\n",
        "\n",
        "    if \"heart rate data\" in header:\n",
        "        return \"hr\"\n",
        "    if \"electrocardiogram data\" in header or \"ecg data\" in header:\n",
        "        return \"ecg\"\n",
        "    if \"accelerometer data\" in header:\n",
        "        return \"acc\"\n",
        "    return \"unknown\"\n",
        "\n",
        "def load_device_export(file_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Load an export text file into:\n",
        "      {'type': ..., 'meta': ..., 'df': DataFrame}\n",
        "    Adds:\n",
        "      - t_s: seconds since first sample (from MS)\n",
        "      - dt: absolute datetime if collection timestamp is parseable\n",
        "    \"\"\"\n",
        "    lines = _read_lines_strip_nulls(file_path)\n",
        "\n",
        "    header_lines = []\n",
        "    data_start = None\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.startswith(\"#\"):\n",
        "            header_lines.append(line)\n",
        "            continue\n",
        "        if line.strip() == \"\":\n",
        "            continue\n",
        "        data_start = i\n",
        "        break\n",
        "\n",
        "    if data_start is None:\n",
        "        raise ValueError(f\"Could not find data section in {file_path}\")\n",
        "\n",
        "    csv_text = \"\\n\".join(lines[data_start:])\n",
        "    df = pd.read_csv(io.StringIO(csv_text), skipinitialspace=True)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    meta = {\n",
        "        \"file_name\": Path(file_path).name,\n",
        "        \"collection_dt\": _parse_collection_timestamp(header_lines),\n",
        "        \"header_lines\": header_lines[:],\n",
        "    }\n",
        "\n",
        "    if \"MS\" in df.columns:\n",
        "        df[\"MS\"] = pd.to_numeric(df[\"MS\"], errors=\"coerce\")\n",
        "        df = df.dropna(subset=[\"MS\"]).reset_index(drop=True)\n",
        "        df[\"t_s\"] = (df[\"MS\"] - df[\"MS\"].iloc[0]) / 1000.0\n",
        "        if meta[\"collection_dt\"] is not None:\n",
        "            df[\"dt\"] = meta[\"collection_dt\"] + pd.to_timedelta(df[\"MS\"], unit=\"ms\")\n",
        "\n",
        "    # Coerce common numeric columns if present\n",
        "    for col in [\"HR\", \"RR\", \"SC\", \"ECG\", \"ACCX\", \"ACCY\", \"ACCZ\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    ftype = detect_file_type(file_path)\n",
        "    return {\"type\": ftype, \"meta\": meta, \"df\": df}\n",
        "\n",
        "def summarize_hr(df: pd.DataFrame) -> dict:\n",
        "    out = {\"rows\": len(df)}\n",
        "    out[\"duration_s\"] = float(df[\"t_s\"].iloc[-1]) if \"t_s\" in df.columns and len(df) else np.nan\n",
        "\n",
        "    if \"SC\" in df.columns and len(df):\n",
        "        out[\"skin_contact_%\"] = float(100 * np.nanmean(df[\"SC\"].values == 1))\n",
        "    else:\n",
        "        out[\"skin_contact_%\"] = np.nan\n",
        "\n",
        "    if \"HR\" in df.columns and len(df):\n",
        "        hr = df.loc[df.get(\"SC\", 1) == 1, \"HR\"] if \"SC\" in df.columns else df[\"HR\"]\n",
        "        hr = hr.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "        out[\"HR_mean\"] = float(hr.mean()) if len(hr) else np.nan\n",
        "        out[\"HR_min\"] = float(hr.min()) if len(hr) else np.nan\n",
        "        out[\"HR_max\"] = float(hr.max()) if len(hr) else np.nan\n",
        "    else:\n",
        "        out[\"HR_mean\"] = out[\"HR_min\"] = out[\"HR_max\"] = np.nan\n",
        "\n",
        "    return out\n",
        "\n",
        "def ms_range(df: pd.DataFrame):\n",
        "    \"\"\"Return (minMS, maxMS) if MS exists, else (nan, nan).\"\"\"\n",
        "    if df is None or \"MS\" not in df.columns or len(df) == 0:\n",
        "        return (np.nan, np.nan)\n",
        "    return (float(np.nanmin(df[\"MS\"])), float(np.nanmax(df[\"MS\"])))\n",
        "\n",
        "# ---------- Upload ----------\n",
        "\n",
        "print(\"\\nUpload REQUIRED files now:\")\n",
        "print(\"  - Resting HR file (rest-only recording)\")\n",
        "print(\"  - Workout HR file (weekly lunge test)\")\n",
        "print(\"  - ECG file (recorded during workout)\")\n",
        "print(\"  - ACC file (recorded during workout)\\n\")\n",
        "print(\"Tip: you can select all files at once in the upload dialog.\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise RuntimeError(\"No files uploaded. Re-run this cell and upload ALL required files.\")\n",
        "\n",
        "uploaded_paths = [str(Path(k).resolve()) for k in uploaded.keys()]\n",
        "print(\"\\nUploaded:\")\n",
        "for p in uploaded_paths:\n",
        "    print(\" -\", Path(p).name)\n",
        "\n",
        "# ---------- Load + Organize ----------\n",
        "\n",
        "DATA = {}\n",
        "for p in uploaded_paths:\n",
        "    try:\n",
        "        DATA[p] = load_device_export(p)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n⚠️ Could not load {Path(p).name}: {e}\")\n",
        "\n",
        "HR_FILES  = [p for p, d in DATA.items() if d[\"type\"] == \"hr\"]\n",
        "ECG_FILES = [p for p, d in DATA.items() if d[\"type\"] == \"ecg\"]\n",
        "ACC_FILES = [p for p, d in DATA.items() if d[\"type\"] == \"acc\"]\n",
        "UNK_FILES = [p for p, d in DATA.items() if d[\"type\"] == \"unknown\"]\n",
        "\n",
        "print(\"\\nDetected file types:\")\n",
        "print(\"  HR :\", [Path(p).name for p in HR_FILES])\n",
        "print(\"  ECG:\", [Path(p).name for p in ECG_FILES])\n",
        "print(\"  ACC:\", [Path(p).name for p in ACC_FILES])\n",
        "if UNK_FILES:\n",
        "    print(\"  ???:\", [Path(p).name for p in UNK_FILES])\n",
        "\n",
        "# ---------- Enforce requirements ----------\n",
        "\n",
        "errors = []\n",
        "if len(HR_FILES) < 2:\n",
        "    errors.append(f\"Need at least 2 HR files (resting + workout). Detected: {len(HR_FILES)}\")\n",
        "if len(ECG_FILES) < 1:\n",
        "    errors.append(\"Missing ECG file (workout-only).\")\n",
        "if len(ACC_FILES) < 1:\n",
        "    errors.append(\"Missing ACC file (workout-only).\")\n",
        "if len(UNK_FILES) > 0:\n",
        "    errors.append(f\"Some files were not recognized: {[Path(p).name for p in UNK_FILES]}\")\n",
        "\n",
        "if errors:\n",
        "    msg = \"\\n\".join([f\"- {e}\" for e in errors])\n",
        "    raise ValueError(\n",
        "        \"Upload/typing check failed:\\n\"\n",
        "        f\"{msg}\\n\\nFix: re-run this cell and upload the correct exports.\"\n",
        "    )\n",
        "\n",
        "# ---------- Summary table ----------\n",
        "\n",
        "summary_rows = []\n",
        "for p in HR_FILES:\n",
        "    df = DATA[p][\"df\"]\n",
        "    s = summarize_hr(df)\n",
        "    summary_rows.append({\n",
        "        \"file\": Path(p).name,\n",
        "        \"type\": \"hr\",\n",
        "        \"rows\": s[\"rows\"],\n",
        "        \"duration_s\": round(s[\"duration_s\"], 1) if np.isfinite(s[\"duration_s\"]) else np.nan,\n",
        "        \"skin_contact_%\": round(s[\"skin_contact_%\"], 1) if np.isfinite(s[\"skin_contact_%\"]) else np.nan,\n",
        "        \"HR_mean\": round(s[\"HR_mean\"], 1) if np.isfinite(s[\"HR_mean\"]) else np.nan,\n",
        "        \"HR_min\": round(s[\"HR_min\"], 1) if np.isfinite(s[\"HR_min\"]) else np.nan,\n",
        "        \"HR_max\": round(s[\"HR_max\"], 1) if np.isfinite(s[\"HR_max\"]) else np.nan,\n",
        "    })\n",
        "\n",
        "for p in ECG_FILES + ACC_FILES:\n",
        "    df = DATA[p][\"df\"]\n",
        "    dur = float(df[\"t_s\"].iloc[-1]) if \"t_s\" in df.columns and len(df) else np.nan\n",
        "    summary_rows.append({\n",
        "        \"file\": Path(p).name,\n",
        "        \"type\": DATA[p][\"type\"],\n",
        "        \"rows\": len(df),\n",
        "        \"duration_s\": round(dur, 1) if np.isfinite(dur) else np.nan,\n",
        "        \"skin_contact_%\": np.nan,\n",
        "        \"HR_mean\": np.nan,\n",
        "        \"HR_min\": np.nan,\n",
        "        \"HR_max\": np.nan,\n",
        "    })\n",
        "\n",
        "SUMMARY = pd.DataFrame(summary_rows).sort_values([\"type\", \"file\"]).reset_index(drop=True)\n",
        "display(SUMMARY)\n",
        "\n",
        "# ---------- Select Resting vs Workout HR ----------\n",
        "\n",
        "# Suggest resting = shortest HR duration, workout = longest HR duration\n",
        "durations = []\n",
        "for p in HR_FILES:\n",
        "    df = DATA[p][\"df\"]\n",
        "    dur = float(df[\"t_s\"].iloc[-1]) if \"t_s\" in df.columns and len(df) else np.inf\n",
        "    durations.append((p, dur))\n",
        "durations_sorted = sorted(durations, key=lambda x: x[1])\n",
        "suggested_rest = durations_sorted[0][0]\n",
        "suggested_workout = durations_sorted[-1][0]\n",
        "\n",
        "print(\"\\nSelect which HR file is RESTING vs WORKOUT:\")\n",
        "\n",
        "rest_dropdown = widgets.Dropdown(\n",
        "    options=[(Path(p).name, p) for p in HR_FILES],\n",
        "    value=suggested_rest,\n",
        "    description=\"Resting HR:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"60%\")\n",
        ")\n",
        "workout_dropdown = widgets.Dropdown(\n",
        "    options=[(Path(p).name, p) for p in HR_FILES],\n",
        "    value=suggested_workout,\n",
        "    description=\"Workout HR:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"60%\")\n",
        ")\n",
        "\n",
        "# ---------- Select ECG + ACC (workout-only) ----------\n",
        "\n",
        "ecg_dropdown = widgets.Dropdown(\n",
        "    options=[(Path(p).name, p) for p in ECG_FILES],\n",
        "    value=ECG_FILES[0],\n",
        "    description=\"Workout ECG:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"60%\")\n",
        ")\n",
        "acc_dropdown = widgets.Dropdown(\n",
        "    options=[(Path(p).name, p) for p in ACC_FILES],\n",
        "    value=ACC_FILES[0],\n",
        "    description=\"Workout ACC:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        "    layout=widgets.Layout(width=\"60%\")\n",
        ")\n",
        "\n",
        "warn = widgets.HTML(value=\"\")\n",
        "\n",
        "REST_HR_PATH = None\n",
        "WORKOUT_HR_PATH = None\n",
        "WORKOUT_ECG_PATH = None\n",
        "WORKOUT_ACC_PATH = None\n",
        "\n",
        "def _update_paths(change=None):\n",
        "    global REST_HR_PATH, WORKOUT_HR_PATH, WORKOUT_ECG_PATH, WORKOUT_ACC_PATH\n",
        "    REST_HR_PATH = rest_dropdown.value\n",
        "    WORKOUT_HR_PATH = workout_dropdown.value\n",
        "    WORKOUT_ECG_PATH = ecg_dropdown.value\n",
        "    WORKOUT_ACC_PATH = acc_dropdown.value\n",
        "\n",
        "    messages = []\n",
        "    if REST_HR_PATH == WORKOUT_HR_PATH:\n",
        "        messages.append(\"⚠️ Resting and Workout HR are set to the same file. Choose two different HR files.\")\n",
        "\n",
        "    # Overlap checks (very simple): MS ranges should overlap for workout HR vs ECG/ACC\n",
        "    df_work = DATA[WORKOUT_HR_PATH][\"df\"] if WORKOUT_HR_PATH else None\n",
        "    df_ecg = DATA[WORKOUT_ECG_PATH][\"df\"] if WORKOUT_ECG_PATH else None\n",
        "    df_acc = DATA[WORKOUT_ACC_PATH][\"df\"] if WORKOUT_ACC_PATH else None\n",
        "\n",
        "    w0, w1 = ms_range(df_work)\n",
        "    e0, e1 = ms_range(df_ecg)\n",
        "    a0, a1 = ms_range(df_acc)\n",
        "\n",
        "    def overlap_frac(x0, x1, y0, y1):\n",
        "        if not np.isfinite([x0, x1, y0, y1]).all():\n",
        "            return np.nan\n",
        "        inter = max(0.0, min(x1, y1) - max(x0, y0))\n",
        "        denom = max(1.0, (x1 - x0))\n",
        "        return inter / denom\n",
        "\n",
        "    oe = overlap_frac(w0, w1, e0, e1)\n",
        "    oa = overlap_frac(w0, w1, a0, a1)\n",
        "\n",
        "    if np.isfinite(oe) and oe < 0.2:\n",
        "        messages.append(\"⚠️ Workout ECG time range barely overlaps Workout HR. Double-check you selected the matching workout ECG file.\")\n",
        "    if np.isfinite(oa) and oa < 0.2:\n",
        "        messages.append(\"⚠️ Workout ACC time range barely overlaps Workout HR. Double-check you selected the matching workout ACC file.\")\n",
        "\n",
        "    if messages:\n",
        "        warn.value = \"<br>\".join([f\"<b style='color:#b71c1c'>{m}</b>\" for m in messages])\n",
        "    else:\n",
        "        warn.value = \"\"\n",
        "\n",
        "rest_dropdown.observe(_update_paths, names=\"value\")\n",
        "workout_dropdown.observe(_update_paths, names=\"value\")\n",
        "ecg_dropdown.observe(_update_paths, names=\"value\")\n",
        "acc_dropdown.observe(_update_paths, names=\"value\")\n",
        "_update_paths()\n",
        "\n",
        "display(rest_dropdown, workout_dropdown, ecg_dropdown, acc_dropdown, warn)\n",
        "\n",
        "# ---------- Convenience variables for later cells ----------\n",
        "\n",
        "df_rest = DATA[REST_HR_PATH][\"df\"].copy()\n",
        "df_work = DATA[WORKOUT_HR_PATH][\"df\"].copy()\n",
        "df_ecg  = DATA[WORKOUT_ECG_PATH][\"df\"].copy()\n",
        "df_acc  = DATA[WORKOUT_ACC_PATH][\"df\"].copy()\n",
        "\n",
        "print(\"\\nCurrent selections:\")\n",
        "print(\"  REST HR   :\", Path(REST_HR_PATH).name)\n",
        "print(\"  WORK HR   :\", Path(WORKOUT_HR_PATH).name)\n",
        "print(\"  WORK ECG  :\", Path(WORKOUT_ECG_PATH).name)\n",
        "print(\"  WORK ACC  :\", Path(WORKOUT_ACC_PATH).name)\n",
        "\n",
        "print(\"\\n✅ Setup complete.\")\n",
        "print(\"Next we’ll make a sanity plot of Resting HR/RR and Workout HR/RR + ECG + ACC to confirm everything lines up.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6dquoT8a8E0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 — Workout overview: HR and relative speed, with walk/trot/stand highlighted\n",
        "\n",
        "This figure gives a clean “big picture” view of the workout.\n",
        "\n",
        "We compute a **relative speed estimate** from the accelerometer (ACC) by:\n",
        "1. converting ACC from milli-g → m/s²,\n",
        "2. removing gravity/tilt using a rolling mean,\n",
        "3. taking the magnitude of the remaining (dynamic) acceleration,\n",
        "4. applying strong smoothing.\n",
        "\n",
        "Then we classify each moment into **stand / walk / trot** using the *speed estimate* (three-level clustering), and we **shade the background** of both plots using those labels.\n",
        "\n",
        "**Important:** the “speed” here is a *proxy* (relative intensity). It’s great for detecting structure (stand vs moving, walk-ish vs trot-ish), but it is not guaranteed to be true speed in m/s without calibration.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TMbx_6LH-W4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2 — Two-panel plot: HR + relative speed, with stand/walk/trot shading from the speed proxy\n",
        "# Requires: df_work (workout HR), df_acc (workout accelerometer), numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# -----------------------------\n",
        "# #TODO parameters (tune these)\n",
        "# -----------------------------\n",
        "GRAVITY_WIN_S   = 3.0   # seconds: stronger gravity/tilt estimate (bigger = stronger)\n",
        "SPEED_SMOOTH_S  = 2.5   # seconds: strong smoothing for speed proxy (bigger = smoother)\n",
        "PLOT_FS         = 10    # Hz: downsample for plotting + classification\n",
        "MIN_BOUT_S      = 4.0   # seconds: merge bouts shorter than this (reduces flicker)\n",
        "SPEED_SCALE     = 1.0   # unitless scaling for the speed axis\n",
        "\n",
        "# -----------------------------\n",
        "# Helper functions\n",
        "# -----------------------------\n",
        "def kmeans_1d(x, k=3, n_iter=30, seed=0):\n",
        "    \"\"\"Simple 1D k-means (no sklearn). Returns sorted centers and labels (0..k-1 mapped to sorted centers).\"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if len(x) < k:\n",
        "        raise ValueError(\"Not enough finite points for k-means.\")\n",
        "    rng = np.random.default_rng(seed)\n",
        "    # init centers: spread across quantiles\n",
        "    qs = np.linspace(0.1, 0.9, k)\n",
        "    centers = np.quantile(x, qs)\n",
        "\n",
        "    for _ in range(n_iter):\n",
        "        # assign\n",
        "        d = np.abs(x[:, None] - centers[None, :])\n",
        "        labels = np.argmin(d, axis=1)\n",
        "        new_centers = centers.copy()\n",
        "        for j in range(k):\n",
        "            pts = x[labels == j]\n",
        "            if len(pts) > 0:\n",
        "                new_centers[j] = np.mean(pts)\n",
        "        if np.allclose(new_centers, centers, rtol=1e-6, atol=1e-8):\n",
        "            centers = new_centers\n",
        "            break\n",
        "        centers = new_centers\n",
        "\n",
        "    # sort centers and remap labels accordingly\n",
        "    order = np.argsort(centers)\n",
        "    centers_sorted = centers[order]\n",
        "    return centers_sorted\n",
        "\n",
        "def classify_speed(speed, centers):\n",
        "    \"\"\"Classify speed into 3 bins using midpoints between sorted centers.\"\"\"\n",
        "    c0, c1, c2 = centers\n",
        "    b01 = 0.5*(c0 + c1)\n",
        "    b12 = 0.5*(c1 + c2)\n",
        "    # 0=stand, 1=walk, 2=trot\n",
        "    labels = np.zeros_like(speed, dtype=int)\n",
        "    labels[speed >= b01] = 1\n",
        "    labels[speed >= b12] = 2\n",
        "    return labels, (b01, b12)\n",
        "\n",
        "def segments_from_labels(t, labels):\n",
        "    \"\"\"Return list of (label, t0, t1) contiguous segments.\"\"\"\n",
        "    t = np.asarray(t, dtype=float)\n",
        "    labels = np.asarray(labels, dtype=int)\n",
        "    segs = []\n",
        "    if len(t) == 0:\n",
        "        return segs\n",
        "    start = 0\n",
        "    for i in range(1, len(labels)):\n",
        "        if labels[i] != labels[i-1]:\n",
        "            segs.append((labels[i-1], t[start], t[i-1]))\n",
        "            start = i\n",
        "    segs.append((labels[-1], t[start], t[-1]))\n",
        "    return segs\n",
        "\n",
        "def merge_short_segments(segs, min_dur_s):\n",
        "    \"\"\"Merge segments shorter than min_dur_s into a neighbor (reduces flicker).\"\"\"\n",
        "    if not segs:\n",
        "        return segs\n",
        "    segs = segs.copy()\n",
        "    i = 0\n",
        "    while i < len(segs):\n",
        "        lab, t0, t1 = segs[i]\n",
        "        dur = t1 - t0\n",
        "        if dur < min_dur_s:\n",
        "            # Prefer merging into the larger neighbor segment\n",
        "            if i > 0 and i < len(segs) - 1:\n",
        "                prev = segs[i-1]\n",
        "                nxt = segs[i+1]\n",
        "                prev_dur = prev[2] - prev[1]\n",
        "                nxt_dur = nxt[2] - nxt[1]\n",
        "                if prev_dur >= nxt_dur:\n",
        "                    # merge into prev: extend prev end, drop current\n",
        "                    segs[i-1] = (prev[0], prev[1], t1)\n",
        "                    segs.pop(i)\n",
        "                    continue\n",
        "                else:\n",
        "                    # merge into next: extend next start backward, drop current\n",
        "                    segs[i+1] = (nxt[0], t0, nxt[2])\n",
        "                    segs.pop(i)\n",
        "                    continue\n",
        "            elif i > 0:\n",
        "                prev = segs[i-1]\n",
        "                segs[i-1] = (prev[0], prev[1], t1)\n",
        "                segs.pop(i)\n",
        "                continue\n",
        "            elif i < len(segs) - 1:\n",
        "                nxt = segs[i+1]\n",
        "                segs[i+1] = (nxt[0], t0, nxt[2])\n",
        "                segs.pop(i)\n",
        "                continue\n",
        "            else:\n",
        "                # only one segment; keep it\n",
        "                i += 1\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    # If merges created adjacent same-label segments, collapse them\n",
        "    collapsed = []\n",
        "    for lab, t0, t1 in segs:\n",
        "        if collapsed and collapsed[-1][0] == lab:\n",
        "            collapsed[-1] = (lab, collapsed[-1][1], t1)\n",
        "        else:\n",
        "            collapsed.append((lab, t0, t1))\n",
        "    return collapsed\n",
        "\n",
        "def shade_segments(ax, segs, colors, alpha=0.60):\n",
        "    \"\"\"Shade ax background according to segments.\"\"\"\n",
        "    for lab, t0, t1 in segs:\n",
        "        ax.axvspan(t0, t1, color=colors[lab], alpha=alpha, linewidth=0)\n",
        "\n",
        "# -----------------------------\n",
        "# Prepare HR (remove HR==0)\n",
        "# -----------------------------\n",
        "work = df_work.copy()\n",
        "work[\"HR\"] = pd.to_numeric(work[\"HR\"], errors=\"coerce\")\n",
        "work = work[work[\"HR\"] > 0].copy()\n",
        "work = work.sort_values(\"t_s\")\n",
        "\n",
        "# -----------------------------\n",
        "# Prepare accelerometer → speed proxy\n",
        "# -----------------------------\n",
        "g = 9.80665  # m/s^2 per g\n",
        "\n",
        "acc_ms2 = (df_acc[[\"ACCX\", \"ACCY\", \"ACCZ\"]].to_numpy(dtype=float) * g) / 1000.0\n",
        "\n",
        "# Estimate ACC sampling rate\n",
        "if \"MS\" in df_acc.columns and len(df_acc) > 1:\n",
        "    dt = np.nanmedian(np.diff(df_acc[\"MS\"].to_numpy(dtype=float))) / 1000.0\n",
        "else:\n",
        "    dt = np.nanmedian(np.diff(df_acc[\"t_s\"].to_numpy(dtype=float)))\n",
        "fs = 1.0 / dt\n",
        "\n",
        "# Gravity/tilt (low-pass)\n",
        "win_g = max(1, int(round(GRAVITY_WIN_S * fs)))\n",
        "grav = (\n",
        "    pd.DataFrame(acc_ms2, columns=[\"ax\", \"ay\", \"az\"])\n",
        "    .rolling(win_g, center=True, min_periods=1)\n",
        "    .mean()\n",
        "    .to_numpy()\n",
        ")\n",
        "\n",
        "# Dynamic acceleration magnitude\n",
        "acc_dyn = acc_ms2 - grav\n",
        "acc_dyn_mag = np.linalg.norm(acc_dyn, axis=1)\n",
        "\n",
        "# Strong smoothing → speed proxy\n",
        "win_s = max(1, int(round(SPEED_SMOOTH_S * fs)))\n",
        "speed_proxy = (\n",
        "    pd.Series(acc_dyn_mag)\n",
        "    .rolling(win_s, center=True, min_periods=1)\n",
        "    .mean()\n",
        "    .to_numpy()\n",
        ")\n",
        "\n",
        "speed_est = SPEED_SCALE * speed_proxy\n",
        "\n",
        "# Downsample for plotting + classification\n",
        "step = max(1, int(round(fs / PLOT_FS)))\n",
        "t_speed = df_acc[\"t_s\"].to_numpy(dtype=float)[::step]\n",
        "speed_plot = speed_est[::step]\n",
        "\n",
        "# -----------------------------\n",
        "# Classify stand / walk / trot from speed_plot\n",
        "# -----------------------------\n",
        "centers = kmeans_1d(speed_plot[np.isfinite(speed_plot)], k=3, n_iter=50, seed=0)\n",
        "labels, (thr01, thr12) = classify_speed(speed_plot, centers)\n",
        "\n",
        "# Build segments and merge short bouts\n",
        "segs = segments_from_labels(t_speed, labels)\n",
        "segs = merge_short_segments(segs, MIN_BOUT_S)\n",
        "\n",
        "# Colors for background shading (0=stand, 1=walk, 2=trot)\n",
        "bg_colors = {\n",
        "    0: \"lightgreen\",   # stand\n",
        "    1: \"lightblue\",  # walk\n",
        "    2: \"pink\",  # trot\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Plot: 2 panels (HR, speed)\n",
        "# -----------------------------\n",
        "fig, (ax_hr, ax_spd) = plt.subplots(2, 1, sharex=True, figsize=(12, 6), constrained_layout=True)\n",
        "\n",
        "# Shade both panels\n",
        "shade_segments(ax_hr, segs, bg_colors, alpha=0.6)\n",
        "shade_segments(ax_spd, segs, bg_colors, alpha=0.6)\n",
        "\n",
        "# HR panel\n",
        "ax_hr.plot(work[\"t_s\"], work[\"HR\"], color=\"crimson\", linewidth=1.4)\n",
        "ax_hr.set_ylabel(\"HR (bpm)\")\n",
        "ax_hr.set_title(\"Workout HR and ACC-based relative speed (stand/walk/trot highlighted)\")\n",
        "\n",
        "# Speed panel (blue as requested)\n",
        "ax_spd.plot(t_speed, speed_plot, color=\"blue\", linewidth=1.4)\n",
        "ax_spd.set_ylabel(\"relative speed (proxy)\")\n",
        "ax_spd.set_xlabel(\"time (s)\")\n",
        "\n",
        "# Legend for background labels\n",
        "legend_patches = [\n",
        "    Patch(facecolor=bg_colors[0], edgecolor=\"none\", alpha=0.6, label=\"stand\"),\n",
        "    Patch(facecolor=bg_colors[1], edgecolor=\"none\", alpha=0.6, label=\"walk\"),\n",
        "    Patch(facecolor=bg_colors[2], edgecolor=\"none\", alpha=0.6, label=\"trot\"),\n",
        "]\n",
        "ax_hr.legend(handles=legend_patches, loc=\"upper right\")\n",
        "\n",
        "# Print thresholds so you can sanity-check\n",
        "print(\"Speed clustering centers:\", np.round(centers, 4))\n",
        "print(\"Thresholds (stand→walk, walk→trot):\", np.round([thr01, thr12], 4))\n",
        "print(f\"Min bout merged: {MIN_BOUT_S:.1f} s\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rngynLoy8xzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 — Identify protocol segments (Left/Right), flag missing parts, and flag *extra* added bouts\n",
        "\n",
        "Now we use the ACC-based speed proxy to break the workout into **stand / walk / trot** bouts and map those onto the intended weekly protocol:\n",
        "\n",
        "**Left direction**\n",
        "- Walk L (5 min)\n",
        "- Trot 1 L (2 min)\n",
        "- Walk 1 L (2 min)\n",
        "- Trot 2 L (2 min)\n",
        "- Walk 2 L (2 min)\n",
        "\n",
        "**Change directions**\n",
        "- Stand / pause (variable)\n",
        "\n",
        "**Right direction**\n",
        "- Walk R (5 min)\n",
        "- Trot 1 R (2 min)\n",
        "- Walk 1 R (2 min)\n",
        "- Trot 2 R (2 min)\n",
        "- Walk 2 R (2 min)\n",
        "\n",
        "**Finish**\n",
        "- Final stand (variable)\n",
        "\n",
        "Instead of forcing the workout into a fixed template, we will label the **segments that were actually detected** from the ACC-based speed estimate.\n",
        "\n",
        "For each detected segment, you will choose:\n",
        "- **Gait:** Stand / Walk / Trot  \n",
        "- **Direction:** Left / Right / Transition / Unknown  \n",
        "- **Ignore:** check this if the segment is noise, a stop to fix equipment, etc.\n",
        "\n",
        "The table shows the code’s **suggested gait** and the **duration** to help you label quickly.\n",
        "\n",
        "When you click **Save labels**, the notebook will create a clean `segments_labeled` table that we’ll use for HR/HRV analysis by segment.\n",
        "Code cell (Python)"
      ],
      "metadata": {
        "id": "LHZlthVSErpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3 — Ask user to label each detected segment (gait + direction + ignore)\n",
        "# Assumes you already ran Step 2 and have `segs` as a list of (label_id, t0, t1)\n",
        "# where label_id: 0=stand, 1=walk, 2=trot (based on speed proxy classification).\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"ipywidgets not available. Re-run Step 0 setup cell first.\") from e\n",
        "\n",
        "# ---- safety checks ----\n",
        "if \"segs\" not in globals():\n",
        "    raise ValueError(\"Missing `segs`. Re-run Step 2 (the plot with shading) first.\")\n",
        "\n",
        "label_name = {0: \"Stand\", 1: \"Walk\", 2: \"Trot\"}\n",
        "\n",
        "# Build a clean segments table\n",
        "segments_df = pd.DataFrame(\n",
        "    [{\n",
        "        \"segment_id\": i+1,\n",
        "        \"suggested_gait\": label_name.get(lab, \"Unknown\"),\n",
        "        \"t0_s\": float(t0),\n",
        "        \"t1_s\": float(t1),\n",
        "        \"dur_s\": float(t1 - t0),\n",
        "        \"dur_min\": float(t1 - t0) / 60.0,\n",
        "    } for i, (lab, t0, t1) in enumerate(segs)]\n",
        ").sort_values(\"t0_s\").reset_index(drop=True)\n",
        "\n",
        "display(segments_df)\n",
        "\n",
        "# --- UI controls for each segment ---\n",
        "GAIT_OPTIONS = [\"Stand\", \"Walk\", \"Trot\"]\n",
        "DIR_OPTIONS  = [\"Left\", \"Right\", \"Transition\", \"Unknown\"]\n",
        "\n",
        "rows = []\n",
        "widgets_rows = []\n",
        "\n",
        "for _, r in segments_df.iterrows():\n",
        "    seg_id = int(r[\"segment_id\"])\n",
        "    sug = r[\"suggested_gait\"]\n",
        "    dur_min = r[\"dur_min\"]\n",
        "    t0 = r[\"t0_s\"]\n",
        "    t1 = r[\"t1_s\"]\n",
        "\n",
        "    # default gait = suggested gait (if in options)\n",
        "    gait_default = sug if sug in GAIT_OPTIONS else \"Stand\"\n",
        "\n",
        "    ignore_chk = widgets.Checkbox(value=False, description=\"Ignore\", indent=False)\n",
        "    gait_dd = widgets.Dropdown(options=GAIT_OPTIONS, value=gait_default, description=\"Gait:\",\n",
        "                              layout=widgets.Layout(width=\"220px\"))\n",
        "    dir_dd  = widgets.Dropdown(options=DIR_OPTIONS, value=\"Unknown\", description=\"Direction:\",\n",
        "                              layout=widgets.Layout(width=\"280px\"))\n",
        "    note_tx = widgets.Text(value=\"\", description=\"Note:\", layout=widgets.Layout(width=\"420px\"))\n",
        "\n",
        "    info = widgets.HTML(\n",
        "        f\"<b>Seg {seg_id}</b> \"\n",
        "        f\"<span style='color:#555'>(suggested: {sug}, {dur_min:.2f} min, {t0:.1f}–{t1:.1f}s)</span>\"\n",
        "    )\n",
        "\n",
        "    row = widgets.HBox([info, ignore_chk, gait_dd, dir_dd, note_tx])\n",
        "    widgets_rows.append((seg_id, ignore_chk, gait_dd, dir_dd, note_tx))\n",
        "    rows.append(row)\n",
        "\n",
        "ui = widgets.VBox(rows)\n",
        "\n",
        "save_btn = widgets.Button(description=\"Save labels\", button_style=\"success\")\n",
        "out = widgets.Output()\n",
        "\n",
        "def on_save(_):\n",
        "    labeled_rows = []\n",
        "    for seg_id, ignore_chk, gait_dd, dir_dd, note_tx in widgets_rows:\n",
        "        base = segments_df.loc[segments_df[\"segment_id\"] == seg_id].iloc[0].to_dict()\n",
        "        base.update({\n",
        "            \"gait\": gait_dd.value,\n",
        "            \"direction\": dir_dd.value,\n",
        "            \"ignore\": bool(ignore_chk.value),\n",
        "            \"note\": note_tx.value.strip(),\n",
        "        })\n",
        "        labeled_rows.append(base)\n",
        "\n",
        "    global segments_labeled\n",
        "    segments_labeled = pd.DataFrame(labeled_rows).sort_values(\"t0_s\").reset_index(drop=True)\n",
        "\n",
        "    with out:\n",
        "        out.clear_output()\n",
        "        print(\"✅ Saved segments_labeled (we’ll use this in later steps).\")\n",
        "        display(segments_labeled)\n",
        "\n",
        "        # quick summaries\n",
        "        kept = segments_labeled[~segments_labeled[\"ignore\"]].copy()\n",
        "\n",
        "        if len(kept):\n",
        "            print(\"\\nSummary (time by gait):\")\n",
        "            display(kept.groupby(\"gait\", as_index=False)[\"dur_s\"].sum().assign(dur_min=lambda d: d[\"dur_s\"]/60))\n",
        "\n",
        "            print(\"\\nSummary (time by direction):\")\n",
        "            display(kept.groupby(\"direction\", as_index=False)[\"dur_s\"].sum().assign(dur_min=lambda d: d[\"dur_s\"]/60))\n",
        "\n",
        "            print(\"\\nSummary (time by direction × gait):\")\n",
        "            pivot = kept.pivot_table(index=\"direction\", columns=\"gait\", values=\"dur_s\", aggfunc=\"sum\", fill_value=0.0)\n",
        "            display((pivot/60).round(2))  # minutes\n",
        "        else:\n",
        "            print(\"\\n(Everything is marked ignore — nothing to summarize.)\")\n",
        "\n",
        "save_btn.on_click(on_save)\n",
        "\n",
        "display(ui, save_btn, out)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FD0P8NrUEfHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Recovery rate estimates: what we compute, what the tables mean, and what is “standard”\n",
        "\n",
        "### Why we estimate recovery\n",
        "Heart rate recovery after work is one of the simplest indicators we can track week-to-week. We are **not diagnosing** anything. We are trying to measure whether Duque:\n",
        "- returns toward baseline more quickly over time (fitness/training effect), or\n",
        "- shows slower recovery on a given day (fatigue, heat, stress, soreness, equipment issues, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## A. What “recovery rate” means in this notebook\n",
        "\n",
        "We estimate recovery across two transition types:\n",
        "1. **Trot → Walk**\n",
        "2. **Walk → Stand** (including the final transition into the end stand)\n",
        "\n",
        "For each transition, we look at how HR changes **immediately after the transition**, using a fixed early window (default: **60 seconds**). This gives a consistent number we can compare week-to-week.\n",
        "\n",
        "---\n",
        "\n",
        "## B. What the code actually does (step-by-step)\n",
        "\n",
        "### 1) Segment-average HR\n",
        "For every labeled segment (Walk / Trot / Stand), we compute:\n",
        "- **HR_mean** = average HR over all HR samples whose timestamps fall inside that segment.\n",
        "\n",
        "This gives a simple overview of effort level in each segment.\n",
        "\n",
        "### 2) End-of-segment HR (the “starting point” for recovery)\n",
        "For a transition (e.g., Trot → Walk), we estimate HR at the *end* of the “from” segment:\n",
        "- **HR_from_end_med** = the **median HR** over the last *W* seconds (default **20 s**) of the “from” segment.\n",
        "\n",
        "We use a **median** because it is less sensitive to spikes/dropouts.\n",
        "\n",
        "### 3) HR after the transition (the “early recovery” point)\n",
        "Inside the “to” segment (Walk or Stand), we look at the **first 60 seconds** (or shorter if the segment is shorter, but we require segments ≥ 1 minute to compute recovery).\n",
        "\n",
        "We compute:\n",
        "- **HR_to_end_med(first_60s)** = median HR over the **last W seconds** within the first 60 seconds of the “to” segment.\n",
        "\n",
        "This means we are comparing:\n",
        "- HR right before the transition (end of the “from” segment)\n",
        "vs\n",
        "- HR after one minute of recovery (inside the “to” segment)\n",
        "\n",
        "### 4) Two recovery numbers\n",
        "We report **two versions** of recovery:\n",
        "\n",
        "**(i) 60-second drop rate (primary)**\n",
        "- **ΔHR_end60_minus_fromEnd** = HR_to_end_med(first_60s) − HR_from_end_med  \n",
        "  (negative means HR dropped — recovery)\n",
        "- **rate_bpm_per_min(ΔHR/60s)** = ΔHR divided by 1 minute  \n",
        "  (so it is numerically the same as ΔHR, but explicitly “bpm/min”)\n",
        "\n",
        "**(ii) Linear slope over the first 60 seconds (secondary)**\n",
        "- **slope_bpm_per_min(fit_0-60s)** = best-fit straight-line slope of HR vs time over the first 60 seconds of the “to” segment.\n",
        "\n",
        "This can be helpful if HR is noisy, but it is still a simple model.\n",
        "\n",
        "---\n",
        "\n",
        "## C. What the recovery tables contain\n",
        "\n",
        "Each row corresponds to a single transition that met our quality rules:\n",
        "- **from_seg / to_seg**: segment IDs in your labeled table\n",
        "- **HR_from_mean / HR_to_mean**: average HR in each entire segment\n",
        "- **HR_from_end_med**: median HR in the last 20 seconds before transition\n",
        "- **HR_to_end_med(first_60s)**: median HR near the end of the first 60 seconds after transition\n",
        "- **ΔHR_end60_minus_fromEnd**: the HR change after 60 s (negative is “good recovery”)\n",
        "- **rate_bpm_per_min(ΔHR/60s)**: same change expressed as bpm/min\n",
        "- **slope_bpm_per_min(fit_0-60s)**: slope of HR during early recovery (optional)\n",
        "- **Final Walk→Stand (blips ignored)**: for the final stand, we skip tiny walk segments (settling steps) so we capture the true final recovery.\n",
        "\n",
        "---\n",
        "\n",
        "## D. Is this “standard”? Should we fit an exponential?\n",
        "\n",
        "There isn’t one single universally “standard” method across sports science and veterinary contexts, but there are common families:\n",
        "\n",
        "### Common/simple approaches (very common for field tracking)\n",
        "- **HR at 1 min post-exercise** (or 2 min, etc.)\n",
        "- **ΔHR over first 60 s** (what we do)\n",
        "- **linear slope over a short recovery window** (what we also report)\n",
        "\n",
        "These are popular because they are:\n",
        "- easy to compute,\n",
        "- robust to imperfect field data,\n",
        "- comparable across weeks.\n",
        "\n",
        "### Exponential recovery models (also common in research, but more assumptions)\n",
        "A classic model is:\n",
        "- HR(t) = HR_rest + (HR0 − HR_rest) * exp(−t/τ)\n",
        "\n",
        "Where **τ** is the recovery time constant.\n",
        "\n",
        "This can be great if:\n",
        "- you have a clean recovery segment,\n",
        "- HR is sampled reliably,\n",
        "- the recovery is approximately monotone,\n",
        "- you have a well-defined baseline (HR_rest) and start point HR0.\n",
        "\n",
        "But in real horse field data, recovery can be messy:\n",
        "- movement during “stand,”\n",
        "- environmental changes,\n",
        "- sensor contact artifacts,\n",
        "- handler interruptions,\n",
        "- non-monotone HR (spikes).\n",
        "\n",
        "So exponential fitting can be **more fragile** unless we do careful filtering and quality control.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vo6aPm_9Hni-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4 — Recovery estimates + include final Walk→Stand (ignore brief walk blips during final stand)\n",
        "# Requires: segments_labeled (from Step 3) and df_work (workout HR)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# -----------------------------\n",
        "# #TODO parameters\n",
        "# -----------------------------\n",
        "MIN_SEG_S = 60.0          # segments must be at least 1 minute long to be used for recovery estimates\n",
        "R_SEC     = 60.0          # recovery window: first 60 seconds of the \"to\" segment\n",
        "W_SEC     = 20.0          # end-of-segment HR median window (seconds)\n",
        "HR_TIME_OFFSET_S = 0.0    # shift segment times to line up with HR time axis if needed\n",
        "\n",
        "WALK_BLIP_S = 30.0        # treat walk segments shorter than this as \"blips\" inside the final stand\n",
        "\n",
        "# -----------------------------\n",
        "# Prep HR signal (remove zeros, sort)\n",
        "# -----------------------------\n",
        "work = df_work.copy()\n",
        "work[\"HR\"] = pd.to_numeric(work[\"HR\"], errors=\"coerce\")\n",
        "work = work[(work[\"HR\"] > 0) & np.isfinite(work[\"t_s\"])].dropna(subset=[\"HR\", \"t_s\"])\n",
        "work = work.sort_values(\"t_s\").reset_index(drop=True)\n",
        "\n",
        "t_hr = work[\"t_s\"].to_numpy(dtype=float)\n",
        "hr   = work[\"HR\"].to_numpy(dtype=float)\n",
        "\n",
        "def hr_stats(t0, t1):\n",
        "    if t1 <= t0:\n",
        "        return (np.nan, 0)\n",
        "    m = (t_hr >= t0) & (t_hr <= t1)\n",
        "    if not np.any(m):\n",
        "        return (np.nan, 0)\n",
        "    return (float(np.nanmean(hr[m])), int(np.sum(m)))\n",
        "\n",
        "def hr_median_last(t0, t1, w=W_SEC):\n",
        "    a = max(t0, t1 - w)\n",
        "    m = (t_hr >= a) & (t_hr <= t1)\n",
        "    if np.sum(m) < 3:\n",
        "        return (np.nan, int(np.sum(m)))\n",
        "    return (float(np.nanmedian(hr[m])), int(np.sum(m)))\n",
        "\n",
        "def hr_slope_bpm_per_min(t0, t1):\n",
        "    if t1 <= t0:\n",
        "        return (np.nan, 0)\n",
        "    m = (t_hr >= t0) & (t_hr <= t1)\n",
        "    if np.sum(m) < 6:\n",
        "        return (np.nan, int(np.sum(m)))\n",
        "    x = t_hr[m]\n",
        "    y = hr[m]\n",
        "    slope_bpm_per_s = np.polyfit(x, y, 1)[0]\n",
        "    return (float(slope_bpm_per_s * 60.0), int(np.sum(m)))\n",
        "\n",
        "# -----------------------------\n",
        "# Build segment table with average HR\n",
        "# -----------------------------\n",
        "if \"segments_labeled\" not in globals():\n",
        "    raise ValueError(\"segments_labeled not found. Run Step 3 labeling and click Save labels first.\")\n",
        "\n",
        "seg = segments_labeled.copy().sort_values(\"t0_s\").reset_index(drop=True)\n",
        "seg[\"dur_s\"] = seg[\"t1_s\"] - seg[\"t0_s\"]\n",
        "\n",
        "seg[\"t0_hr\"] = seg[\"t0_s\"] + HR_TIME_OFFSET_S\n",
        "seg[\"t1_hr\"] = seg[\"t1_s\"] + HR_TIME_OFFSET_S\n",
        "\n",
        "seg_hr_rows = []\n",
        "for _, r in seg.iterrows():\n",
        "    t0, t1 = float(r[\"t0_hr\"]), float(r[\"t1_hr\"])\n",
        "    mean_hr, n = hr_stats(t0, t1)\n",
        "    seg_hr_rows.append({\n",
        "        \"segment_id\": int(r[\"segment_id\"]),\n",
        "        \"gait\": r[\"gait\"],\n",
        "        \"direction\": r[\"direction\"],\n",
        "        \"ignore\": bool(r[\"ignore\"]),\n",
        "        \"t0_s\": float(r[\"t0_s\"]),\n",
        "        \"t1_s\": float(r[\"t1_s\"]),\n",
        "        \"dur_s\": float(r[\"dur_s\"]),\n",
        "        \"dur_min\": float(r[\"dur_s\"]/60.0),\n",
        "        \"HR_mean\": mean_hr,\n",
        "        \"HR_samples\": n,\n",
        "        \"note\": r.get(\"note\",\"\"),\n",
        "    })\n",
        "\n",
        "segments_hr = pd.DataFrame(seg_hr_rows)\n",
        "print(\"Segment averages (all segments):\")\n",
        "display(segments_hr)\n",
        "\n",
        "# Keep only non-ignored segments\n",
        "kept = segments_hr[~segments_hr[\"ignore\"]].copy().reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Generic transition finder (adjacent segments only)\n",
        "# -----------------------------\n",
        "def transition_table_adjacent(from_gait, to_gait, label):\n",
        "    rows = []\n",
        "    for i in range(len(kept) - 1):\n",
        "        a = kept.iloc[i]\n",
        "        b = kept.iloc[i+1]\n",
        "\n",
        "        if a[\"gait\"] != from_gait or b[\"gait\"] != to_gait:\n",
        "            continue\n",
        "\n",
        "        if (a[\"dur_s\"] < MIN_SEG_S) or (b[\"dur_s\"] < MIN_SEG_S):\n",
        "            continue\n",
        "\n",
        "        # HR end of from-segment (median over last W_SEC)\n",
        "        a_t0 = float(a[\"t0_s\"] + HR_TIME_OFFSET_S)\n",
        "        a_t1 = float(a[\"t1_s\"] + HR_TIME_OFFSET_S)\n",
        "        hr_from_end, _ = hr_median_last(a_t0, a_t1, W_SEC)\n",
        "\n",
        "        # HR over first R_SEC of to-segment (use median at end of that window)\n",
        "        b_t0 = float(b[\"t0_s\"] + HR_TIME_OFFSET_S)\n",
        "        b_t1 = float(b[\"t1_s\"] + HR_TIME_OFFSET_S)\n",
        "        b_early_end = min(b_t1, b_t0 + R_SEC)\n",
        "        hr_to_early_end, _ = hr_median_last(b_t0, b_early_end, W_SEC)\n",
        "\n",
        "        slope_bpm_min, n_fit = hr_slope_bpm_per_min(b_t0, b_early_end)\n",
        "\n",
        "        if np.isfinite(hr_from_end) and np.isfinite(hr_to_early_end):\n",
        "            delta_hr = hr_to_early_end - hr_from_end\n",
        "            rate_bpm_min = delta_hr / (R_SEC/60.0)\n",
        "        else:\n",
        "            delta_hr = np.nan\n",
        "            rate_bpm_min = np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"transition\": label,\n",
        "            \"from_seg\": int(a[\"segment_id\"]),\n",
        "            \"to_seg\": int(b[\"segment_id\"]),\n",
        "            \"t_to_start_s\": float(b[\"t0_s\"]),\n",
        "            \"HR_from_mean\": float(a[\"HR_mean\"]),\n",
        "            \"HR_to_mean\": float(b[\"HR_mean\"]),\n",
        "            \"HR_from_end_med\": hr_from_end,\n",
        "            \"HR_to_end_med(first_60s)\": hr_to_early_end,\n",
        "            \"ΔHR_end60_minus_fromEnd\": delta_hr,\n",
        "            \"rate_bpm_per_min(ΔHR/60s)\": rate_bpm_min,\n",
        "            \"slope_bpm_per_min(fit_0-60s)\": slope_bpm_min,\n",
        "            \"HR_samples_fit\": n_fit,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -----------------------------\n",
        "# Special: final Walk → Stand (skip brief walk blips)\n",
        "# -----------------------------\n",
        "def final_walk_to_stand_transition():\n",
        "    \"\"\"\n",
        "    Find the last meaningful Walk→Stand transition.\n",
        "    Starting near the end:\n",
        "      - find the final Stand segment that is >= MIN_SEG_S\n",
        "      - walk backward to find the most recent Walk segment >= MIN_SEG_S\n",
        "      - ignore any intervening Walk segments shorter than WALK_BLIP_S (walk blips)\n",
        "    Returns a dict with indices into `kept` or None.\n",
        "    \"\"\"\n",
        "    if len(kept) < 2:\n",
        "        return None\n",
        "\n",
        "    # candidate stand segments at end\n",
        "    stand_idxs = [i for i in range(len(kept)) if kept.loc[i, \"gait\"] == \"Stand\" and kept.loc[i, \"dur_s\"] >= MIN_SEG_S]\n",
        "    if not stand_idxs:\n",
        "        return None\n",
        "\n",
        "    stand_i = stand_idxs[-1]  # last stand\n",
        "    # now search backward for last good walk before this stand, skipping short walk blips\n",
        "    j = stand_i - 1\n",
        "    while j >= 0:\n",
        "        if kept.loc[j, \"gait\"] == \"Walk\":\n",
        "            if kept.loc[j, \"dur_s\"] >= MIN_SEG_S:\n",
        "                return {\"walk_index\": j, \"stand_index\": stand_i}\n",
        "            elif kept.loc[j, \"dur_s\"] < WALK_BLIP_S:\n",
        "                # ignore blip and keep moving left\n",
        "                j -= 1\n",
        "                continue\n",
        "            else:\n",
        "                # walk segment is between blip and \"good\"—still not long enough for estimate\n",
        "                j -= 1\n",
        "                continue\n",
        "        else:\n",
        "            j -= 1\n",
        "    return None\n",
        "\n",
        "def compute_transition_row(a, b, label):\n",
        "    \"\"\"Compute recovery metrics from segment a (from) to segment b (to).\"\"\"\n",
        "    a_t0 = float(a[\"t0_s\"] + HR_TIME_OFFSET_S)\n",
        "    a_t1 = float(a[\"t1_s\"] + HR_TIME_OFFSET_S)\n",
        "    b_t0 = float(b[\"t0_s\"] + HR_TIME_OFFSET_S)\n",
        "    b_t1 = float(b[\"t1_s\"] + HR_TIME_OFFSET_S)\n",
        "\n",
        "    hr_from_end, _ = hr_median_last(a_t0, a_t1, W_SEC)\n",
        "    b_early_end = min(b_t1, b_t0 + R_SEC)\n",
        "    hr_to_early_end, _ = hr_median_last(b_t0, b_early_end, W_SEC)\n",
        "\n",
        "    slope_bpm_min, n_fit = hr_slope_bpm_per_min(b_t0, b_early_end)\n",
        "\n",
        "    if np.isfinite(hr_from_end) and np.isfinite(hr_to_early_end):\n",
        "        delta_hr = hr_to_early_end - hr_from_end\n",
        "        rate_bpm_min = delta_hr / (R_SEC/60.0)\n",
        "    else:\n",
        "        delta_hr = np.nan\n",
        "        rate_bpm_min = np.nan\n",
        "\n",
        "    return {\n",
        "        \"transition\": label,\n",
        "        \"from_seg\": int(a[\"segment_id\"]),\n",
        "        \"to_seg\": int(b[\"segment_id\"]),\n",
        "        \"t_to_start_s\": float(b[\"t0_s\"]),\n",
        "        \"HR_from_mean\": float(a[\"HR_mean\"]),\n",
        "        \"HR_to_mean\": float(b[\"HR_mean\"]),\n",
        "        \"HR_from_end_med\": hr_from_end,\n",
        "        \"HR_to_end_med(first_60s)\": hr_to_early_end,\n",
        "        \"ΔHR_end60_minus_fromEnd\": delta_hr,\n",
        "        \"rate_bpm_per_min(ΔHR/60s)\": rate_bpm_min,\n",
        "        \"slope_bpm_per_min(fit_0-60s)\": slope_bpm_min,\n",
        "        \"HR_samples_fit\": n_fit,\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Compute tables\n",
        "# -----------------------------\n",
        "tw = transition_table_adjacent(\"Trot\", \"Walk\", \"Trot→Walk\")\n",
        "ws_adj = transition_table_adjacent(\"Walk\", \"Stand\", \"Walk→Stand\")\n",
        "\n",
        "# Final walk→stand (robust)\n",
        "final_pair = final_walk_to_stand_transition()\n",
        "if final_pair is None:\n",
        "    ws_final = pd.DataFrame([])\n",
        "else:\n",
        "    a = kept.iloc[final_pair[\"walk_index\"]]\n",
        "    b = kept.iloc[final_pair[\"stand_index\"]]\n",
        "    ws_final = pd.DataFrame([compute_transition_row(a, b, \"Walk→Stand (final)\")])\n",
        "\n",
        "# -----------------------------\n",
        "# Combined recovery report (major results only)\n",
        "# -----------------------------\n",
        "def _pick_cols(df, cols):\n",
        "    if df is None or len(df) == 0:\n",
        "        return pd.DataFrame(columns=cols)\n",
        "    keep = [c for c in cols if c in df.columns]\n",
        "    out = df[keep].copy()\n",
        "    # ensure all columns exist for concat\n",
        "    for c in cols:\n",
        "        if c not in out.columns:\n",
        "            out[c] = np.nan\n",
        "    return out[cols]\n",
        "\n",
        "MAJOR_COLS = [\n",
        "    \"transition\",\n",
        "    \"from_seg\", \"to_seg\",\n",
        "    \"t_to_start_s\",\n",
        "    \"HR_from_mean\", \"HR_to_mean\",\n",
        "    \"rate_bpm_per_min(ΔHR/60s)\",\n",
        "    \"slope_bpm_per_min(fit_0-60s)\",\n",
        "]\n",
        "\n",
        "tw_s  = _pick_cols(tw, MAJOR_COLS)\n",
        "ws_s  = _pick_cols(ws_adj, MAJOR_COLS)\n",
        "wf_s  = _pick_cols(ws_final, MAJOR_COLS)\n",
        "\n",
        "combined_recovery = pd.concat([tw_s, ws_s, wf_s], ignore_index=True)\n",
        "\n",
        "# Add a simple interpretation column\n",
        "rate_col = \"rate_bpm_per_min(ΔHR/60s)\"\n",
        "rate_num = pd.to_numeric(combined_recovery[rate_col], errors=\"coerce\")\n",
        "combined_recovery[\"interpretation\"] = np.where(\n",
        "    rate_num.isna(),\n",
        "    \"insufficient data\",\n",
        "    np.where(rate_num < 0, \"HR dropping (recovery)\", \"HR not dropping / unclear\")\n",
        ")\n",
        "\n",
        "# Add segment duration context (minutes), if segments_hr exists\n",
        "if \"segments_hr\" in globals() and len(segments_hr):\n",
        "    dur_from = segments_hr[[\"segment_id\", \"dur_min\"]].rename(columns={\"segment_id\": \"from_seg\", \"dur_min\": \"from_dur_min\"})\n",
        "    dur_to   = segments_hr[[\"segment_id\", \"dur_min\"]].rename(columns={\"segment_id\": \"to_seg\",   \"dur_min\": \"to_dur_min\"})\n",
        "    combined_recovery = combined_recovery.merge(dur_from, on=\"from_seg\", how=\"left\")\n",
        "    combined_recovery = combined_recovery.merge(dur_to,   on=\"to_seg\",   how=\"left\")\n",
        "\n",
        "# Round for readability\n",
        "for c in [\"t_to_start_s\", \"HR_from_mean\", \"HR_to_mean\", rate_col, \"slope_bpm_per_min(fit_0-60s)\", \"from_dur_min\", \"to_dur_min\"]:\n",
        "    if c in combined_recovery.columns:\n",
        "        combined_recovery[c] = pd.to_numeric(combined_recovery[c], errors=\"coerce\").round(4)\n",
        "\n",
        "# Clean report view\n",
        "REPORT_COLS = [\n",
        "    \"transition\",\n",
        "    \"from_seg\", \"to_seg\",\n",
        "    \"from_dur_min\", \"to_dur_min\",\n",
        "    \"HR_from_mean\", \"HR_to_mean\",\n",
        "    rate_col,\n",
        "    \"slope_bpm_per_min(fit_0-60s)\",\n",
        "    \"t_to_start_s\",\n",
        "]\n",
        "REPORT_COLS = [c for c in REPORT_COLS if c in combined_recovery.columns]\n",
        "\n",
        "combined_recovery_report = (\n",
        "    combined_recovery[REPORT_COLS]\n",
        "    .sort_values([\"transition\", \"t_to_start_s\"], na_position=\"last\")\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"\\nCombined recovery report (major results only):\")\n",
        "display(combined_recovery_report)\n"
      ],
      "metadata": {
        "id": "D-OEDn4XFI3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 — HRV metrics for (1) the initial resting stand and (2) the final stand\n",
        "\n",
        "In this step we compute **time-domain HRV metrics** from **NN (RR) intervals** for two standing periods:\n",
        "\n",
        "1. **Initial resting stand** (from the separate “resting HR” file)\n",
        "2. **Final stand** (from the workout recording, using your labeled segments)\n",
        "\n",
        "### What data we use\n",
        "- HRV is computed from **NN / RR intervals** (in milliseconds).  \n",
        "- We **clean** the NN series to remove obvious artifacts (dropouts, spikes, bad-contact segments if flagged).\n",
        "\n",
        "### Metrics reported (time-domain)\n",
        "Let NN be the sequence of cleaned NN intervals (ms):\n",
        "- **Mean NN (ms)**: average interval length  \n",
        "- **Mean HR (bpm)**: average heart rate (from HR column if available, otherwise 60000 / Mean NN)\n",
        "- **SDNN (ms)**: standard deviation of NN intervals  \n",
        "- **RMSSD (ms)**: sqrt(mean( (diff(NN))² ))  \n",
        "- **pNN50 (%)**: percent of successive NN differences > 50 ms  \n",
        "- **N_intervals**: how many NN intervals were used after cleaning (data quality indicator)\n",
        "\n",
        "### Notes\n",
        "- HRV is most meaningful during **quiet standing**. If the horse is shifting, stepping, or contact is poor, HRV will be less reliable.\n",
        "- We trim a small amount off the start/end of each stand by default to avoid transition effects.\n"
      ],
      "metadata": {
        "id": "Rk3fAkwwNuEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5 — HRV metrics for initial resting stand (separate file) + final stand (from labeled workout segments)\n",
        "# Requires: segments_labeled (from Step 3) and df_work (workout HR dataframe already loaded earlier).\n",
        "# This code will prompt for the resting HR file if df_rest is not already defined.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# -----------------------------\n",
        "# #TODO parameters\n",
        "# -----------------------------\n",
        "# Basic NN artifact filters (tune if needed)\n",
        "NN_MIN_MS = 300      # too short to be real (artifact)\n",
        "NN_MAX_MS = 2000     # too long to be real (artifact)\n",
        "MAX_DIFF_MS = 250    # drop successive NN jumps bigger than this (artifact)\n",
        "\n",
        "# Trims (seconds) to avoid transition edges\n",
        "REST_TRIM_START_S = 20\n",
        "REST_TRIM_END_S   = 10\n",
        "\n",
        "FINAL_TRIM_START_S = 10\n",
        "FINAL_TRIM_END_S   = 10\n",
        "\n",
        "# If HR and segment times are slightly misaligned, reuse the same offset you used earlier\n",
        "HR_TIME_OFFSET_S = 0.0\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers: load + standardize a Polar-style txt export\n",
        "# -----------------------------\n",
        "def load_hr_txt_to_df(path):\n",
        "    \"\"\"Robust-ish loader for tab/space/comma-separated Polar exports.\"\"\"\n",
        "    df = pd.read_csv(path, sep=None, engine=\"python\")\n",
        "    # normalize column names\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def ensure_time_seconds(df):\n",
        "    \"\"\"Ensure a 't_s' column exists.\"\"\"\n",
        "    if \"t_s\" in df.columns:\n",
        "        df[\"t_s\"] = pd.to_numeric(df[\"t_s\"], errors=\"coerce\")\n",
        "        return df\n",
        "\n",
        "    # Common alternatives\n",
        "    col_lower = {c.lower(): c for c in df.columns}\n",
        "\n",
        "    if \"ms\" in col_lower:\n",
        "        c = col_lower[\"ms\"]\n",
        "        ms = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        df[\"t_s\"] = (ms - ms.iloc[0]) / 1000.0\n",
        "        return df\n",
        "\n",
        "    # try timestamp-like columns\n",
        "    for key in [\"timestamp\", \"time\", \"datetime\", \"date_time\", \"date time\"]:\n",
        "        if key in col_lower:\n",
        "            c = col_lower[key]\n",
        "            ts = pd.to_datetime(df[c], errors=\"coerce\", utc=False)\n",
        "            df[\"t_s\"] = (ts - ts.iloc[0]).dt.total_seconds()\n",
        "            return df\n",
        "\n",
        "    raise ValueError(\"Could not infer time column. Expected 't_s' or 'MS' or a timestamp column.\")\n",
        "\n",
        "def find_rr_column(df):\n",
        "    \"\"\"Find a likely NN/RR column and return its name.\"\"\"\n",
        "    candidates = [\n",
        "        \"rr\", \"rr_ms\", \"rr(ms)\", \"rr (ms)\", \"rrinterval\", \"rr_interval\",\n",
        "        \"nn\", \"nn_ms\", \"ibi\", \"ibi_ms\", \"rri\", \"rri_ms\"\n",
        "    ]\n",
        "    col_lower = {c.lower().replace(\" \", \"\"): c for c in df.columns}\n",
        "    for k in candidates:\n",
        "        kk = k.lower().replace(\" \", \"\")\n",
        "        if kk in col_lower:\n",
        "            return col_lower[kk]\n",
        "    # fallback: look for any column containing 'rr' and 'ms'\n",
        "    for c in df.columns:\n",
        "        cl = c.lower()\n",
        "        if (\"rr\" in cl or \"nn\" in cl or \"ibi\" in cl) and (\"ms\" in cl or \"interval\" in cl):\n",
        "            return c\n",
        "    raise ValueError(\"Could not find an RR/NN interval column in this file.\")\n",
        "\n",
        "def find_hr_column(df):\n",
        "    \"\"\"Find a likely HR column; return name or None.\"\"\"\n",
        "    col_lower = {c.lower().replace(\" \", \"\"): c for c in df.columns}\n",
        "    for k in [\"hr\", \"heart_rate\", \"heartrate\", \"hr(bpm)\", \"hr(bpm)\"]:\n",
        "        kk = k.lower().replace(\" \", \"\")\n",
        "        if kk in col_lower:\n",
        "            return col_lower[kk]\n",
        "    # fallback: any column that looks like HR\n",
        "    for c in df.columns:\n",
        "        cl = c.lower()\n",
        "        if cl in [\"hr\", \"heart rate\", \"heartrate\"] or (\"hr\" in cl and \"bpm\" in cl):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def filter_nn(nn_ms):\n",
        "    \"\"\"Simple NN cleaning: range + big-jump removal.\"\"\"\n",
        "    nn = pd.to_numeric(pd.Series(nn_ms), errors=\"coerce\").dropna().to_numpy(dtype=float)\n",
        "    if len(nn) == 0:\n",
        "        return nn, 0\n",
        "\n",
        "    mask = (nn >= NN_MIN_MS) & (nn <= NN_MAX_MS)\n",
        "    nn = nn[mask]\n",
        "\n",
        "    if len(nn) < 3:\n",
        "        return nn, 0\n",
        "\n",
        "    # remove points that create huge successive jumps\n",
        "    d = np.abs(np.diff(nn))\n",
        "    keep = np.ones(len(nn), dtype=bool)\n",
        "    keep[1:] &= (d <= MAX_DIFF_MS)\n",
        "    nn2 = nn[keep]\n",
        "    removed = int(len(nn) - len(nn2))\n",
        "    return nn2, removed\n",
        "\n",
        "def hrv_metrics_from_nn(nn_ms):\n",
        "    \"\"\"Compute time-domain HRV metrics from NN (ms).\"\"\"\n",
        "    nn = np.asarray(nn_ms, dtype=float)\n",
        "    nn = nn[np.isfinite(nn)]\n",
        "    out = {}\n",
        "    out[\"N_intervals\"] = int(len(nn))\n",
        "\n",
        "    if len(nn) < 10:\n",
        "        # Too few points for stable HRV\n",
        "        out.update({\n",
        "            \"Mean_NN_ms\": np.nan,\n",
        "            \"Mean_HR_bpm_fromNN\": np.nan,\n",
        "            \"SDNN_ms\": np.nan,\n",
        "            \"RMSSD_ms\": np.nan,\n",
        "            \"pNN50_pct\": np.nan\n",
        "        })\n",
        "        return out\n",
        "\n",
        "    out[\"Mean_NN_ms\"] = float(np.mean(nn))\n",
        "    out[\"Mean_HR_bpm_fromNN\"] = float(60000.0 / out[\"Mean_NN_ms\"])\n",
        "    out[\"SDNN_ms\"] = float(np.std(nn, ddof=1))\n",
        "\n",
        "    diff_nn = np.diff(nn)\n",
        "    out[\"RMSSD_ms\"] = float(np.sqrt(np.mean(diff_nn**2)))\n",
        "    out[\"pNN50_pct\"] = float(100.0 * np.mean(np.abs(diff_nn) > 50.0))\n",
        "    return out\n",
        "\n",
        "def compute_hrv_for_window(df, t0, t1, label, trim_start=0, trim_end=0):\n",
        "    \"\"\"Compute HRV on [t0+trim_start, t1-trim_end] using RR/NN column.\"\"\"\n",
        "    t0u = float(t0 + trim_start)\n",
        "    t1u = float(t1 - trim_end)\n",
        "    if t1u <= t0u:\n",
        "        return {\"segment\": label, \"t0_s\": t0, \"t1_s\": t1, \"used_start_s\": t0u, \"used_end_s\": t1u, \"note\": \"window too short after trimming\"}\n",
        "\n",
        "    rr_col = find_rr_column(df)\n",
        "    hr_col = find_hr_column(df)\n",
        "\n",
        "    sub = df[(df[\"t_s\"] >= t0u) & (df[\"t_s\"] <= t1u)].copy()\n",
        "    rr = pd.to_numeric(sub[rr_col], errors=\"coerce\").to_numpy(dtype=float)\n",
        "\n",
        "    nn_clean, removed_jumps = filter_nn(rr)\n",
        "\n",
        "    metrics = hrv_metrics_from_nn(nn_clean)\n",
        "    metrics[\"segment\"] = label\n",
        "    metrics[\"t0_s\"] = float(t0)\n",
        "    metrics[\"t1_s\"] = float(t1)\n",
        "    metrics[\"used_start_s\"] = float(t0u)\n",
        "    metrics[\"used_end_s\"] = float(t1u)\n",
        "    metrics[\"RR_col\"] = rr_col\n",
        "    metrics[\"removed_after_range/jump_filter\"] = int(removed_jumps)\n",
        "\n",
        "    # Mean HR from HR column if available\n",
        "    if hr_col is not None and hr_col in sub.columns:\n",
        "        hr_vals = pd.to_numeric(sub[hr_col], errors=\"coerce\")\n",
        "        hr_vals = hr_vals[(hr_vals > 0) & np.isfinite(hr_vals)]\n",
        "        metrics[\"Mean_HR_bpm_fromHR\"] = float(hr_vals.mean()) if len(hr_vals) else np.nan\n",
        "    else:\n",
        "        metrics[\"Mean_HR_bpm_fromHR\"] = np.nan\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# -----------------------------\n",
        "# Load resting file if needed\n",
        "# -----------------------------\n",
        "if \"df_rest\" not in globals():\n",
        "    # Colab upload prompt (works in Colab; safe to ignore elsewhere)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Upload the *resting* HR file (initial stand):\")\n",
        "        up = files.upload()\n",
        "        rest_path = next(iter(up.keys()))\n",
        "        df_rest = load_hr_txt_to_df(rest_path)\n",
        "    except Exception:\n",
        "        raise ValueError(\"df_rest not found. Please upload the resting HR file and assign it to df_rest.\")\n",
        "\n",
        "# Standardize time columns for rest + workout\n",
        "df_rest = ensure_time_seconds(df_rest)\n",
        "\n",
        "# We assume df_work already exists from your earlier steps\n",
        "if \"df_work\" not in globals():\n",
        "    raise ValueError(\"df_work not found. Please load the workout HR file into df_work first.\")\n",
        "\n",
        "df_work = ensure_time_seconds(df_work)\n",
        "\n",
        "# Apply the same \"HR > 0\" cleaning for convenience (HRV uses RR, but this helps if HR column is used)\n",
        "if \"HR\" in df_work.columns:\n",
        "    df_work[\"HR\"] = pd.to_numeric(df_work[\"HR\"], errors=\"coerce\")\n",
        "if \"HR\" in df_rest.columns:\n",
        "    df_rest[\"HR\"] = pd.to_numeric(df_rest[\"HR\"], errors=\"coerce\")\n",
        "\n",
        "# -----------------------------\n",
        "# Define the two windows:\n",
        "#  (1) Initial resting stand: use the full rest file time range\n",
        "#  (2) Final stand: last labeled Stand segment (not ignored)\n",
        "# -----------------------------\n",
        "# Initial rest window\n",
        "rest_t0 = float(np.nanmin(df_rest[\"t_s\"]))\n",
        "rest_t1 = float(np.nanmax(df_rest[\"t_s\"]))\n",
        "\n",
        "# Final stand from labeled segments\n",
        "if \"segments_labeled\" not in globals():\n",
        "    raise ValueError(\"segments_labeled not found. Run Step 3 labeling and click Save labels first.\")\n",
        "\n",
        "seg_keep = segments_labeled[(~segments_labeled[\"ignore\"]) & (segments_labeled[\"gait\"] == \"Stand\")].copy()\n",
        "if len(seg_keep) == 0:\n",
        "    raise ValueError(\"No final stand segment found in segments_labeled (gait == 'Stand' and not ignored).\")\n",
        "\n",
        "final_stand = seg_keep.sort_values(\"t0_s\").iloc[-1]\n",
        "final_t0 = float(final_stand[\"t0_s\"] + HR_TIME_OFFSET_S)\n",
        "final_t1 = float(final_stand[\"t1_s\"] + HR_TIME_OFFSET_S)\n",
        "\n",
        "# -----------------------------\n",
        "# Compute HRV metrics\n",
        "# -----------------------------\n",
        "rest_metrics = compute_hrv_for_window(\n",
        "    df_rest, rest_t0, rest_t1,\n",
        "    label=\"Initial stand (rest file)\",\n",
        "    trim_start=REST_TRIM_START_S,\n",
        "    trim_end=REST_TRIM_END_S\n",
        ")\n",
        "\n",
        "final_metrics = compute_hrv_for_window(\n",
        "    df_work, final_t0, final_t1,\n",
        "    label=\"Final stand (workout file)\",\n",
        "    trim_start=FINAL_TRIM_START_S,\n",
        "    trim_end=FINAL_TRIM_END_S\n",
        ")\n",
        "\n",
        "hrv_table = pd.DataFrame([rest_metrics, final_metrics])\n",
        "\n",
        "# Keep the “headline” columns first\n",
        "headline_cols = [\n",
        "    \"segment\",\n",
        "    \"N_intervals\",\n",
        "    \"Mean_HR_bpm_fromHR\",\n",
        "    \"Mean_HR_bpm_fromNN\",\n",
        "    \"Mean_NN_ms\",\n",
        "    \"SDNN_ms\",\n",
        "    \"RMSSD_ms\",\n",
        "    \"pNN50_pct\",\n",
        "    \"removed_after_range/jump_filter\",\n",
        "    \"used_start_s\",\n",
        "    \"used_end_s\",\n",
        "    \"RR_col\",\n",
        "]\n",
        "headline_cols = [c for c in headline_cols if c in hrv_table.columns]\n",
        "hrv_table = hrv_table[headline_cols]\n",
        "\n",
        "# Light rounding for display only (does not affect underlying calculations)\n",
        "for c in [\"Mean_HR_bpm_fromHR\", \"Mean_HR_bpm_fromNN\", \"Mean_NN_ms\", \"SDNN_ms\", \"RMSSD_ms\", \"pNN50_pct\"]:\n",
        "    if c in hrv_table.columns:\n",
        "        hrv_table[c] = pd.to_numeric(hrv_table[c], errors=\"coerce\").round(2)\n",
        "\n",
        "print(\"HRV metrics (initial rest stand vs final stand):\")\n",
        "display(hrv_table)\n",
        "\n"
      ],
      "metadata": {
        "id": "2s4OmurUHoXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 — HRV over time with gait shading (two plots: normalized and raw)\n",
        "\n",
        "In this section we visualize how HRV changes throughout the workout using a **moving-window** estimate computed from the **NN (RR) intervals**.\n",
        "\n",
        "We make **two plots** (same gait shading on both):\n",
        "\n",
        "### Plot A: Normalized HRV (local normalization)\n",
        "We compute RMSSD in a moving window, then normalize it by the **local mean NN** computed over a (possibly longer) moving window:\n",
        "\n",
        "- **RMSSD(t)**: computed over the last `WINDOW_S` seconds  \n",
        "- **meanNN_local(t)**: mean NN over the last `NORM_WINDOW_S` seconds  \n",
        "- **Normalized HRV**:  \n",
        "  **CVRMSSD_local(t) = 100 × RMSSD(t) / meanNN_local(t)**\n",
        "\n",
        "This gives a dimensionless percent-like measure that is less sensitive to overall heart rate level and is easier to compare across time and across weeks.\n",
        "\n",
        "### Plot B: Raw HRV (no normalization)\n",
        "We plot **RMSSD(t)** in milliseconds. This is the same moving-window HRV estimate, but without dividing by mean NN.\n",
        "\n",
        "### Notes\n",
        "- HRV is most interpretable during **quiet standing**; movement and contact artifacts can distort it.\n",
        "- We apply several NN cleaning steps (range filter, jump filter, and robust outlier removal) to reduce artifacts.\n",
        "- Use these plots as **tracking and context**, not as diagnosis.\n",
        "\n"
      ],
      "metadata": {
        "id": "1S1KAl8qO11E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6 — HRV vs time with gait shading (two separate graphs)\n",
        "# Plot A: normalized (local) CVRMSSD_local = 100 * RMSSD / meanNN_local\n",
        "# Plot B: raw RMSSD (ms)\n",
        "#\n",
        "# Shading uses gait segments `segs` where label_id: 0=stand,1=walk,2=trot\n",
        "# Legend includes ONLY gaits (shading), not the HRV lines.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# -----------------------------\n",
        "# #TODO parameters\n",
        "# -----------------------------\n",
        "WINDOW_S = 60                 # window for RMSSD (seconds)\n",
        "NORM_WINDOW_S = 60           # window for local meanNN used for normalization (seconds)\n",
        "MIN_NN_IN_WINDOW = 15         # min NN points required in RMSSD window\n",
        "\n",
        "# Artifact cleaning\n",
        "NN_MIN_MS = 300               # physiological bounds\n",
        "NN_MAX_MS = 2000\n",
        "MAX_DIFF_MS = 250             # drop beats with huge successive NN jumps\n",
        "NN_OUTLIER_Z = 4.0            # MAD-based global outlier filter (bigger = less aggressive)\n",
        "\n",
        "# Time alignment\n",
        "HR_TIME_OFFSET_S = 0.0        # align beat times with segment times if needed\n",
        "\n",
        "# Shading colors (0=stand, 1=walk, 2=trot)\n",
        "bg_colors = {\n",
        "    0: \"lightgreen\",  # stand\n",
        "    1: \"lightblue\",   # walk\n",
        "    2: \"pink\",        # trot\n",
        "}\n",
        "SHADE_ALPHA = 0.6\n",
        "\n",
        "# Line colors\n",
        "NORM_LINE_COLOR = \"purple\"\n",
        "RAW_LINE_COLOR  = \"black\"\n",
        "\n",
        "# -----------------------------\n",
        "# Preconditions\n",
        "# -----------------------------\n",
        "if \"df_work\" not in globals():\n",
        "    raise ValueError(\"df_work not found. Load the workout HR file into df_work first.\")\n",
        "if \"segs\" not in globals():\n",
        "    raise ValueError(\"segs not found. Run the speed/segmentation step first (the one that produces `segs`).\")\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def ensure_time_seconds(df):\n",
        "    \"\"\"Ensure a 't_s' column exists.\"\"\"\n",
        "    if \"t_s\" in df.columns:\n",
        "        df[\"t_s\"] = pd.to_numeric(df[\"t_s\"], errors=\"coerce\")\n",
        "        return df\n",
        "\n",
        "    col_lower = {c.lower(): c for c in df.columns}\n",
        "    if \"ms\" in col_lower:\n",
        "        ms = pd.to_numeric(df[col_lower[\"ms\"]], errors=\"coerce\")\n",
        "        df[\"t_s\"] = (ms - ms.iloc[0]) / 1000.0\n",
        "        return df\n",
        "\n",
        "    for key in [\"timestamp\", \"time\", \"datetime\", \"date_time\", \"date time\"]:\n",
        "        if key in col_lower:\n",
        "            ts = pd.to_datetime(df[col_lower[key]], errors=\"coerce\")\n",
        "            df[\"t_s\"] = (ts - ts.iloc[0]).dt.total_seconds()\n",
        "            return df\n",
        "\n",
        "    raise ValueError(\"Could not infer time column. Expected 't_s' or 'MS' or a timestamp-like column.\")\n",
        "\n",
        "def find_rr_column(df):\n",
        "    \"\"\"Find likely NN/RR interval column in ms.\"\"\"\n",
        "    candidates = [\n",
        "        \"rr\", \"rr_ms\", \"rr(ms)\", \"rr (ms)\", \"rrinterval\", \"rr_interval\",\n",
        "        \"nn\", \"nn_ms\", \"ibi\", \"ibi_ms\", \"rri\", \"rri_ms\"\n",
        "    ]\n",
        "    col_map = {c.lower().replace(\" \", \"\"): c for c in df.columns}\n",
        "    for k in candidates:\n",
        "        kk = k.lower().replace(\" \", \"\")\n",
        "        if kk in col_map:\n",
        "            return col_map[kk]\n",
        "    for c in df.columns:\n",
        "        cl = c.lower()\n",
        "        if (\"rr\" in cl or \"nn\" in cl or \"ibi\" in cl) and (\"ms\" in cl or \"interval\" in cl):\n",
        "            return c\n",
        "    raise ValueError(\"Could not find an RR/NN interval column in df_work.\")\n",
        "\n",
        "def mad_based_inlier_mask(x, z=4.0):\n",
        "    \"\"\"True = keep; False = outlier.\"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    m = np.nanmedian(x)\n",
        "    mad = np.nanmedian(np.abs(x - m))\n",
        "    if not np.isfinite(mad) or mad == 0:\n",
        "        return np.isfinite(x)\n",
        "    robust_z = 0.6745 * (x - m) / mad\n",
        "    return np.isfinite(x) & (np.abs(robust_z) <= z)\n",
        "\n",
        "def rmssd_raw(x):\n",
        "    \"\"\"RMSSD on a 1D array of NN intervals (ms).\"\"\"\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    if len(x) < MIN_NN_IN_WINDOW:\n",
        "        return np.nan\n",
        "    dx = np.diff(x)\n",
        "    if len(dx) == 0:\n",
        "        return np.nan\n",
        "    return float(np.sqrt(np.mean(dx * dx)))\n",
        "\n",
        "def shade_segments(ax, segs, alpha=0.6):\n",
        "    \"\"\"Shade gait segments on x-axis in minutes. segs: list of (label_id, t0, t1).\"\"\"\n",
        "    for lab, t0, t1 in segs:\n",
        "        if lab not in bg_colors:\n",
        "            continue\n",
        "        ax.axvspan(t0/60.0, t1/60.0, alpha=alpha, color=bg_colors[lab], lw=0)\n",
        "\n",
        "def add_gait_legend(ax):\n",
        "    \"\"\"Legend for gait shading only.\"\"\"\n",
        "    gait_labels = {0: \"Stand\", 1: \"Walk\", 2: \"Trot\"}\n",
        "    patches = [mpatches.Patch(color=bg_colors[k], label=gait_labels[k], alpha=SHADE_ALPHA) for k in [0, 1, 2]]\n",
        "    ax.legend(handles=patches, loc=\"best\", title=\"Gait (shading)\")\n",
        "\n",
        "# -----------------------------\n",
        "# Prepare beat series (time + NN)\n",
        "# -----------------------------\n",
        "df_work = ensure_time_seconds(df_work)\n",
        "rr_col = find_rr_column(df_work)\n",
        "\n",
        "beat = df_work[[\"t_s\", rr_col]].copy()\n",
        "beat[\"t_s\"] = pd.to_numeric(beat[\"t_s\"], errors=\"coerce\") + HR_TIME_OFFSET_S\n",
        "beat[\"NN_ms\"] = pd.to_numeric(beat[rr_col], errors=\"coerce\")\n",
        "beat = beat.dropna(subset=[\"t_s\", \"NN_ms\"]).sort_values(\"t_s\").reset_index(drop=True)\n",
        "\n",
        "# Range filter\n",
        "beat = beat[(beat[\"NN_ms\"] >= NN_MIN_MS) & (beat[\"NN_ms\"] <= NN_MAX_MS)].reset_index(drop=True)\n",
        "\n",
        "# Jump filter\n",
        "if len(beat) >= 3:\n",
        "    d = np.abs(np.diff(beat[\"NN_ms\"].to_numpy(dtype=float)))\n",
        "    keep = np.ones(len(beat), dtype=bool)\n",
        "    keep[1:] &= (d <= MAX_DIFF_MS)\n",
        "    beat = beat.loc[keep].reset_index(drop=True)\n",
        "\n",
        "# Global MAD outlier filter\n",
        "inliers = mad_based_inlier_mask(beat[\"NN_ms\"].to_numpy(dtype=float), z=NN_OUTLIER_Z)\n",
        "beat = beat.loc[inliers].reset_index(drop=True)\n",
        "\n",
        "if len(beat) < 50:\n",
        "    print(\"⚠️ Very few NN points after filtering. Consider loosening filters (MAX_DIFF_MS or NN_OUTLIER_Z).\")\n",
        "\n",
        "# -----------------------------\n",
        "# Rolling metrics using time-based windows\n",
        "# -----------------------------\n",
        "idx = pd.to_timedelta(beat[\"t_s\"], unit=\"s\")\n",
        "nn = pd.Series(beat[\"NN_ms\"].to_numpy(dtype=float), index=idx)\n",
        "\n",
        "rmssd = nn.rolling(f\"{WINDOW_S}s\").apply(rmssd_raw, raw=True)\n",
        "mean_nn_local = nn.rolling(f\"{NORM_WINDOW_S}s\").mean()\n",
        "cvrmssd_local = 100.0 * (rmssd / mean_nn_local)\n",
        "\n",
        "ts_norm = pd.DataFrame({\n",
        "    \"t_s\": beat[\"t_s\"].to_numpy(dtype=float),\n",
        "    \"CVRMSSD_local_pct\": cvrmssd_local.to_numpy(dtype=float),\n",
        "}).dropna(subset=[\"CVRMSSD_local_pct\"]).reset_index(drop=True)\n",
        "\n",
        "ts_raw = pd.DataFrame({\n",
        "    \"t_s\": beat[\"t_s\"].to_numpy(dtype=float),\n",
        "    \"RMSSD_ms\": rmssd.to_numpy(dtype=float),\n",
        "}).dropna(subset=[\"RMSSD_ms\"]).reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Plot A: Normalized HRV (local)\n",
        "# -----------------------------\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "shade_segments(ax, segs, alpha=SHADE_ALPHA)\n",
        "ax.plot(ts_norm[\"t_s\"]/60.0, ts_norm[\"CVRMSSD_local_pct\"], color=NORM_LINE_COLOR, linewidth=1.6)\n",
        "\n",
        "ax.set_xlabel(\"Time (min)\")\n",
        "ax.set_ylabel(f\"CVRMSSD_local (%) = 100 × RMSSD/meanNN_local\")\n",
        "ax.set_title(\"Normalized HRV over time (local normalization) with gait shading\")\n",
        "ax.grid(True, alpha=0.25)\n",
        "add_gait_legend(ax)\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Plot B: Raw RMSSD (ms)\n",
        "# -----------------------------\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "shade_segments(ax, segs, alpha=SHADE_ALPHA)\n",
        "ax.plot(ts_raw[\"t_s\"]/60.0, ts_raw[\"RMSSD_ms\"], color=RAW_LINE_COLOR, linewidth=1.4)\n",
        "\n",
        "ax.set_xlabel(\"Time (min)\")\n",
        "ax.set_ylabel(f\"RMSSD (ms)  [window={WINDOW_S}s]\")\n",
        "ax.set_title(\"Raw HRV over time (RMSSD) with gait shading\")\n",
        "ax.grid(True, alpha=0.25)\n",
        "add_gait_legend(ax)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "J8xUAAPQNwsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzW_UpXdO2hz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}